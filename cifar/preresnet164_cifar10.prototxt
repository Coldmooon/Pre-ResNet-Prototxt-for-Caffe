layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    mean_file: "./mean"
  }
  data_param {
    source: "./train"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    mean_file: "./mean"
  }
  data_param {
    source: "./test"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv_start"
  type: "Convolution"
  bottom: "data"
  top: "conv_start"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn_pre"
  type: "BatchNorm"
  bottom: "conv_start"
  top: "conv_start"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn_pre"
  type: "BatchNorm"
  bottom: "conv_start"
  top: "conv_start"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_pre_scale"
  type: "Scale"
  bottom: "conv_start"
  top: "conv_start"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_pre_relu"
  type: "ReLU"
  bottom: "conv_start"
  top: "conv_start"
}
layer {
  name: "map16_1_conv1"
  type: "Convolution"
  bottom: "conv_start"
  top: "map16_1_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn1"
  type: "BatchNorm"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn1"
  type: "BatchNorm"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_scale1"
  type: "Scale"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_relu1"
  type: "ReLU"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
}
layer {
  name: "map16_1_conv2"
  type: "Convolution"
  bottom: "map16_1_conv1"
  top: "map16_1_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn2"
  type: "BatchNorm"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn2"
  type: "BatchNorm"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_scale2"
  type: "Scale"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_relu2"
  type: "ReLU"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
}
layer {
  name: "map16_1_conv_end"
  type: "Convolution"
  bottom: "map16_1_conv2"
  top: "map16_1_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "conv_start"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_eltsum"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "map16_1_conv_end"
  top: "map16_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_2_bn_pre"
  type: "BatchNorm"
  bottom: "map16_1_eltsum"
  top: "map16_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn_pre"
  type: "BatchNorm"
  bottom: "map16_1_eltsum"
  top: "map16_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_pre_scale"
  type: "Scale"
  bottom: "map16_2_bn_pre"
  top: "map16_2_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_pre_relu"
  type: "ReLU"
  bottom: "map16_2_bn_pre"
  top: "map16_2_bn_pre"
}
layer {
  name: "map16_2_conv1"
  type: "Convolution"
  bottom: "map16_2_bn_pre"
  top: "map16_2_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_bn1"
  type: "BatchNorm"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn1"
  type: "BatchNorm"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_scale1"
  type: "Scale"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_relu1"
  type: "ReLU"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
}
layer {
  name: "map16_2_conv2"
  type: "Convolution"
  bottom: "map16_2_conv1"
  top: "map16_2_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_bn2"
  type: "BatchNorm"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn2"
  type: "BatchNorm"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_scale2"
  type: "Scale"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_relu2"
  type: "ReLU"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
}
layer {
  name: "map16_2_conv_end"
  type: "Convolution"
  bottom: "map16_2_conv2"
  top: "map16_2_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_eltsum"
  type: "Eltwise"
  bottom: "map16_1_eltsum"
  bottom: "map16_2_conv_end"
  top: "map16_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_3_bn_pre"
  type: "BatchNorm"
  bottom: "map16_2_eltsum"
  top: "map16_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn_pre"
  type: "BatchNorm"
  bottom: "map16_2_eltsum"
  top: "map16_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_pre_scale"
  type: "Scale"
  bottom: "map16_3_bn_pre"
  top: "map16_3_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_pre_relu"
  type: "ReLU"
  bottom: "map16_3_bn_pre"
  top: "map16_3_bn_pre"
}
layer {
  name: "map16_3_conv1"
  type: "Convolution"
  bottom: "map16_3_bn_pre"
  top: "map16_3_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_bn1"
  type: "BatchNorm"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn1"
  type: "BatchNorm"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_scale1"
  type: "Scale"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_relu1"
  type: "ReLU"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
}
layer {
  name: "map16_3_conv2"
  type: "Convolution"
  bottom: "map16_3_conv1"
  top: "map16_3_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_bn2"
  type: "BatchNorm"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn2"
  type: "BatchNorm"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_scale2"
  type: "Scale"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_relu2"
  type: "ReLU"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
}
layer {
  name: "map16_3_conv_end"
  type: "Convolution"
  bottom: "map16_3_conv2"
  top: "map16_3_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_eltsum"
  type: "Eltwise"
  bottom: "map16_2_eltsum"
  bottom: "map16_3_conv_end"
  top: "map16_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_4_bn_pre"
  type: "BatchNorm"
  bottom: "map16_3_eltsum"
  top: "map16_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_4_bn_pre"
  type: "BatchNorm"
  bottom: "map16_3_eltsum"
  top: "map16_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_4_pre_scale"
  type: "Scale"
  bottom: "map16_4_bn_pre"
  top: "map16_4_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_4_pre_relu"
  type: "ReLU"
  bottom: "map16_4_bn_pre"
  top: "map16_4_bn_pre"
}
layer {
  name: "map16_4_conv1"
  type: "Convolution"
  bottom: "map16_4_bn_pre"
  top: "map16_4_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_4_bn1"
  type: "BatchNorm"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_4_bn1"
  type: "BatchNorm"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_4_scale1"
  type: "Scale"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_4_relu1"
  type: "ReLU"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
}
layer {
  name: "map16_4_conv2"
  type: "Convolution"
  bottom: "map16_4_conv1"
  top: "map16_4_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_4_bn2"
  type: "BatchNorm"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_4_bn2"
  type: "BatchNorm"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_4_scale2"
  type: "Scale"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_4_relu2"
  type: "ReLU"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
}
layer {
  name: "map16_4_conv_end"
  type: "Convolution"
  bottom: "map16_4_conv2"
  top: "map16_4_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_4_eltsum"
  type: "Eltwise"
  bottom: "map16_3_eltsum"
  bottom: "map16_4_conv_end"
  top: "map16_4_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_5_bn_pre"
  type: "BatchNorm"
  bottom: "map16_4_eltsum"
  top: "map16_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_5_bn_pre"
  type: "BatchNorm"
  bottom: "map16_4_eltsum"
  top: "map16_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_5_pre_scale"
  type: "Scale"
  bottom: "map16_5_bn_pre"
  top: "map16_5_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_5_pre_relu"
  type: "ReLU"
  bottom: "map16_5_bn_pre"
  top: "map16_5_bn_pre"
}
layer {
  name: "map16_5_conv1"
  type: "Convolution"
  bottom: "map16_5_bn_pre"
  top: "map16_5_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_5_bn1"
  type: "BatchNorm"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_5_bn1"
  type: "BatchNorm"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_5_scale1"
  type: "Scale"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_5_relu1"
  type: "ReLU"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
}
layer {
  name: "map16_5_conv2"
  type: "Convolution"
  bottom: "map16_5_conv1"
  top: "map16_5_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_5_bn2"
  type: "BatchNorm"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_5_bn2"
  type: "BatchNorm"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_5_scale2"
  type: "Scale"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_5_relu2"
  type: "ReLU"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
}
layer {
  name: "map16_5_conv_end"
  type: "Convolution"
  bottom: "map16_5_conv2"
  top: "map16_5_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_5_eltsum"
  type: "Eltwise"
  bottom: "map16_4_eltsum"
  bottom: "map16_5_conv_end"
  top: "map16_5_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_6_bn_pre"
  type: "BatchNorm"
  bottom: "map16_5_eltsum"
  top: "map16_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_6_bn_pre"
  type: "BatchNorm"
  bottom: "map16_5_eltsum"
  top: "map16_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_6_pre_scale"
  type: "Scale"
  bottom: "map16_6_bn_pre"
  top: "map16_6_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_6_pre_relu"
  type: "ReLU"
  bottom: "map16_6_bn_pre"
  top: "map16_6_bn_pre"
}
layer {
  name: "map16_6_conv1"
  type: "Convolution"
  bottom: "map16_6_bn_pre"
  top: "map16_6_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_6_bn1"
  type: "BatchNorm"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_6_bn1"
  type: "BatchNorm"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_6_scale1"
  type: "Scale"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_6_relu1"
  type: "ReLU"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
}
layer {
  name: "map16_6_conv2"
  type: "Convolution"
  bottom: "map16_6_conv1"
  top: "map16_6_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_6_bn2"
  type: "BatchNorm"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_6_bn2"
  type: "BatchNorm"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_6_scale2"
  type: "Scale"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_6_relu2"
  type: "ReLU"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
}
layer {
  name: "map16_6_conv_end"
  type: "Convolution"
  bottom: "map16_6_conv2"
  top: "map16_6_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_6_eltsum"
  type: "Eltwise"
  bottom: "map16_5_eltsum"
  bottom: "map16_6_conv_end"
  top: "map16_6_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_7_bn_pre"
  type: "BatchNorm"
  bottom: "map16_6_eltsum"
  top: "map16_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_7_bn_pre"
  type: "BatchNorm"
  bottom: "map16_6_eltsum"
  top: "map16_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_7_pre_scale"
  type: "Scale"
  bottom: "map16_7_bn_pre"
  top: "map16_7_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_7_pre_relu"
  type: "ReLU"
  bottom: "map16_7_bn_pre"
  top: "map16_7_bn_pre"
}
layer {
  name: "map16_7_conv1"
  type: "Convolution"
  bottom: "map16_7_bn_pre"
  top: "map16_7_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_7_bn1"
  type: "BatchNorm"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_7_bn1"
  type: "BatchNorm"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_7_scale1"
  type: "Scale"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_7_relu1"
  type: "ReLU"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
}
layer {
  name: "map16_7_conv2"
  type: "Convolution"
  bottom: "map16_7_conv1"
  top: "map16_7_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_7_bn2"
  type: "BatchNorm"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_7_bn2"
  type: "BatchNorm"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_7_scale2"
  type: "Scale"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_7_relu2"
  type: "ReLU"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
}
layer {
  name: "map16_7_conv_end"
  type: "Convolution"
  bottom: "map16_7_conv2"
  top: "map16_7_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_7_eltsum"
  type: "Eltwise"
  bottom: "map16_6_eltsum"
  bottom: "map16_7_conv_end"
  top: "map16_7_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_8_bn_pre"
  type: "BatchNorm"
  bottom: "map16_7_eltsum"
  top: "map16_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_8_bn_pre"
  type: "BatchNorm"
  bottom: "map16_7_eltsum"
  top: "map16_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_8_pre_scale"
  type: "Scale"
  bottom: "map16_8_bn_pre"
  top: "map16_8_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_8_pre_relu"
  type: "ReLU"
  bottom: "map16_8_bn_pre"
  top: "map16_8_bn_pre"
}
layer {
  name: "map16_8_conv1"
  type: "Convolution"
  bottom: "map16_8_bn_pre"
  top: "map16_8_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_8_bn1"
  type: "BatchNorm"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_8_bn1"
  type: "BatchNorm"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_8_scale1"
  type: "Scale"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_8_relu1"
  type: "ReLU"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
}
layer {
  name: "map16_8_conv2"
  type: "Convolution"
  bottom: "map16_8_conv1"
  top: "map16_8_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_8_bn2"
  type: "BatchNorm"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_8_bn2"
  type: "BatchNorm"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_8_scale2"
  type: "Scale"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_8_relu2"
  type: "ReLU"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
}
layer {
  name: "map16_8_conv_end"
  type: "Convolution"
  bottom: "map16_8_conv2"
  top: "map16_8_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_8_eltsum"
  type: "Eltwise"
  bottom: "map16_7_eltsum"
  bottom: "map16_8_conv_end"
  top: "map16_8_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_9_bn_pre"
  type: "BatchNorm"
  bottom: "map16_8_eltsum"
  top: "map16_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_9_bn_pre"
  type: "BatchNorm"
  bottom: "map16_8_eltsum"
  top: "map16_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_9_pre_scale"
  type: "Scale"
  bottom: "map16_9_bn_pre"
  top: "map16_9_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_9_pre_relu"
  type: "ReLU"
  bottom: "map16_9_bn_pre"
  top: "map16_9_bn_pre"
}
layer {
  name: "map16_9_conv1"
  type: "Convolution"
  bottom: "map16_9_bn_pre"
  top: "map16_9_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_9_bn1"
  type: "BatchNorm"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_9_bn1"
  type: "BatchNorm"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_9_scale1"
  type: "Scale"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_9_relu1"
  type: "ReLU"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
}
layer {
  name: "map16_9_conv2"
  type: "Convolution"
  bottom: "map16_9_conv1"
  top: "map16_9_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_9_bn2"
  type: "BatchNorm"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_9_bn2"
  type: "BatchNorm"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_9_scale2"
  type: "Scale"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_9_relu2"
  type: "ReLU"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
}
layer {
  name: "map16_9_conv_end"
  type: "Convolution"
  bottom: "map16_9_conv2"
  top: "map16_9_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_9_eltsum"
  type: "Eltwise"
  bottom: "map16_8_eltsum"
  bottom: "map16_9_conv_end"
  top: "map16_9_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_10_bn_pre"
  type: "BatchNorm"
  bottom: "map16_9_eltsum"
  top: "map16_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_10_bn_pre"
  type: "BatchNorm"
  bottom: "map16_9_eltsum"
  top: "map16_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_10_pre_scale"
  type: "Scale"
  bottom: "map16_10_bn_pre"
  top: "map16_10_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_10_pre_relu"
  type: "ReLU"
  bottom: "map16_10_bn_pre"
  top: "map16_10_bn_pre"
}
layer {
  name: "map16_10_conv1"
  type: "Convolution"
  bottom: "map16_10_bn_pre"
  top: "map16_10_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_10_bn1"
  type: "BatchNorm"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_10_bn1"
  type: "BatchNorm"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_10_scale1"
  type: "Scale"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_10_relu1"
  type: "ReLU"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
}
layer {
  name: "map16_10_conv2"
  type: "Convolution"
  bottom: "map16_10_conv1"
  top: "map16_10_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_10_bn2"
  type: "BatchNorm"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_10_bn2"
  type: "BatchNorm"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_10_scale2"
  type: "Scale"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_10_relu2"
  type: "ReLU"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
}
layer {
  name: "map16_10_conv_end"
  type: "Convolution"
  bottom: "map16_10_conv2"
  top: "map16_10_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_10_eltsum"
  type: "Eltwise"
  bottom: "map16_9_eltsum"
  bottom: "map16_10_conv_end"
  top: "map16_10_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_11_bn_pre"
  type: "BatchNorm"
  bottom: "map16_10_eltsum"
  top: "map16_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_11_bn_pre"
  type: "BatchNorm"
  bottom: "map16_10_eltsum"
  top: "map16_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_11_pre_scale"
  type: "Scale"
  bottom: "map16_11_bn_pre"
  top: "map16_11_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_11_pre_relu"
  type: "ReLU"
  bottom: "map16_11_bn_pre"
  top: "map16_11_bn_pre"
}
layer {
  name: "map16_11_conv1"
  type: "Convolution"
  bottom: "map16_11_bn_pre"
  top: "map16_11_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_11_bn1"
  type: "BatchNorm"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_11_bn1"
  type: "BatchNorm"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_11_scale1"
  type: "Scale"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_11_relu1"
  type: "ReLU"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
}
layer {
  name: "map16_11_conv2"
  type: "Convolution"
  bottom: "map16_11_conv1"
  top: "map16_11_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_11_bn2"
  type: "BatchNorm"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_11_bn2"
  type: "BatchNorm"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_11_scale2"
  type: "Scale"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_11_relu2"
  type: "ReLU"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
}
layer {
  name: "map16_11_conv_end"
  type: "Convolution"
  bottom: "map16_11_conv2"
  top: "map16_11_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_11_eltsum"
  type: "Eltwise"
  bottom: "map16_10_eltsum"
  bottom: "map16_11_conv_end"
  top: "map16_11_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_12_bn_pre"
  type: "BatchNorm"
  bottom: "map16_11_eltsum"
  top: "map16_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_12_bn_pre"
  type: "BatchNorm"
  bottom: "map16_11_eltsum"
  top: "map16_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_12_pre_scale"
  type: "Scale"
  bottom: "map16_12_bn_pre"
  top: "map16_12_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_12_pre_relu"
  type: "ReLU"
  bottom: "map16_12_bn_pre"
  top: "map16_12_bn_pre"
}
layer {
  name: "map16_12_conv1"
  type: "Convolution"
  bottom: "map16_12_bn_pre"
  top: "map16_12_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_12_bn1"
  type: "BatchNorm"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_12_bn1"
  type: "BatchNorm"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_12_scale1"
  type: "Scale"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_12_relu1"
  type: "ReLU"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
}
layer {
  name: "map16_12_conv2"
  type: "Convolution"
  bottom: "map16_12_conv1"
  top: "map16_12_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_12_bn2"
  type: "BatchNorm"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_12_bn2"
  type: "BatchNorm"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_12_scale2"
  type: "Scale"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_12_relu2"
  type: "ReLU"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
}
layer {
  name: "map16_12_conv_end"
  type: "Convolution"
  bottom: "map16_12_conv2"
  top: "map16_12_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_12_eltsum"
  type: "Eltwise"
  bottom: "map16_11_eltsum"
  bottom: "map16_12_conv_end"
  top: "map16_12_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_13_bn_pre"
  type: "BatchNorm"
  bottom: "map16_12_eltsum"
  top: "map16_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_13_bn_pre"
  type: "BatchNorm"
  bottom: "map16_12_eltsum"
  top: "map16_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_13_pre_scale"
  type: "Scale"
  bottom: "map16_13_bn_pre"
  top: "map16_13_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_13_pre_relu"
  type: "ReLU"
  bottom: "map16_13_bn_pre"
  top: "map16_13_bn_pre"
}
layer {
  name: "map16_13_conv1"
  type: "Convolution"
  bottom: "map16_13_bn_pre"
  top: "map16_13_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_13_bn1"
  type: "BatchNorm"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_13_bn1"
  type: "BatchNorm"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_13_scale1"
  type: "Scale"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_13_relu1"
  type: "ReLU"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
}
layer {
  name: "map16_13_conv2"
  type: "Convolution"
  bottom: "map16_13_conv1"
  top: "map16_13_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_13_bn2"
  type: "BatchNorm"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_13_bn2"
  type: "BatchNorm"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_13_scale2"
  type: "Scale"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_13_relu2"
  type: "ReLU"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
}
layer {
  name: "map16_13_conv_end"
  type: "Convolution"
  bottom: "map16_13_conv2"
  top: "map16_13_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_13_eltsum"
  type: "Eltwise"
  bottom: "map16_12_eltsum"
  bottom: "map16_13_conv_end"
  top: "map16_13_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_14_bn_pre"
  type: "BatchNorm"
  bottom: "map16_13_eltsum"
  top: "map16_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_14_bn_pre"
  type: "BatchNorm"
  bottom: "map16_13_eltsum"
  top: "map16_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_14_pre_scale"
  type: "Scale"
  bottom: "map16_14_bn_pre"
  top: "map16_14_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_14_pre_relu"
  type: "ReLU"
  bottom: "map16_14_bn_pre"
  top: "map16_14_bn_pre"
}
layer {
  name: "map16_14_conv1"
  type: "Convolution"
  bottom: "map16_14_bn_pre"
  top: "map16_14_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_14_bn1"
  type: "BatchNorm"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_14_bn1"
  type: "BatchNorm"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_14_scale1"
  type: "Scale"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_14_relu1"
  type: "ReLU"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
}
layer {
  name: "map16_14_conv2"
  type: "Convolution"
  bottom: "map16_14_conv1"
  top: "map16_14_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_14_bn2"
  type: "BatchNorm"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_14_bn2"
  type: "BatchNorm"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_14_scale2"
  type: "Scale"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_14_relu2"
  type: "ReLU"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
}
layer {
  name: "map16_14_conv_end"
  type: "Convolution"
  bottom: "map16_14_conv2"
  top: "map16_14_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_14_eltsum"
  type: "Eltwise"
  bottom: "map16_13_eltsum"
  bottom: "map16_14_conv_end"
  top: "map16_14_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_15_bn_pre"
  type: "BatchNorm"
  bottom: "map16_14_eltsum"
  top: "map16_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_15_bn_pre"
  type: "BatchNorm"
  bottom: "map16_14_eltsum"
  top: "map16_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_15_pre_scale"
  type: "Scale"
  bottom: "map16_15_bn_pre"
  top: "map16_15_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_15_pre_relu"
  type: "ReLU"
  bottom: "map16_15_bn_pre"
  top: "map16_15_bn_pre"
}
layer {
  name: "map16_15_conv1"
  type: "Convolution"
  bottom: "map16_15_bn_pre"
  top: "map16_15_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_15_bn1"
  type: "BatchNorm"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_15_bn1"
  type: "BatchNorm"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_15_scale1"
  type: "Scale"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_15_relu1"
  type: "ReLU"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
}
layer {
  name: "map16_15_conv2"
  type: "Convolution"
  bottom: "map16_15_conv1"
  top: "map16_15_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_15_bn2"
  type: "BatchNorm"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_15_bn2"
  type: "BatchNorm"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_15_scale2"
  type: "Scale"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_15_relu2"
  type: "ReLU"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
}
layer {
  name: "map16_15_conv_end"
  type: "Convolution"
  bottom: "map16_15_conv2"
  top: "map16_15_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_15_eltsum"
  type: "Eltwise"
  bottom: "map16_14_eltsum"
  bottom: "map16_15_conv_end"
  top: "map16_15_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_16_bn_pre"
  type: "BatchNorm"
  bottom: "map16_15_eltsum"
  top: "map16_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_16_bn_pre"
  type: "BatchNorm"
  bottom: "map16_15_eltsum"
  top: "map16_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_16_pre_scale"
  type: "Scale"
  bottom: "map16_16_bn_pre"
  top: "map16_16_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_16_pre_relu"
  type: "ReLU"
  bottom: "map16_16_bn_pre"
  top: "map16_16_bn_pre"
}
layer {
  name: "map16_16_conv1"
  type: "Convolution"
  bottom: "map16_16_bn_pre"
  top: "map16_16_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_16_bn1"
  type: "BatchNorm"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_16_bn1"
  type: "BatchNorm"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_16_scale1"
  type: "Scale"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_16_relu1"
  type: "ReLU"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
}
layer {
  name: "map16_16_conv2"
  type: "Convolution"
  bottom: "map16_16_conv1"
  top: "map16_16_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_16_bn2"
  type: "BatchNorm"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_16_bn2"
  type: "BatchNorm"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_16_scale2"
  type: "Scale"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_16_relu2"
  type: "ReLU"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
}
layer {
  name: "map16_16_conv_end"
  type: "Convolution"
  bottom: "map16_16_conv2"
  top: "map16_16_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_16_eltsum"
  type: "Eltwise"
  bottom: "map16_15_eltsum"
  bottom: "map16_16_conv_end"
  top: "map16_16_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_17_bn_pre"
  type: "BatchNorm"
  bottom: "map16_16_eltsum"
  top: "map16_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_17_bn_pre"
  type: "BatchNorm"
  bottom: "map16_16_eltsum"
  top: "map16_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_17_pre_scale"
  type: "Scale"
  bottom: "map16_17_bn_pre"
  top: "map16_17_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_17_pre_relu"
  type: "ReLU"
  bottom: "map16_17_bn_pre"
  top: "map16_17_bn_pre"
}
layer {
  name: "map16_17_conv1"
  type: "Convolution"
  bottom: "map16_17_bn_pre"
  top: "map16_17_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_17_bn1"
  type: "BatchNorm"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_17_bn1"
  type: "BatchNorm"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_17_scale1"
  type: "Scale"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_17_relu1"
  type: "ReLU"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
}
layer {
  name: "map16_17_conv2"
  type: "Convolution"
  bottom: "map16_17_conv1"
  top: "map16_17_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_17_bn2"
  type: "BatchNorm"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_17_bn2"
  type: "BatchNorm"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_17_scale2"
  type: "Scale"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_17_relu2"
  type: "ReLU"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
}
layer {
  name: "map16_17_conv_end"
  type: "Convolution"
  bottom: "map16_17_conv2"
  top: "map16_17_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_17_eltsum"
  type: "Eltwise"
  bottom: "map16_16_eltsum"
  bottom: "map16_17_conv_end"
  top: "map16_17_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_18_bn_pre"
  type: "BatchNorm"
  bottom: "map16_17_eltsum"
  top: "map16_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_18_bn_pre"
  type: "BatchNorm"
  bottom: "map16_17_eltsum"
  top: "map16_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_18_pre_scale"
  type: "Scale"
  bottom: "map16_18_bn_pre"
  top: "map16_18_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_18_pre_relu"
  type: "ReLU"
  bottom: "map16_18_bn_pre"
  top: "map16_18_bn_pre"
}
layer {
  name: "map16_18_conv1"
  type: "Convolution"
  bottom: "map16_18_bn_pre"
  top: "map16_18_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_18_bn1"
  type: "BatchNorm"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_18_bn1"
  type: "BatchNorm"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_18_scale1"
  type: "Scale"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_18_relu1"
  type: "ReLU"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
}
layer {
  name: "map16_18_conv2"
  type: "Convolution"
  bottom: "map16_18_conv1"
  top: "map16_18_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_18_bn2"
  type: "BatchNorm"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_18_bn2"
  type: "BatchNorm"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_18_scale2"
  type: "Scale"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_18_relu2"
  type: "ReLU"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
}
layer {
  name: "map16_18_conv_end"
  type: "Convolution"
  bottom: "map16_18_conv2"
  top: "map16_18_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_18_eltsum"
  type: "Eltwise"
  bottom: "map16_17_eltsum"
  bottom: "map16_18_conv_end"
  top: "map16_18_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_1_bn_pre"
  type: "BatchNorm"
  bottom: "map16_18_eltsum"
  top: "map32_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn_pre"
  type: "BatchNorm"
  bottom: "map16_18_eltsum"
  top: "map32_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_pre_scale"
  type: "Scale"
  bottom: "map32_1_bn_pre"
  top: "map32_1_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_pre_relu"
  type: "ReLU"
  bottom: "map32_1_bn_pre"
  top: "map32_1_bn_pre"
}
layer {
  name: "map32_1_conv1"
  type: "Convolution"
  bottom: "map32_1_bn_pre"
  top: "map32_1_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn1"
  type: "BatchNorm"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn1"
  type: "BatchNorm"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale1"
  type: "Scale"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_relu1"
  type: "ReLU"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
}
layer {
  name: "map32_1_conv2"
  type: "Convolution"
  bottom: "map32_1_conv1"
  top: "map32_1_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn2"
  type: "BatchNorm"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn2"
  type: "BatchNorm"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale2"
  type: "Scale"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_relu2"
  type: "ReLU"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
}
layer {
  name: "map32_1_conv_end"
  type: "Convolution"
  bottom: "map32_1_conv2"
  top: "map32_1_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "map32_1_bn_pre"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_eltsum"
  type: "Eltwise"
  bottom: "Convolution2"
  bottom: "map32_1_conv_end"
  top: "map32_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_2_bn_pre"
  type: "BatchNorm"
  bottom: "map32_1_eltsum"
  top: "map32_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn_pre"
  type: "BatchNorm"
  bottom: "map32_1_eltsum"
  top: "map32_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_pre_scale"
  type: "Scale"
  bottom: "map32_2_bn_pre"
  top: "map32_2_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_pre_relu"
  type: "ReLU"
  bottom: "map32_2_bn_pre"
  top: "map32_2_bn_pre"
}
layer {
  name: "map32_2_conv1"
  type: "Convolution"
  bottom: "map32_2_bn_pre"
  top: "map32_2_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_bn1"
  type: "BatchNorm"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn1"
  type: "BatchNorm"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_scale1"
  type: "Scale"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_relu1"
  type: "ReLU"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
}
layer {
  name: "map32_2_conv2"
  type: "Convolution"
  bottom: "map32_2_conv1"
  top: "map32_2_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_bn2"
  type: "BatchNorm"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn2"
  type: "BatchNorm"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_scale2"
  type: "Scale"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_relu2"
  type: "ReLU"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
}
layer {
  name: "map32_2_conv_end"
  type: "Convolution"
  bottom: "map32_2_conv2"
  top: "map32_2_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_eltsum"
  type: "Eltwise"
  bottom: "map32_1_eltsum"
  bottom: "map32_2_conv_end"
  top: "map32_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_3_bn_pre"
  type: "BatchNorm"
  bottom: "map32_2_eltsum"
  top: "map32_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn_pre"
  type: "BatchNorm"
  bottom: "map32_2_eltsum"
  top: "map32_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_pre_scale"
  type: "Scale"
  bottom: "map32_3_bn_pre"
  top: "map32_3_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_pre_relu"
  type: "ReLU"
  bottom: "map32_3_bn_pre"
  top: "map32_3_bn_pre"
}
layer {
  name: "map32_3_conv1"
  type: "Convolution"
  bottom: "map32_3_bn_pre"
  top: "map32_3_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_bn1"
  type: "BatchNorm"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn1"
  type: "BatchNorm"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_scale1"
  type: "Scale"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_relu1"
  type: "ReLU"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
}
layer {
  name: "map32_3_conv2"
  type: "Convolution"
  bottom: "map32_3_conv1"
  top: "map32_3_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_bn2"
  type: "BatchNorm"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn2"
  type: "BatchNorm"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_scale2"
  type: "Scale"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_relu2"
  type: "ReLU"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
}
layer {
  name: "map32_3_conv_end"
  type: "Convolution"
  bottom: "map32_3_conv2"
  top: "map32_3_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_eltsum"
  type: "Eltwise"
  bottom: "map32_2_eltsum"
  bottom: "map32_3_conv_end"
  top: "map32_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_4_bn_pre"
  type: "BatchNorm"
  bottom: "map32_3_eltsum"
  top: "map32_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_4_bn_pre"
  type: "BatchNorm"
  bottom: "map32_3_eltsum"
  top: "map32_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_4_pre_scale"
  type: "Scale"
  bottom: "map32_4_bn_pre"
  top: "map32_4_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_4_pre_relu"
  type: "ReLU"
  bottom: "map32_4_bn_pre"
  top: "map32_4_bn_pre"
}
layer {
  name: "map32_4_conv1"
  type: "Convolution"
  bottom: "map32_4_bn_pre"
  top: "map32_4_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_4_bn1"
  type: "BatchNorm"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_4_bn1"
  type: "BatchNorm"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_4_scale1"
  type: "Scale"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_4_relu1"
  type: "ReLU"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
}
layer {
  name: "map32_4_conv2"
  type: "Convolution"
  bottom: "map32_4_conv1"
  top: "map32_4_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_4_bn2"
  type: "BatchNorm"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_4_bn2"
  type: "BatchNorm"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_4_scale2"
  type: "Scale"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_4_relu2"
  type: "ReLU"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
}
layer {
  name: "map32_4_conv_end"
  type: "Convolution"
  bottom: "map32_4_conv2"
  top: "map32_4_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_4_eltsum"
  type: "Eltwise"
  bottom: "map32_3_eltsum"
  bottom: "map32_4_conv_end"
  top: "map32_4_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_5_bn_pre"
  type: "BatchNorm"
  bottom: "map32_4_eltsum"
  top: "map32_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_5_bn_pre"
  type: "BatchNorm"
  bottom: "map32_4_eltsum"
  top: "map32_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_5_pre_scale"
  type: "Scale"
  bottom: "map32_5_bn_pre"
  top: "map32_5_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_5_pre_relu"
  type: "ReLU"
  bottom: "map32_5_bn_pre"
  top: "map32_5_bn_pre"
}
layer {
  name: "map32_5_conv1"
  type: "Convolution"
  bottom: "map32_5_bn_pre"
  top: "map32_5_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_5_bn1"
  type: "BatchNorm"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_5_bn1"
  type: "BatchNorm"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_5_scale1"
  type: "Scale"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_5_relu1"
  type: "ReLU"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
}
layer {
  name: "map32_5_conv2"
  type: "Convolution"
  bottom: "map32_5_conv1"
  top: "map32_5_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_5_bn2"
  type: "BatchNorm"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_5_bn2"
  type: "BatchNorm"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_5_scale2"
  type: "Scale"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_5_relu2"
  type: "ReLU"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
}
layer {
  name: "map32_5_conv_end"
  type: "Convolution"
  bottom: "map32_5_conv2"
  top: "map32_5_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_5_eltsum"
  type: "Eltwise"
  bottom: "map32_4_eltsum"
  bottom: "map32_5_conv_end"
  top: "map32_5_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_6_bn_pre"
  type: "BatchNorm"
  bottom: "map32_5_eltsum"
  top: "map32_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_6_bn_pre"
  type: "BatchNorm"
  bottom: "map32_5_eltsum"
  top: "map32_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_6_pre_scale"
  type: "Scale"
  bottom: "map32_6_bn_pre"
  top: "map32_6_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_6_pre_relu"
  type: "ReLU"
  bottom: "map32_6_bn_pre"
  top: "map32_6_bn_pre"
}
layer {
  name: "map32_6_conv1"
  type: "Convolution"
  bottom: "map32_6_bn_pre"
  top: "map32_6_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_6_bn1"
  type: "BatchNorm"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_6_bn1"
  type: "BatchNorm"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_6_scale1"
  type: "Scale"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_6_relu1"
  type: "ReLU"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
}
layer {
  name: "map32_6_conv2"
  type: "Convolution"
  bottom: "map32_6_conv1"
  top: "map32_6_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_6_bn2"
  type: "BatchNorm"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_6_bn2"
  type: "BatchNorm"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_6_scale2"
  type: "Scale"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_6_relu2"
  type: "ReLU"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
}
layer {
  name: "map32_6_conv_end"
  type: "Convolution"
  bottom: "map32_6_conv2"
  top: "map32_6_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_6_eltsum"
  type: "Eltwise"
  bottom: "map32_5_eltsum"
  bottom: "map32_6_conv_end"
  top: "map32_6_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_7_bn_pre"
  type: "BatchNorm"
  bottom: "map32_6_eltsum"
  top: "map32_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_7_bn_pre"
  type: "BatchNorm"
  bottom: "map32_6_eltsum"
  top: "map32_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_7_pre_scale"
  type: "Scale"
  bottom: "map32_7_bn_pre"
  top: "map32_7_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_7_pre_relu"
  type: "ReLU"
  bottom: "map32_7_bn_pre"
  top: "map32_7_bn_pre"
}
layer {
  name: "map32_7_conv1"
  type: "Convolution"
  bottom: "map32_7_bn_pre"
  top: "map32_7_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_7_bn1"
  type: "BatchNorm"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_7_bn1"
  type: "BatchNorm"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_7_scale1"
  type: "Scale"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_7_relu1"
  type: "ReLU"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
}
layer {
  name: "map32_7_conv2"
  type: "Convolution"
  bottom: "map32_7_conv1"
  top: "map32_7_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_7_bn2"
  type: "BatchNorm"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_7_bn2"
  type: "BatchNorm"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_7_scale2"
  type: "Scale"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_7_relu2"
  type: "ReLU"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
}
layer {
  name: "map32_7_conv_end"
  type: "Convolution"
  bottom: "map32_7_conv2"
  top: "map32_7_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_7_eltsum"
  type: "Eltwise"
  bottom: "map32_6_eltsum"
  bottom: "map32_7_conv_end"
  top: "map32_7_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_8_bn_pre"
  type: "BatchNorm"
  bottom: "map32_7_eltsum"
  top: "map32_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_8_bn_pre"
  type: "BatchNorm"
  bottom: "map32_7_eltsum"
  top: "map32_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_8_pre_scale"
  type: "Scale"
  bottom: "map32_8_bn_pre"
  top: "map32_8_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_8_pre_relu"
  type: "ReLU"
  bottom: "map32_8_bn_pre"
  top: "map32_8_bn_pre"
}
layer {
  name: "map32_8_conv1"
  type: "Convolution"
  bottom: "map32_8_bn_pre"
  top: "map32_8_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_8_bn1"
  type: "BatchNorm"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_8_bn1"
  type: "BatchNorm"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_8_scale1"
  type: "Scale"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_8_relu1"
  type: "ReLU"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
}
layer {
  name: "map32_8_conv2"
  type: "Convolution"
  bottom: "map32_8_conv1"
  top: "map32_8_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_8_bn2"
  type: "BatchNorm"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_8_bn2"
  type: "BatchNorm"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_8_scale2"
  type: "Scale"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_8_relu2"
  type: "ReLU"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
}
layer {
  name: "map32_8_conv_end"
  type: "Convolution"
  bottom: "map32_8_conv2"
  top: "map32_8_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_8_eltsum"
  type: "Eltwise"
  bottom: "map32_7_eltsum"
  bottom: "map32_8_conv_end"
  top: "map32_8_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_9_bn_pre"
  type: "BatchNorm"
  bottom: "map32_8_eltsum"
  top: "map32_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_9_bn_pre"
  type: "BatchNorm"
  bottom: "map32_8_eltsum"
  top: "map32_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_9_pre_scale"
  type: "Scale"
  bottom: "map32_9_bn_pre"
  top: "map32_9_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_9_pre_relu"
  type: "ReLU"
  bottom: "map32_9_bn_pre"
  top: "map32_9_bn_pre"
}
layer {
  name: "map32_9_conv1"
  type: "Convolution"
  bottom: "map32_9_bn_pre"
  top: "map32_9_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_9_bn1"
  type: "BatchNorm"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_9_bn1"
  type: "BatchNorm"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_9_scale1"
  type: "Scale"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_9_relu1"
  type: "ReLU"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
}
layer {
  name: "map32_9_conv2"
  type: "Convolution"
  bottom: "map32_9_conv1"
  top: "map32_9_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_9_bn2"
  type: "BatchNorm"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_9_bn2"
  type: "BatchNorm"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_9_scale2"
  type: "Scale"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_9_relu2"
  type: "ReLU"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
}
layer {
  name: "map32_9_conv_end"
  type: "Convolution"
  bottom: "map32_9_conv2"
  top: "map32_9_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_9_eltsum"
  type: "Eltwise"
  bottom: "map32_8_eltsum"
  bottom: "map32_9_conv_end"
  top: "map32_9_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_10_bn_pre"
  type: "BatchNorm"
  bottom: "map32_9_eltsum"
  top: "map32_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_10_bn_pre"
  type: "BatchNorm"
  bottom: "map32_9_eltsum"
  top: "map32_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_10_pre_scale"
  type: "Scale"
  bottom: "map32_10_bn_pre"
  top: "map32_10_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_10_pre_relu"
  type: "ReLU"
  bottom: "map32_10_bn_pre"
  top: "map32_10_bn_pre"
}
layer {
  name: "map32_10_conv1"
  type: "Convolution"
  bottom: "map32_10_bn_pre"
  top: "map32_10_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_10_bn1"
  type: "BatchNorm"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_10_bn1"
  type: "BatchNorm"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_10_scale1"
  type: "Scale"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_10_relu1"
  type: "ReLU"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
}
layer {
  name: "map32_10_conv2"
  type: "Convolution"
  bottom: "map32_10_conv1"
  top: "map32_10_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_10_bn2"
  type: "BatchNorm"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_10_bn2"
  type: "BatchNorm"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_10_scale2"
  type: "Scale"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_10_relu2"
  type: "ReLU"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
}
layer {
  name: "map32_10_conv_end"
  type: "Convolution"
  bottom: "map32_10_conv2"
  top: "map32_10_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_10_eltsum"
  type: "Eltwise"
  bottom: "map32_9_eltsum"
  bottom: "map32_10_conv_end"
  top: "map32_10_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_11_bn_pre"
  type: "BatchNorm"
  bottom: "map32_10_eltsum"
  top: "map32_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_11_bn_pre"
  type: "BatchNorm"
  bottom: "map32_10_eltsum"
  top: "map32_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_11_pre_scale"
  type: "Scale"
  bottom: "map32_11_bn_pre"
  top: "map32_11_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_11_pre_relu"
  type: "ReLU"
  bottom: "map32_11_bn_pre"
  top: "map32_11_bn_pre"
}
layer {
  name: "map32_11_conv1"
  type: "Convolution"
  bottom: "map32_11_bn_pre"
  top: "map32_11_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_11_bn1"
  type: "BatchNorm"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_11_bn1"
  type: "BatchNorm"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_11_scale1"
  type: "Scale"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_11_relu1"
  type: "ReLU"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
}
layer {
  name: "map32_11_conv2"
  type: "Convolution"
  bottom: "map32_11_conv1"
  top: "map32_11_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_11_bn2"
  type: "BatchNorm"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_11_bn2"
  type: "BatchNorm"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_11_scale2"
  type: "Scale"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_11_relu2"
  type: "ReLU"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
}
layer {
  name: "map32_11_conv_end"
  type: "Convolution"
  bottom: "map32_11_conv2"
  top: "map32_11_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_11_eltsum"
  type: "Eltwise"
  bottom: "map32_10_eltsum"
  bottom: "map32_11_conv_end"
  top: "map32_11_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_12_bn_pre"
  type: "BatchNorm"
  bottom: "map32_11_eltsum"
  top: "map32_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_12_bn_pre"
  type: "BatchNorm"
  bottom: "map32_11_eltsum"
  top: "map32_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_12_pre_scale"
  type: "Scale"
  bottom: "map32_12_bn_pre"
  top: "map32_12_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_12_pre_relu"
  type: "ReLU"
  bottom: "map32_12_bn_pre"
  top: "map32_12_bn_pre"
}
layer {
  name: "map32_12_conv1"
  type: "Convolution"
  bottom: "map32_12_bn_pre"
  top: "map32_12_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_12_bn1"
  type: "BatchNorm"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_12_bn1"
  type: "BatchNorm"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_12_scale1"
  type: "Scale"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_12_relu1"
  type: "ReLU"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
}
layer {
  name: "map32_12_conv2"
  type: "Convolution"
  bottom: "map32_12_conv1"
  top: "map32_12_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_12_bn2"
  type: "BatchNorm"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_12_bn2"
  type: "BatchNorm"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_12_scale2"
  type: "Scale"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_12_relu2"
  type: "ReLU"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
}
layer {
  name: "map32_12_conv_end"
  type: "Convolution"
  bottom: "map32_12_conv2"
  top: "map32_12_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_12_eltsum"
  type: "Eltwise"
  bottom: "map32_11_eltsum"
  bottom: "map32_12_conv_end"
  top: "map32_12_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_13_bn_pre"
  type: "BatchNorm"
  bottom: "map32_12_eltsum"
  top: "map32_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_13_bn_pre"
  type: "BatchNorm"
  bottom: "map32_12_eltsum"
  top: "map32_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_13_pre_scale"
  type: "Scale"
  bottom: "map32_13_bn_pre"
  top: "map32_13_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_13_pre_relu"
  type: "ReLU"
  bottom: "map32_13_bn_pre"
  top: "map32_13_bn_pre"
}
layer {
  name: "map32_13_conv1"
  type: "Convolution"
  bottom: "map32_13_bn_pre"
  top: "map32_13_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_13_bn1"
  type: "BatchNorm"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_13_bn1"
  type: "BatchNorm"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_13_scale1"
  type: "Scale"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_13_relu1"
  type: "ReLU"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
}
layer {
  name: "map32_13_conv2"
  type: "Convolution"
  bottom: "map32_13_conv1"
  top: "map32_13_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_13_bn2"
  type: "BatchNorm"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_13_bn2"
  type: "BatchNorm"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_13_scale2"
  type: "Scale"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_13_relu2"
  type: "ReLU"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
}
layer {
  name: "map32_13_conv_end"
  type: "Convolution"
  bottom: "map32_13_conv2"
  top: "map32_13_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_13_eltsum"
  type: "Eltwise"
  bottom: "map32_12_eltsum"
  bottom: "map32_13_conv_end"
  top: "map32_13_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_14_bn_pre"
  type: "BatchNorm"
  bottom: "map32_13_eltsum"
  top: "map32_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_14_bn_pre"
  type: "BatchNorm"
  bottom: "map32_13_eltsum"
  top: "map32_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_14_pre_scale"
  type: "Scale"
  bottom: "map32_14_bn_pre"
  top: "map32_14_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_14_pre_relu"
  type: "ReLU"
  bottom: "map32_14_bn_pre"
  top: "map32_14_bn_pre"
}
layer {
  name: "map32_14_conv1"
  type: "Convolution"
  bottom: "map32_14_bn_pre"
  top: "map32_14_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_14_bn1"
  type: "BatchNorm"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_14_bn1"
  type: "BatchNorm"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_14_scale1"
  type: "Scale"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_14_relu1"
  type: "ReLU"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
}
layer {
  name: "map32_14_conv2"
  type: "Convolution"
  bottom: "map32_14_conv1"
  top: "map32_14_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_14_bn2"
  type: "BatchNorm"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_14_bn2"
  type: "BatchNorm"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_14_scale2"
  type: "Scale"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_14_relu2"
  type: "ReLU"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
}
layer {
  name: "map32_14_conv_end"
  type: "Convolution"
  bottom: "map32_14_conv2"
  top: "map32_14_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_14_eltsum"
  type: "Eltwise"
  bottom: "map32_13_eltsum"
  bottom: "map32_14_conv_end"
  top: "map32_14_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_15_bn_pre"
  type: "BatchNorm"
  bottom: "map32_14_eltsum"
  top: "map32_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_15_bn_pre"
  type: "BatchNorm"
  bottom: "map32_14_eltsum"
  top: "map32_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_15_pre_scale"
  type: "Scale"
  bottom: "map32_15_bn_pre"
  top: "map32_15_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_15_pre_relu"
  type: "ReLU"
  bottom: "map32_15_bn_pre"
  top: "map32_15_bn_pre"
}
layer {
  name: "map32_15_conv1"
  type: "Convolution"
  bottom: "map32_15_bn_pre"
  top: "map32_15_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_15_bn1"
  type: "BatchNorm"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_15_bn1"
  type: "BatchNorm"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_15_scale1"
  type: "Scale"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_15_relu1"
  type: "ReLU"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
}
layer {
  name: "map32_15_conv2"
  type: "Convolution"
  bottom: "map32_15_conv1"
  top: "map32_15_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_15_bn2"
  type: "BatchNorm"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_15_bn2"
  type: "BatchNorm"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_15_scale2"
  type: "Scale"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_15_relu2"
  type: "ReLU"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
}
layer {
  name: "map32_15_conv_end"
  type: "Convolution"
  bottom: "map32_15_conv2"
  top: "map32_15_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_15_eltsum"
  type: "Eltwise"
  bottom: "map32_14_eltsum"
  bottom: "map32_15_conv_end"
  top: "map32_15_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_16_bn_pre"
  type: "BatchNorm"
  bottom: "map32_15_eltsum"
  top: "map32_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_16_bn_pre"
  type: "BatchNorm"
  bottom: "map32_15_eltsum"
  top: "map32_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_16_pre_scale"
  type: "Scale"
  bottom: "map32_16_bn_pre"
  top: "map32_16_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_16_pre_relu"
  type: "ReLU"
  bottom: "map32_16_bn_pre"
  top: "map32_16_bn_pre"
}
layer {
  name: "map32_16_conv1"
  type: "Convolution"
  bottom: "map32_16_bn_pre"
  top: "map32_16_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_16_bn1"
  type: "BatchNorm"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_16_bn1"
  type: "BatchNorm"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_16_scale1"
  type: "Scale"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_16_relu1"
  type: "ReLU"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
}
layer {
  name: "map32_16_conv2"
  type: "Convolution"
  bottom: "map32_16_conv1"
  top: "map32_16_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_16_bn2"
  type: "BatchNorm"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_16_bn2"
  type: "BatchNorm"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_16_scale2"
  type: "Scale"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_16_relu2"
  type: "ReLU"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
}
layer {
  name: "map32_16_conv_end"
  type: "Convolution"
  bottom: "map32_16_conv2"
  top: "map32_16_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_16_eltsum"
  type: "Eltwise"
  bottom: "map32_15_eltsum"
  bottom: "map32_16_conv_end"
  top: "map32_16_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_17_bn_pre"
  type: "BatchNorm"
  bottom: "map32_16_eltsum"
  top: "map32_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_17_bn_pre"
  type: "BatchNorm"
  bottom: "map32_16_eltsum"
  top: "map32_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_17_pre_scale"
  type: "Scale"
  bottom: "map32_17_bn_pre"
  top: "map32_17_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_17_pre_relu"
  type: "ReLU"
  bottom: "map32_17_bn_pre"
  top: "map32_17_bn_pre"
}
layer {
  name: "map32_17_conv1"
  type: "Convolution"
  bottom: "map32_17_bn_pre"
  top: "map32_17_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_17_bn1"
  type: "BatchNorm"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_17_bn1"
  type: "BatchNorm"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_17_scale1"
  type: "Scale"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_17_relu1"
  type: "ReLU"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
}
layer {
  name: "map32_17_conv2"
  type: "Convolution"
  bottom: "map32_17_conv1"
  top: "map32_17_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_17_bn2"
  type: "BatchNorm"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_17_bn2"
  type: "BatchNorm"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_17_scale2"
  type: "Scale"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_17_relu2"
  type: "ReLU"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
}
layer {
  name: "map32_17_conv_end"
  type: "Convolution"
  bottom: "map32_17_conv2"
  top: "map32_17_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_17_eltsum"
  type: "Eltwise"
  bottom: "map32_16_eltsum"
  bottom: "map32_17_conv_end"
  top: "map32_17_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_18_bn_pre"
  type: "BatchNorm"
  bottom: "map32_17_eltsum"
  top: "map32_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_18_bn_pre"
  type: "BatchNorm"
  bottom: "map32_17_eltsum"
  top: "map32_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_18_pre_scale"
  type: "Scale"
  bottom: "map32_18_bn_pre"
  top: "map32_18_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_18_pre_relu"
  type: "ReLU"
  bottom: "map32_18_bn_pre"
  top: "map32_18_bn_pre"
}
layer {
  name: "map32_18_conv1"
  type: "Convolution"
  bottom: "map32_18_bn_pre"
  top: "map32_18_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_18_bn1"
  type: "BatchNorm"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_18_bn1"
  type: "BatchNorm"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_18_scale1"
  type: "Scale"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_18_relu1"
  type: "ReLU"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
}
layer {
  name: "map32_18_conv2"
  type: "Convolution"
  bottom: "map32_18_conv1"
  top: "map32_18_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_18_bn2"
  type: "BatchNorm"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_18_bn2"
  type: "BatchNorm"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_18_scale2"
  type: "Scale"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_18_relu2"
  type: "ReLU"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
}
layer {
  name: "map32_18_conv_end"
  type: "Convolution"
  bottom: "map32_18_conv2"
  top: "map32_18_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_18_eltsum"
  type: "Eltwise"
  bottom: "map32_17_eltsum"
  bottom: "map32_18_conv_end"
  top: "map32_18_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_1_bn_pre"
  type: "BatchNorm"
  bottom: "map32_18_eltsum"
  top: "map64_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn_pre"
  type: "BatchNorm"
  bottom: "map32_18_eltsum"
  top: "map64_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_pre_scale"
  type: "Scale"
  bottom: "map64_1_bn_pre"
  top: "map64_1_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_pre_relu"
  type: "ReLU"
  bottom: "map64_1_bn_pre"
  top: "map64_1_bn_pre"
}
layer {
  name: "map64_1_conv1"
  type: "Convolution"
  bottom: "map64_1_bn_pre"
  top: "map64_1_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn1"
  type: "BatchNorm"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn1"
  type: "BatchNorm"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale1"
  type: "Scale"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_relu1"
  type: "ReLU"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
}
layer {
  name: "map64_1_conv2"
  type: "Convolution"
  bottom: "map64_1_conv1"
  top: "map64_1_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn2"
  type: "BatchNorm"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn2"
  type: "BatchNorm"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale2"
  type: "Scale"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_relu2"
  type: "ReLU"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
}
layer {
  name: "map64_1_conv_end"
  type: "Convolution"
  bottom: "map64_1_conv2"
  top: "map64_1_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "map64_1_bn_pre"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_eltsum"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "map64_1_conv_end"
  top: "map64_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_2_bn_pre"
  type: "BatchNorm"
  bottom: "map64_1_eltsum"
  top: "map64_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn_pre"
  type: "BatchNorm"
  bottom: "map64_1_eltsum"
  top: "map64_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_pre_scale"
  type: "Scale"
  bottom: "map64_2_bn_pre"
  top: "map64_2_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_pre_relu"
  type: "ReLU"
  bottom: "map64_2_bn_pre"
  top: "map64_2_bn_pre"
}
layer {
  name: "map64_2_conv1"
  type: "Convolution"
  bottom: "map64_2_bn_pre"
  top: "map64_2_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_bn1"
  type: "BatchNorm"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn1"
  type: "BatchNorm"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_scale1"
  type: "Scale"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_relu1"
  type: "ReLU"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
}
layer {
  name: "map64_2_conv2"
  type: "Convolution"
  bottom: "map64_2_conv1"
  top: "map64_2_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_bn2"
  type: "BatchNorm"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn2"
  type: "BatchNorm"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_scale2"
  type: "Scale"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_relu2"
  type: "ReLU"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
}
layer {
  name: "map64_2_conv_end"
  type: "Convolution"
  bottom: "map64_2_conv2"
  top: "map64_2_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_eltsum"
  type: "Eltwise"
  bottom: "map64_1_eltsum"
  bottom: "map64_2_conv_end"
  top: "map64_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_3_bn_pre"
  type: "BatchNorm"
  bottom: "map64_2_eltsum"
  top: "map64_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn_pre"
  type: "BatchNorm"
  bottom: "map64_2_eltsum"
  top: "map64_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_pre_scale"
  type: "Scale"
  bottom: "map64_3_bn_pre"
  top: "map64_3_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_pre_relu"
  type: "ReLU"
  bottom: "map64_3_bn_pre"
  top: "map64_3_bn_pre"
}
layer {
  name: "map64_3_conv1"
  type: "Convolution"
  bottom: "map64_3_bn_pre"
  top: "map64_3_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_bn1"
  type: "BatchNorm"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn1"
  type: "BatchNorm"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_scale1"
  type: "Scale"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_relu1"
  type: "ReLU"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
}
layer {
  name: "map64_3_conv2"
  type: "Convolution"
  bottom: "map64_3_conv1"
  top: "map64_3_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_bn2"
  type: "BatchNorm"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn2"
  type: "BatchNorm"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_scale2"
  type: "Scale"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_relu2"
  type: "ReLU"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
}
layer {
  name: "map64_3_conv_end"
  type: "Convolution"
  bottom: "map64_3_conv2"
  top: "map64_3_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_eltsum"
  type: "Eltwise"
  bottom: "map64_2_eltsum"
  bottom: "map64_3_conv_end"
  top: "map64_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_4_bn_pre"
  type: "BatchNorm"
  bottom: "map64_3_eltsum"
  top: "map64_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_4_bn_pre"
  type: "BatchNorm"
  bottom: "map64_3_eltsum"
  top: "map64_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_4_pre_scale"
  type: "Scale"
  bottom: "map64_4_bn_pre"
  top: "map64_4_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_4_pre_relu"
  type: "ReLU"
  bottom: "map64_4_bn_pre"
  top: "map64_4_bn_pre"
}
layer {
  name: "map64_4_conv1"
  type: "Convolution"
  bottom: "map64_4_bn_pre"
  top: "map64_4_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_4_bn1"
  type: "BatchNorm"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_4_bn1"
  type: "BatchNorm"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_4_scale1"
  type: "Scale"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_4_relu1"
  type: "ReLU"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
}
layer {
  name: "map64_4_conv2"
  type: "Convolution"
  bottom: "map64_4_conv1"
  top: "map64_4_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_4_bn2"
  type: "BatchNorm"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_4_bn2"
  type: "BatchNorm"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_4_scale2"
  type: "Scale"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_4_relu2"
  type: "ReLU"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
}
layer {
  name: "map64_4_conv_end"
  type: "Convolution"
  bottom: "map64_4_conv2"
  top: "map64_4_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_4_eltsum"
  type: "Eltwise"
  bottom: "map64_3_eltsum"
  bottom: "map64_4_conv_end"
  top: "map64_4_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_5_bn_pre"
  type: "BatchNorm"
  bottom: "map64_4_eltsum"
  top: "map64_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_5_bn_pre"
  type: "BatchNorm"
  bottom: "map64_4_eltsum"
  top: "map64_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_5_pre_scale"
  type: "Scale"
  bottom: "map64_5_bn_pre"
  top: "map64_5_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_5_pre_relu"
  type: "ReLU"
  bottom: "map64_5_bn_pre"
  top: "map64_5_bn_pre"
}
layer {
  name: "map64_5_conv1"
  type: "Convolution"
  bottom: "map64_5_bn_pre"
  top: "map64_5_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_5_bn1"
  type: "BatchNorm"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_5_bn1"
  type: "BatchNorm"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_5_scale1"
  type: "Scale"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_5_relu1"
  type: "ReLU"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
}
layer {
  name: "map64_5_conv2"
  type: "Convolution"
  bottom: "map64_5_conv1"
  top: "map64_5_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_5_bn2"
  type: "BatchNorm"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_5_bn2"
  type: "BatchNorm"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_5_scale2"
  type: "Scale"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_5_relu2"
  type: "ReLU"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
}
layer {
  name: "map64_5_conv_end"
  type: "Convolution"
  bottom: "map64_5_conv2"
  top: "map64_5_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_5_eltsum"
  type: "Eltwise"
  bottom: "map64_4_eltsum"
  bottom: "map64_5_conv_end"
  top: "map64_5_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_6_bn_pre"
  type: "BatchNorm"
  bottom: "map64_5_eltsum"
  top: "map64_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_6_bn_pre"
  type: "BatchNorm"
  bottom: "map64_5_eltsum"
  top: "map64_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_6_pre_scale"
  type: "Scale"
  bottom: "map64_6_bn_pre"
  top: "map64_6_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_6_pre_relu"
  type: "ReLU"
  bottom: "map64_6_bn_pre"
  top: "map64_6_bn_pre"
}
layer {
  name: "map64_6_conv1"
  type: "Convolution"
  bottom: "map64_6_bn_pre"
  top: "map64_6_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_6_bn1"
  type: "BatchNorm"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_6_bn1"
  type: "BatchNorm"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_6_scale1"
  type: "Scale"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_6_relu1"
  type: "ReLU"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
}
layer {
  name: "map64_6_conv2"
  type: "Convolution"
  bottom: "map64_6_conv1"
  top: "map64_6_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_6_bn2"
  type: "BatchNorm"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_6_bn2"
  type: "BatchNorm"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_6_scale2"
  type: "Scale"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_6_relu2"
  type: "ReLU"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
}
layer {
  name: "map64_6_conv_end"
  type: "Convolution"
  bottom: "map64_6_conv2"
  top: "map64_6_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_6_eltsum"
  type: "Eltwise"
  bottom: "map64_5_eltsum"
  bottom: "map64_6_conv_end"
  top: "map64_6_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_7_bn_pre"
  type: "BatchNorm"
  bottom: "map64_6_eltsum"
  top: "map64_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_7_bn_pre"
  type: "BatchNorm"
  bottom: "map64_6_eltsum"
  top: "map64_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_7_pre_scale"
  type: "Scale"
  bottom: "map64_7_bn_pre"
  top: "map64_7_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_7_pre_relu"
  type: "ReLU"
  bottom: "map64_7_bn_pre"
  top: "map64_7_bn_pre"
}
layer {
  name: "map64_7_conv1"
  type: "Convolution"
  bottom: "map64_7_bn_pre"
  top: "map64_7_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_7_bn1"
  type: "BatchNorm"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_7_bn1"
  type: "BatchNorm"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_7_scale1"
  type: "Scale"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_7_relu1"
  type: "ReLU"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
}
layer {
  name: "map64_7_conv2"
  type: "Convolution"
  bottom: "map64_7_conv1"
  top: "map64_7_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_7_bn2"
  type: "BatchNorm"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_7_bn2"
  type: "BatchNorm"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_7_scale2"
  type: "Scale"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_7_relu2"
  type: "ReLU"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
}
layer {
  name: "map64_7_conv_end"
  type: "Convolution"
  bottom: "map64_7_conv2"
  top: "map64_7_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_7_eltsum"
  type: "Eltwise"
  bottom: "map64_6_eltsum"
  bottom: "map64_7_conv_end"
  top: "map64_7_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_8_bn_pre"
  type: "BatchNorm"
  bottom: "map64_7_eltsum"
  top: "map64_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_8_bn_pre"
  type: "BatchNorm"
  bottom: "map64_7_eltsum"
  top: "map64_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_8_pre_scale"
  type: "Scale"
  bottom: "map64_8_bn_pre"
  top: "map64_8_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_8_pre_relu"
  type: "ReLU"
  bottom: "map64_8_bn_pre"
  top: "map64_8_bn_pre"
}
layer {
  name: "map64_8_conv1"
  type: "Convolution"
  bottom: "map64_8_bn_pre"
  top: "map64_8_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_8_bn1"
  type: "BatchNorm"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_8_bn1"
  type: "BatchNorm"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_8_scale1"
  type: "Scale"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_8_relu1"
  type: "ReLU"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
}
layer {
  name: "map64_8_conv2"
  type: "Convolution"
  bottom: "map64_8_conv1"
  top: "map64_8_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_8_bn2"
  type: "BatchNorm"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_8_bn2"
  type: "BatchNorm"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_8_scale2"
  type: "Scale"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_8_relu2"
  type: "ReLU"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
}
layer {
  name: "map64_8_conv_end"
  type: "Convolution"
  bottom: "map64_8_conv2"
  top: "map64_8_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_8_eltsum"
  type: "Eltwise"
  bottom: "map64_7_eltsum"
  bottom: "map64_8_conv_end"
  top: "map64_8_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_9_bn_pre"
  type: "BatchNorm"
  bottom: "map64_8_eltsum"
  top: "map64_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_9_bn_pre"
  type: "BatchNorm"
  bottom: "map64_8_eltsum"
  top: "map64_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_9_pre_scale"
  type: "Scale"
  bottom: "map64_9_bn_pre"
  top: "map64_9_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_9_pre_relu"
  type: "ReLU"
  bottom: "map64_9_bn_pre"
  top: "map64_9_bn_pre"
}
layer {
  name: "map64_9_conv1"
  type: "Convolution"
  bottom: "map64_9_bn_pre"
  top: "map64_9_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_9_bn1"
  type: "BatchNorm"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_9_bn1"
  type: "BatchNorm"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_9_scale1"
  type: "Scale"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_9_relu1"
  type: "ReLU"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
}
layer {
  name: "map64_9_conv2"
  type: "Convolution"
  bottom: "map64_9_conv1"
  top: "map64_9_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_9_bn2"
  type: "BatchNorm"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_9_bn2"
  type: "BatchNorm"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_9_scale2"
  type: "Scale"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_9_relu2"
  type: "ReLU"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
}
layer {
  name: "map64_9_conv_end"
  type: "Convolution"
  bottom: "map64_9_conv2"
  top: "map64_9_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_9_eltsum"
  type: "Eltwise"
  bottom: "map64_8_eltsum"
  bottom: "map64_9_conv_end"
  top: "map64_9_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_10_bn_pre"
  type: "BatchNorm"
  bottom: "map64_9_eltsum"
  top: "map64_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_10_bn_pre"
  type: "BatchNorm"
  bottom: "map64_9_eltsum"
  top: "map64_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_10_pre_scale"
  type: "Scale"
  bottom: "map64_10_bn_pre"
  top: "map64_10_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_10_pre_relu"
  type: "ReLU"
  bottom: "map64_10_bn_pre"
  top: "map64_10_bn_pre"
}
layer {
  name: "map64_10_conv1"
  type: "Convolution"
  bottom: "map64_10_bn_pre"
  top: "map64_10_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_10_bn1"
  type: "BatchNorm"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_10_bn1"
  type: "BatchNorm"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_10_scale1"
  type: "Scale"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_10_relu1"
  type: "ReLU"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
}
layer {
  name: "map64_10_conv2"
  type: "Convolution"
  bottom: "map64_10_conv1"
  top: "map64_10_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_10_bn2"
  type: "BatchNorm"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_10_bn2"
  type: "BatchNorm"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_10_scale2"
  type: "Scale"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_10_relu2"
  type: "ReLU"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
}
layer {
  name: "map64_10_conv_end"
  type: "Convolution"
  bottom: "map64_10_conv2"
  top: "map64_10_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_10_eltsum"
  type: "Eltwise"
  bottom: "map64_9_eltsum"
  bottom: "map64_10_conv_end"
  top: "map64_10_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_11_bn_pre"
  type: "BatchNorm"
  bottom: "map64_10_eltsum"
  top: "map64_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_11_bn_pre"
  type: "BatchNorm"
  bottom: "map64_10_eltsum"
  top: "map64_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_11_pre_scale"
  type: "Scale"
  bottom: "map64_11_bn_pre"
  top: "map64_11_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_11_pre_relu"
  type: "ReLU"
  bottom: "map64_11_bn_pre"
  top: "map64_11_bn_pre"
}
layer {
  name: "map64_11_conv1"
  type: "Convolution"
  bottom: "map64_11_bn_pre"
  top: "map64_11_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_11_bn1"
  type: "BatchNorm"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_11_bn1"
  type: "BatchNorm"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_11_scale1"
  type: "Scale"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_11_relu1"
  type: "ReLU"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
}
layer {
  name: "map64_11_conv2"
  type: "Convolution"
  bottom: "map64_11_conv1"
  top: "map64_11_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_11_bn2"
  type: "BatchNorm"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_11_bn2"
  type: "BatchNorm"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_11_scale2"
  type: "Scale"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_11_relu2"
  type: "ReLU"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
}
layer {
  name: "map64_11_conv_end"
  type: "Convolution"
  bottom: "map64_11_conv2"
  top: "map64_11_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_11_eltsum"
  type: "Eltwise"
  bottom: "map64_10_eltsum"
  bottom: "map64_11_conv_end"
  top: "map64_11_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_12_bn_pre"
  type: "BatchNorm"
  bottom: "map64_11_eltsum"
  top: "map64_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_12_bn_pre"
  type: "BatchNorm"
  bottom: "map64_11_eltsum"
  top: "map64_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_12_pre_scale"
  type: "Scale"
  bottom: "map64_12_bn_pre"
  top: "map64_12_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_12_pre_relu"
  type: "ReLU"
  bottom: "map64_12_bn_pre"
  top: "map64_12_bn_pre"
}
layer {
  name: "map64_12_conv1"
  type: "Convolution"
  bottom: "map64_12_bn_pre"
  top: "map64_12_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_12_bn1"
  type: "BatchNorm"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_12_bn1"
  type: "BatchNorm"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_12_scale1"
  type: "Scale"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_12_relu1"
  type: "ReLU"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
}
layer {
  name: "map64_12_conv2"
  type: "Convolution"
  bottom: "map64_12_conv1"
  top: "map64_12_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_12_bn2"
  type: "BatchNorm"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_12_bn2"
  type: "BatchNorm"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_12_scale2"
  type: "Scale"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_12_relu2"
  type: "ReLU"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
}
layer {
  name: "map64_12_conv_end"
  type: "Convolution"
  bottom: "map64_12_conv2"
  top: "map64_12_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_12_eltsum"
  type: "Eltwise"
  bottom: "map64_11_eltsum"
  bottom: "map64_12_conv_end"
  top: "map64_12_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_13_bn_pre"
  type: "BatchNorm"
  bottom: "map64_12_eltsum"
  top: "map64_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_13_bn_pre"
  type: "BatchNorm"
  bottom: "map64_12_eltsum"
  top: "map64_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_13_pre_scale"
  type: "Scale"
  bottom: "map64_13_bn_pre"
  top: "map64_13_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_13_pre_relu"
  type: "ReLU"
  bottom: "map64_13_bn_pre"
  top: "map64_13_bn_pre"
}
layer {
  name: "map64_13_conv1"
  type: "Convolution"
  bottom: "map64_13_bn_pre"
  top: "map64_13_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_13_bn1"
  type: "BatchNorm"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_13_bn1"
  type: "BatchNorm"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_13_scale1"
  type: "Scale"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_13_relu1"
  type: "ReLU"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
}
layer {
  name: "map64_13_conv2"
  type: "Convolution"
  bottom: "map64_13_conv1"
  top: "map64_13_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_13_bn2"
  type: "BatchNorm"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_13_bn2"
  type: "BatchNorm"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_13_scale2"
  type: "Scale"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_13_relu2"
  type: "ReLU"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
}
layer {
  name: "map64_13_conv_end"
  type: "Convolution"
  bottom: "map64_13_conv2"
  top: "map64_13_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_13_eltsum"
  type: "Eltwise"
  bottom: "map64_12_eltsum"
  bottom: "map64_13_conv_end"
  top: "map64_13_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_14_bn_pre"
  type: "BatchNorm"
  bottom: "map64_13_eltsum"
  top: "map64_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_14_bn_pre"
  type: "BatchNorm"
  bottom: "map64_13_eltsum"
  top: "map64_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_14_pre_scale"
  type: "Scale"
  bottom: "map64_14_bn_pre"
  top: "map64_14_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_14_pre_relu"
  type: "ReLU"
  bottom: "map64_14_bn_pre"
  top: "map64_14_bn_pre"
}
layer {
  name: "map64_14_conv1"
  type: "Convolution"
  bottom: "map64_14_bn_pre"
  top: "map64_14_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_14_bn1"
  type: "BatchNorm"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_14_bn1"
  type: "BatchNorm"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_14_scale1"
  type: "Scale"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_14_relu1"
  type: "ReLU"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
}
layer {
  name: "map64_14_conv2"
  type: "Convolution"
  bottom: "map64_14_conv1"
  top: "map64_14_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_14_bn2"
  type: "BatchNorm"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_14_bn2"
  type: "BatchNorm"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_14_scale2"
  type: "Scale"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_14_relu2"
  type: "ReLU"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
}
layer {
  name: "map64_14_conv_end"
  type: "Convolution"
  bottom: "map64_14_conv2"
  top: "map64_14_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_14_eltsum"
  type: "Eltwise"
  bottom: "map64_13_eltsum"
  bottom: "map64_14_conv_end"
  top: "map64_14_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_15_bn_pre"
  type: "BatchNorm"
  bottom: "map64_14_eltsum"
  top: "map64_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_15_bn_pre"
  type: "BatchNorm"
  bottom: "map64_14_eltsum"
  top: "map64_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_15_pre_scale"
  type: "Scale"
  bottom: "map64_15_bn_pre"
  top: "map64_15_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_15_pre_relu"
  type: "ReLU"
  bottom: "map64_15_bn_pre"
  top: "map64_15_bn_pre"
}
layer {
  name: "map64_15_conv1"
  type: "Convolution"
  bottom: "map64_15_bn_pre"
  top: "map64_15_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_15_bn1"
  type: "BatchNorm"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_15_bn1"
  type: "BatchNorm"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_15_scale1"
  type: "Scale"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_15_relu1"
  type: "ReLU"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
}
layer {
  name: "map64_15_conv2"
  type: "Convolution"
  bottom: "map64_15_conv1"
  top: "map64_15_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_15_bn2"
  type: "BatchNorm"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_15_bn2"
  type: "BatchNorm"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_15_scale2"
  type: "Scale"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_15_relu2"
  type: "ReLU"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
}
layer {
  name: "map64_15_conv_end"
  type: "Convolution"
  bottom: "map64_15_conv2"
  top: "map64_15_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_15_eltsum"
  type: "Eltwise"
  bottom: "map64_14_eltsum"
  bottom: "map64_15_conv_end"
  top: "map64_15_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_16_bn_pre"
  type: "BatchNorm"
  bottom: "map64_15_eltsum"
  top: "map64_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_16_bn_pre"
  type: "BatchNorm"
  bottom: "map64_15_eltsum"
  top: "map64_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_16_pre_scale"
  type: "Scale"
  bottom: "map64_16_bn_pre"
  top: "map64_16_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_16_pre_relu"
  type: "ReLU"
  bottom: "map64_16_bn_pre"
  top: "map64_16_bn_pre"
}
layer {
  name: "map64_16_conv1"
  type: "Convolution"
  bottom: "map64_16_bn_pre"
  top: "map64_16_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_16_bn1"
  type: "BatchNorm"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_16_bn1"
  type: "BatchNorm"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_16_scale1"
  type: "Scale"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_16_relu1"
  type: "ReLU"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
}
layer {
  name: "map64_16_conv2"
  type: "Convolution"
  bottom: "map64_16_conv1"
  top: "map64_16_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_16_bn2"
  type: "BatchNorm"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_16_bn2"
  type: "BatchNorm"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_16_scale2"
  type: "Scale"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_16_relu2"
  type: "ReLU"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
}
layer {
  name: "map64_16_conv_end"
  type: "Convolution"
  bottom: "map64_16_conv2"
  top: "map64_16_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_16_eltsum"
  type: "Eltwise"
  bottom: "map64_15_eltsum"
  bottom: "map64_16_conv_end"
  top: "map64_16_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_17_bn_pre"
  type: "BatchNorm"
  bottom: "map64_16_eltsum"
  top: "map64_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_17_bn_pre"
  type: "BatchNorm"
  bottom: "map64_16_eltsum"
  top: "map64_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_17_pre_scale"
  type: "Scale"
  bottom: "map64_17_bn_pre"
  top: "map64_17_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_17_pre_relu"
  type: "ReLU"
  bottom: "map64_17_bn_pre"
  top: "map64_17_bn_pre"
}
layer {
  name: "map64_17_conv1"
  type: "Convolution"
  bottom: "map64_17_bn_pre"
  top: "map64_17_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_17_bn1"
  type: "BatchNorm"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_17_bn1"
  type: "BatchNorm"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_17_scale1"
  type: "Scale"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_17_relu1"
  type: "ReLU"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
}
layer {
  name: "map64_17_conv2"
  type: "Convolution"
  bottom: "map64_17_conv1"
  top: "map64_17_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_17_bn2"
  type: "BatchNorm"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_17_bn2"
  type: "BatchNorm"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_17_scale2"
  type: "Scale"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_17_relu2"
  type: "ReLU"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
}
layer {
  name: "map64_17_conv_end"
  type: "Convolution"
  bottom: "map64_17_conv2"
  top: "map64_17_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_17_eltsum"
  type: "Eltwise"
  bottom: "map64_16_eltsum"
  bottom: "map64_17_conv_end"
  top: "map64_17_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_18_bn_pre"
  type: "BatchNorm"
  bottom: "map64_17_eltsum"
  top: "map64_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_18_bn_pre"
  type: "BatchNorm"
  bottom: "map64_17_eltsum"
  top: "map64_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_18_pre_scale"
  type: "Scale"
  bottom: "map64_18_bn_pre"
  top: "map64_18_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_18_pre_relu"
  type: "ReLU"
  bottom: "map64_18_bn_pre"
  top: "map64_18_bn_pre"
}
layer {
  name: "map64_18_conv1"
  type: "Convolution"
  bottom: "map64_18_bn_pre"
  top: "map64_18_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_18_bn1"
  type: "BatchNorm"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_18_bn1"
  type: "BatchNorm"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_18_scale1"
  type: "Scale"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_18_relu1"
  type: "ReLU"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
}
layer {
  name: "map64_18_conv2"
  type: "Convolution"
  bottom: "map64_18_conv1"
  top: "map64_18_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_18_bn2"
  type: "BatchNorm"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_18_bn2"
  type: "BatchNorm"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_18_scale2"
  type: "Scale"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_18_relu2"
  type: "ReLU"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
}
layer {
  name: "map64_18_conv_end"
  type: "Convolution"
  bottom: "map64_18_conv2"
  top: "map64_18_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_18_eltsum"
  type: "Eltwise"
  bottom: "map64_17_eltsum"
  bottom: "map64_18_conv_end"
  top: "map64_18_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BN_end"
  type: "BatchNorm"
  bottom: "map64_18_eltsum"
  top: "BN_end"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "BN_end"
  type: "BatchNorm"
  bottom: "map64_18_eltsum"
  top: "BN_end"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale_end"
  type: "Scale"
  bottom: "BN_end"
  top: "BN_end"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_end"
  type: "ReLU"
  bottom: "BN_end"
  top: "BN_end"
}
layer {
  name: "pool_global"
  type: "Pooling"
  bottom: "BN_end"
  top: "pool_global"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "pool_global"
  top: "score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "acc"
}
