layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    mean_file: "./mean"
  }
  data_param {
    source: "./train"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    mean_file: "./mean"
  }
  data_param {
    source: "./test"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv_start"
  type: "Convolution"
  bottom: "data"
  top: "conv_start"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn_pre"
  type: "BatchNorm"
  bottom: "conv_start"
  top: "conv_start"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn_pre"
  type: "BatchNorm"
  bottom: "conv_start"
  top: "conv_start"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_pre_scale"
  type: "Scale"
  bottom: "conv_start"
  top: "conv_start"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_pre_relu"
  type: "ReLU"
  bottom: "conv_start"
  top: "conv_start"
}
layer {
  name: "map16_1_conv1"
  type: "Convolution"
  bottom: "conv_start"
  top: "map16_1_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn1"
  type: "BatchNorm"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn1"
  type: "BatchNorm"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_scale1"
  type: "Scale"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_relu1"
  type: "ReLU"
  bottom: "map16_1_conv1"
  top: "map16_1_conv1"
}
layer {
  name: "map16_1_conv2"
  type: "Convolution"
  bottom: "map16_1_conv1"
  top: "map16_1_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_bn2"
  type: "BatchNorm"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_1_bn2"
  type: "BatchNorm"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_1_scale2"
  type: "Scale"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_1_relu2"
  type: "ReLU"
  bottom: "map16_1_conv2"
  top: "map16_1_conv2"
}
layer {
  name: "map16_1_conv_end"
  type: "Convolution"
  bottom: "map16_1_conv2"
  top: "map16_1_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "conv_start"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_1_eltsum"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "map16_1_conv_end"
  top: "map16_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_2_bn_pre"
  type: "BatchNorm"
  bottom: "map16_1_eltsum"
  top: "map16_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn_pre"
  type: "BatchNorm"
  bottom: "map16_1_eltsum"
  top: "map16_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_pre_scale"
  type: "Scale"
  bottom: "map16_2_bn_pre"
  top: "map16_2_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_pre_relu"
  type: "ReLU"
  bottom: "map16_2_bn_pre"
  top: "map16_2_bn_pre"
}
layer {
  name: "map16_2_conv1"
  type: "Convolution"
  bottom: "map16_2_bn_pre"
  top: "map16_2_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_bn1"
  type: "BatchNorm"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn1"
  type: "BatchNorm"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_scale1"
  type: "Scale"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_relu1"
  type: "ReLU"
  bottom: "map16_2_conv1"
  top: "map16_2_conv1"
}
layer {
  name: "map16_2_conv2"
  type: "Convolution"
  bottom: "map16_2_conv1"
  top: "map16_2_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_bn2"
  type: "BatchNorm"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_2_bn2"
  type: "BatchNorm"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_2_scale2"
  type: "Scale"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_2_relu2"
  type: "ReLU"
  bottom: "map16_2_conv2"
  top: "map16_2_conv2"
}
layer {
  name: "map16_2_conv_end"
  type: "Convolution"
  bottom: "map16_2_conv2"
  top: "map16_2_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_2_eltsum"
  type: "Eltwise"
  bottom: "map16_1_eltsum"
  bottom: "map16_2_conv_end"
  top: "map16_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_3_bn_pre"
  type: "BatchNorm"
  bottom: "map16_2_eltsum"
  top: "map16_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn_pre"
  type: "BatchNorm"
  bottom: "map16_2_eltsum"
  top: "map16_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_pre_scale"
  type: "Scale"
  bottom: "map16_3_bn_pre"
  top: "map16_3_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_pre_relu"
  type: "ReLU"
  bottom: "map16_3_bn_pre"
  top: "map16_3_bn_pre"
}
layer {
  name: "map16_3_conv1"
  type: "Convolution"
  bottom: "map16_3_bn_pre"
  top: "map16_3_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_bn1"
  type: "BatchNorm"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn1"
  type: "BatchNorm"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_scale1"
  type: "Scale"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_relu1"
  type: "ReLU"
  bottom: "map16_3_conv1"
  top: "map16_3_conv1"
}
layer {
  name: "map16_3_conv2"
  type: "Convolution"
  bottom: "map16_3_conv1"
  top: "map16_3_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_bn2"
  type: "BatchNorm"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_3_bn2"
  type: "BatchNorm"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_3_scale2"
  type: "Scale"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_3_relu2"
  type: "ReLU"
  bottom: "map16_3_conv2"
  top: "map16_3_conv2"
}
layer {
  name: "map16_3_conv_end"
  type: "Convolution"
  bottom: "map16_3_conv2"
  top: "map16_3_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_3_eltsum"
  type: "Eltwise"
  bottom: "map16_2_eltsum"
  bottom: "map16_3_conv_end"
  top: "map16_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_4_bn_pre"
  type: "BatchNorm"
  bottom: "map16_3_eltsum"
  top: "map16_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_4_bn_pre"
  type: "BatchNorm"
  bottom: "map16_3_eltsum"
  top: "map16_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_4_pre_scale"
  type: "Scale"
  bottom: "map16_4_bn_pre"
  top: "map16_4_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_4_pre_relu"
  type: "ReLU"
  bottom: "map16_4_bn_pre"
  top: "map16_4_bn_pre"
}
layer {
  name: "map16_4_conv1"
  type: "Convolution"
  bottom: "map16_4_bn_pre"
  top: "map16_4_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_4_bn1"
  type: "BatchNorm"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_4_bn1"
  type: "BatchNorm"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_4_scale1"
  type: "Scale"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_4_relu1"
  type: "ReLU"
  bottom: "map16_4_conv1"
  top: "map16_4_conv1"
}
layer {
  name: "map16_4_conv2"
  type: "Convolution"
  bottom: "map16_4_conv1"
  top: "map16_4_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_4_bn2"
  type: "BatchNorm"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_4_bn2"
  type: "BatchNorm"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_4_scale2"
  type: "Scale"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_4_relu2"
  type: "ReLU"
  bottom: "map16_4_conv2"
  top: "map16_4_conv2"
}
layer {
  name: "map16_4_conv_end"
  type: "Convolution"
  bottom: "map16_4_conv2"
  top: "map16_4_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_4_eltsum"
  type: "Eltwise"
  bottom: "map16_3_eltsum"
  bottom: "map16_4_conv_end"
  top: "map16_4_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_5_bn_pre"
  type: "BatchNorm"
  bottom: "map16_4_eltsum"
  top: "map16_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_5_bn_pre"
  type: "BatchNorm"
  bottom: "map16_4_eltsum"
  top: "map16_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_5_pre_scale"
  type: "Scale"
  bottom: "map16_5_bn_pre"
  top: "map16_5_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_5_pre_relu"
  type: "ReLU"
  bottom: "map16_5_bn_pre"
  top: "map16_5_bn_pre"
}
layer {
  name: "map16_5_conv1"
  type: "Convolution"
  bottom: "map16_5_bn_pre"
  top: "map16_5_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_5_bn1"
  type: "BatchNorm"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_5_bn1"
  type: "BatchNorm"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_5_scale1"
  type: "Scale"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_5_relu1"
  type: "ReLU"
  bottom: "map16_5_conv1"
  top: "map16_5_conv1"
}
layer {
  name: "map16_5_conv2"
  type: "Convolution"
  bottom: "map16_5_conv1"
  top: "map16_5_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_5_bn2"
  type: "BatchNorm"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_5_bn2"
  type: "BatchNorm"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_5_scale2"
  type: "Scale"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_5_relu2"
  type: "ReLU"
  bottom: "map16_5_conv2"
  top: "map16_5_conv2"
}
layer {
  name: "map16_5_conv_end"
  type: "Convolution"
  bottom: "map16_5_conv2"
  top: "map16_5_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_5_eltsum"
  type: "Eltwise"
  bottom: "map16_4_eltsum"
  bottom: "map16_5_conv_end"
  top: "map16_5_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_6_bn_pre"
  type: "BatchNorm"
  bottom: "map16_5_eltsum"
  top: "map16_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_6_bn_pre"
  type: "BatchNorm"
  bottom: "map16_5_eltsum"
  top: "map16_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_6_pre_scale"
  type: "Scale"
  bottom: "map16_6_bn_pre"
  top: "map16_6_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_6_pre_relu"
  type: "ReLU"
  bottom: "map16_6_bn_pre"
  top: "map16_6_bn_pre"
}
layer {
  name: "map16_6_conv1"
  type: "Convolution"
  bottom: "map16_6_bn_pre"
  top: "map16_6_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_6_bn1"
  type: "BatchNorm"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_6_bn1"
  type: "BatchNorm"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_6_scale1"
  type: "Scale"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_6_relu1"
  type: "ReLU"
  bottom: "map16_6_conv1"
  top: "map16_6_conv1"
}
layer {
  name: "map16_6_conv2"
  type: "Convolution"
  bottom: "map16_6_conv1"
  top: "map16_6_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_6_bn2"
  type: "BatchNorm"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_6_bn2"
  type: "BatchNorm"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_6_scale2"
  type: "Scale"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_6_relu2"
  type: "ReLU"
  bottom: "map16_6_conv2"
  top: "map16_6_conv2"
}
layer {
  name: "map16_6_conv_end"
  type: "Convolution"
  bottom: "map16_6_conv2"
  top: "map16_6_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_6_eltsum"
  type: "Eltwise"
  bottom: "map16_5_eltsum"
  bottom: "map16_6_conv_end"
  top: "map16_6_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_7_bn_pre"
  type: "BatchNorm"
  bottom: "map16_6_eltsum"
  top: "map16_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_7_bn_pre"
  type: "BatchNorm"
  bottom: "map16_6_eltsum"
  top: "map16_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_7_pre_scale"
  type: "Scale"
  bottom: "map16_7_bn_pre"
  top: "map16_7_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_7_pre_relu"
  type: "ReLU"
  bottom: "map16_7_bn_pre"
  top: "map16_7_bn_pre"
}
layer {
  name: "map16_7_conv1"
  type: "Convolution"
  bottom: "map16_7_bn_pre"
  top: "map16_7_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_7_bn1"
  type: "BatchNorm"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_7_bn1"
  type: "BatchNorm"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_7_scale1"
  type: "Scale"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_7_relu1"
  type: "ReLU"
  bottom: "map16_7_conv1"
  top: "map16_7_conv1"
}
layer {
  name: "map16_7_conv2"
  type: "Convolution"
  bottom: "map16_7_conv1"
  top: "map16_7_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_7_bn2"
  type: "BatchNorm"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_7_bn2"
  type: "BatchNorm"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_7_scale2"
  type: "Scale"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_7_relu2"
  type: "ReLU"
  bottom: "map16_7_conv2"
  top: "map16_7_conv2"
}
layer {
  name: "map16_7_conv_end"
  type: "Convolution"
  bottom: "map16_7_conv2"
  top: "map16_7_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_7_eltsum"
  type: "Eltwise"
  bottom: "map16_6_eltsum"
  bottom: "map16_7_conv_end"
  top: "map16_7_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_8_bn_pre"
  type: "BatchNorm"
  bottom: "map16_7_eltsum"
  top: "map16_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_8_bn_pre"
  type: "BatchNorm"
  bottom: "map16_7_eltsum"
  top: "map16_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_8_pre_scale"
  type: "Scale"
  bottom: "map16_8_bn_pre"
  top: "map16_8_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_8_pre_relu"
  type: "ReLU"
  bottom: "map16_8_bn_pre"
  top: "map16_8_bn_pre"
}
layer {
  name: "map16_8_conv1"
  type: "Convolution"
  bottom: "map16_8_bn_pre"
  top: "map16_8_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_8_bn1"
  type: "BatchNorm"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_8_bn1"
  type: "BatchNorm"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_8_scale1"
  type: "Scale"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_8_relu1"
  type: "ReLU"
  bottom: "map16_8_conv1"
  top: "map16_8_conv1"
}
layer {
  name: "map16_8_conv2"
  type: "Convolution"
  bottom: "map16_8_conv1"
  top: "map16_8_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_8_bn2"
  type: "BatchNorm"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_8_bn2"
  type: "BatchNorm"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_8_scale2"
  type: "Scale"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_8_relu2"
  type: "ReLU"
  bottom: "map16_8_conv2"
  top: "map16_8_conv2"
}
layer {
  name: "map16_8_conv_end"
  type: "Convolution"
  bottom: "map16_8_conv2"
  top: "map16_8_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_8_eltsum"
  type: "Eltwise"
  bottom: "map16_7_eltsum"
  bottom: "map16_8_conv_end"
  top: "map16_8_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_9_bn_pre"
  type: "BatchNorm"
  bottom: "map16_8_eltsum"
  top: "map16_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_9_bn_pre"
  type: "BatchNorm"
  bottom: "map16_8_eltsum"
  top: "map16_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_9_pre_scale"
  type: "Scale"
  bottom: "map16_9_bn_pre"
  top: "map16_9_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_9_pre_relu"
  type: "ReLU"
  bottom: "map16_9_bn_pre"
  top: "map16_9_bn_pre"
}
layer {
  name: "map16_9_conv1"
  type: "Convolution"
  bottom: "map16_9_bn_pre"
  top: "map16_9_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_9_bn1"
  type: "BatchNorm"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_9_bn1"
  type: "BatchNorm"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_9_scale1"
  type: "Scale"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_9_relu1"
  type: "ReLU"
  bottom: "map16_9_conv1"
  top: "map16_9_conv1"
}
layer {
  name: "map16_9_conv2"
  type: "Convolution"
  bottom: "map16_9_conv1"
  top: "map16_9_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_9_bn2"
  type: "BatchNorm"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_9_bn2"
  type: "BatchNorm"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_9_scale2"
  type: "Scale"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_9_relu2"
  type: "ReLU"
  bottom: "map16_9_conv2"
  top: "map16_9_conv2"
}
layer {
  name: "map16_9_conv_end"
  type: "Convolution"
  bottom: "map16_9_conv2"
  top: "map16_9_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_9_eltsum"
  type: "Eltwise"
  bottom: "map16_8_eltsum"
  bottom: "map16_9_conv_end"
  top: "map16_9_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_10_bn_pre"
  type: "BatchNorm"
  bottom: "map16_9_eltsum"
  top: "map16_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_10_bn_pre"
  type: "BatchNorm"
  bottom: "map16_9_eltsum"
  top: "map16_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_10_pre_scale"
  type: "Scale"
  bottom: "map16_10_bn_pre"
  top: "map16_10_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_10_pre_relu"
  type: "ReLU"
  bottom: "map16_10_bn_pre"
  top: "map16_10_bn_pre"
}
layer {
  name: "map16_10_conv1"
  type: "Convolution"
  bottom: "map16_10_bn_pre"
  top: "map16_10_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_10_bn1"
  type: "BatchNorm"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_10_bn1"
  type: "BatchNorm"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_10_scale1"
  type: "Scale"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_10_relu1"
  type: "ReLU"
  bottom: "map16_10_conv1"
  top: "map16_10_conv1"
}
layer {
  name: "map16_10_conv2"
  type: "Convolution"
  bottom: "map16_10_conv1"
  top: "map16_10_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_10_bn2"
  type: "BatchNorm"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_10_bn2"
  type: "BatchNorm"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_10_scale2"
  type: "Scale"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_10_relu2"
  type: "ReLU"
  bottom: "map16_10_conv2"
  top: "map16_10_conv2"
}
layer {
  name: "map16_10_conv_end"
  type: "Convolution"
  bottom: "map16_10_conv2"
  top: "map16_10_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_10_eltsum"
  type: "Eltwise"
  bottom: "map16_9_eltsum"
  bottom: "map16_10_conv_end"
  top: "map16_10_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_11_bn_pre"
  type: "BatchNorm"
  bottom: "map16_10_eltsum"
  top: "map16_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_11_bn_pre"
  type: "BatchNorm"
  bottom: "map16_10_eltsum"
  top: "map16_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_11_pre_scale"
  type: "Scale"
  bottom: "map16_11_bn_pre"
  top: "map16_11_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_11_pre_relu"
  type: "ReLU"
  bottom: "map16_11_bn_pre"
  top: "map16_11_bn_pre"
}
layer {
  name: "map16_11_conv1"
  type: "Convolution"
  bottom: "map16_11_bn_pre"
  top: "map16_11_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_11_bn1"
  type: "BatchNorm"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_11_bn1"
  type: "BatchNorm"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_11_scale1"
  type: "Scale"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_11_relu1"
  type: "ReLU"
  bottom: "map16_11_conv1"
  top: "map16_11_conv1"
}
layer {
  name: "map16_11_conv2"
  type: "Convolution"
  bottom: "map16_11_conv1"
  top: "map16_11_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_11_bn2"
  type: "BatchNorm"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_11_bn2"
  type: "BatchNorm"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_11_scale2"
  type: "Scale"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_11_relu2"
  type: "ReLU"
  bottom: "map16_11_conv2"
  top: "map16_11_conv2"
}
layer {
  name: "map16_11_conv_end"
  type: "Convolution"
  bottom: "map16_11_conv2"
  top: "map16_11_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_11_eltsum"
  type: "Eltwise"
  bottom: "map16_10_eltsum"
  bottom: "map16_11_conv_end"
  top: "map16_11_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_12_bn_pre"
  type: "BatchNorm"
  bottom: "map16_11_eltsum"
  top: "map16_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_12_bn_pre"
  type: "BatchNorm"
  bottom: "map16_11_eltsum"
  top: "map16_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_12_pre_scale"
  type: "Scale"
  bottom: "map16_12_bn_pre"
  top: "map16_12_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_12_pre_relu"
  type: "ReLU"
  bottom: "map16_12_bn_pre"
  top: "map16_12_bn_pre"
}
layer {
  name: "map16_12_conv1"
  type: "Convolution"
  bottom: "map16_12_bn_pre"
  top: "map16_12_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_12_bn1"
  type: "BatchNorm"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_12_bn1"
  type: "BatchNorm"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_12_scale1"
  type: "Scale"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_12_relu1"
  type: "ReLU"
  bottom: "map16_12_conv1"
  top: "map16_12_conv1"
}
layer {
  name: "map16_12_conv2"
  type: "Convolution"
  bottom: "map16_12_conv1"
  top: "map16_12_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_12_bn2"
  type: "BatchNorm"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_12_bn2"
  type: "BatchNorm"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_12_scale2"
  type: "Scale"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_12_relu2"
  type: "ReLU"
  bottom: "map16_12_conv2"
  top: "map16_12_conv2"
}
layer {
  name: "map16_12_conv_end"
  type: "Convolution"
  bottom: "map16_12_conv2"
  top: "map16_12_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_12_eltsum"
  type: "Eltwise"
  bottom: "map16_11_eltsum"
  bottom: "map16_12_conv_end"
  top: "map16_12_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_13_bn_pre"
  type: "BatchNorm"
  bottom: "map16_12_eltsum"
  top: "map16_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_13_bn_pre"
  type: "BatchNorm"
  bottom: "map16_12_eltsum"
  top: "map16_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_13_pre_scale"
  type: "Scale"
  bottom: "map16_13_bn_pre"
  top: "map16_13_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_13_pre_relu"
  type: "ReLU"
  bottom: "map16_13_bn_pre"
  top: "map16_13_bn_pre"
}
layer {
  name: "map16_13_conv1"
  type: "Convolution"
  bottom: "map16_13_bn_pre"
  top: "map16_13_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_13_bn1"
  type: "BatchNorm"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_13_bn1"
  type: "BatchNorm"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_13_scale1"
  type: "Scale"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_13_relu1"
  type: "ReLU"
  bottom: "map16_13_conv1"
  top: "map16_13_conv1"
}
layer {
  name: "map16_13_conv2"
  type: "Convolution"
  bottom: "map16_13_conv1"
  top: "map16_13_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_13_bn2"
  type: "BatchNorm"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_13_bn2"
  type: "BatchNorm"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_13_scale2"
  type: "Scale"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_13_relu2"
  type: "ReLU"
  bottom: "map16_13_conv2"
  top: "map16_13_conv2"
}
layer {
  name: "map16_13_conv_end"
  type: "Convolution"
  bottom: "map16_13_conv2"
  top: "map16_13_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_13_eltsum"
  type: "Eltwise"
  bottom: "map16_12_eltsum"
  bottom: "map16_13_conv_end"
  top: "map16_13_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_14_bn_pre"
  type: "BatchNorm"
  bottom: "map16_13_eltsum"
  top: "map16_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_14_bn_pre"
  type: "BatchNorm"
  bottom: "map16_13_eltsum"
  top: "map16_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_14_pre_scale"
  type: "Scale"
  bottom: "map16_14_bn_pre"
  top: "map16_14_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_14_pre_relu"
  type: "ReLU"
  bottom: "map16_14_bn_pre"
  top: "map16_14_bn_pre"
}
layer {
  name: "map16_14_conv1"
  type: "Convolution"
  bottom: "map16_14_bn_pre"
  top: "map16_14_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_14_bn1"
  type: "BatchNorm"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_14_bn1"
  type: "BatchNorm"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_14_scale1"
  type: "Scale"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_14_relu1"
  type: "ReLU"
  bottom: "map16_14_conv1"
  top: "map16_14_conv1"
}
layer {
  name: "map16_14_conv2"
  type: "Convolution"
  bottom: "map16_14_conv1"
  top: "map16_14_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_14_bn2"
  type: "BatchNorm"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_14_bn2"
  type: "BatchNorm"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_14_scale2"
  type: "Scale"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_14_relu2"
  type: "ReLU"
  bottom: "map16_14_conv2"
  top: "map16_14_conv2"
}
layer {
  name: "map16_14_conv_end"
  type: "Convolution"
  bottom: "map16_14_conv2"
  top: "map16_14_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_14_eltsum"
  type: "Eltwise"
  bottom: "map16_13_eltsum"
  bottom: "map16_14_conv_end"
  top: "map16_14_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_15_bn_pre"
  type: "BatchNorm"
  bottom: "map16_14_eltsum"
  top: "map16_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_15_bn_pre"
  type: "BatchNorm"
  bottom: "map16_14_eltsum"
  top: "map16_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_15_pre_scale"
  type: "Scale"
  bottom: "map16_15_bn_pre"
  top: "map16_15_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_15_pre_relu"
  type: "ReLU"
  bottom: "map16_15_bn_pre"
  top: "map16_15_bn_pre"
}
layer {
  name: "map16_15_conv1"
  type: "Convolution"
  bottom: "map16_15_bn_pre"
  top: "map16_15_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_15_bn1"
  type: "BatchNorm"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_15_bn1"
  type: "BatchNorm"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_15_scale1"
  type: "Scale"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_15_relu1"
  type: "ReLU"
  bottom: "map16_15_conv1"
  top: "map16_15_conv1"
}
layer {
  name: "map16_15_conv2"
  type: "Convolution"
  bottom: "map16_15_conv1"
  top: "map16_15_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_15_bn2"
  type: "BatchNorm"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_15_bn2"
  type: "BatchNorm"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_15_scale2"
  type: "Scale"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_15_relu2"
  type: "ReLU"
  bottom: "map16_15_conv2"
  top: "map16_15_conv2"
}
layer {
  name: "map16_15_conv_end"
  type: "Convolution"
  bottom: "map16_15_conv2"
  top: "map16_15_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_15_eltsum"
  type: "Eltwise"
  bottom: "map16_14_eltsum"
  bottom: "map16_15_conv_end"
  top: "map16_15_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_16_bn_pre"
  type: "BatchNorm"
  bottom: "map16_15_eltsum"
  top: "map16_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_16_bn_pre"
  type: "BatchNorm"
  bottom: "map16_15_eltsum"
  top: "map16_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_16_pre_scale"
  type: "Scale"
  bottom: "map16_16_bn_pre"
  top: "map16_16_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_16_pre_relu"
  type: "ReLU"
  bottom: "map16_16_bn_pre"
  top: "map16_16_bn_pre"
}
layer {
  name: "map16_16_conv1"
  type: "Convolution"
  bottom: "map16_16_bn_pre"
  top: "map16_16_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_16_bn1"
  type: "BatchNorm"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_16_bn1"
  type: "BatchNorm"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_16_scale1"
  type: "Scale"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_16_relu1"
  type: "ReLU"
  bottom: "map16_16_conv1"
  top: "map16_16_conv1"
}
layer {
  name: "map16_16_conv2"
  type: "Convolution"
  bottom: "map16_16_conv1"
  top: "map16_16_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_16_bn2"
  type: "BatchNorm"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_16_bn2"
  type: "BatchNorm"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_16_scale2"
  type: "Scale"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_16_relu2"
  type: "ReLU"
  bottom: "map16_16_conv2"
  top: "map16_16_conv2"
}
layer {
  name: "map16_16_conv_end"
  type: "Convolution"
  bottom: "map16_16_conv2"
  top: "map16_16_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_16_eltsum"
  type: "Eltwise"
  bottom: "map16_15_eltsum"
  bottom: "map16_16_conv_end"
  top: "map16_16_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_17_bn_pre"
  type: "BatchNorm"
  bottom: "map16_16_eltsum"
  top: "map16_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_17_bn_pre"
  type: "BatchNorm"
  bottom: "map16_16_eltsum"
  top: "map16_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_17_pre_scale"
  type: "Scale"
  bottom: "map16_17_bn_pre"
  top: "map16_17_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_17_pre_relu"
  type: "ReLU"
  bottom: "map16_17_bn_pre"
  top: "map16_17_bn_pre"
}
layer {
  name: "map16_17_conv1"
  type: "Convolution"
  bottom: "map16_17_bn_pre"
  top: "map16_17_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_17_bn1"
  type: "BatchNorm"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_17_bn1"
  type: "BatchNorm"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_17_scale1"
  type: "Scale"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_17_relu1"
  type: "ReLU"
  bottom: "map16_17_conv1"
  top: "map16_17_conv1"
}
layer {
  name: "map16_17_conv2"
  type: "Convolution"
  bottom: "map16_17_conv1"
  top: "map16_17_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_17_bn2"
  type: "BatchNorm"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_17_bn2"
  type: "BatchNorm"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_17_scale2"
  type: "Scale"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_17_relu2"
  type: "ReLU"
  bottom: "map16_17_conv2"
  top: "map16_17_conv2"
}
layer {
  name: "map16_17_conv_end"
  type: "Convolution"
  bottom: "map16_17_conv2"
  top: "map16_17_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_17_eltsum"
  type: "Eltwise"
  bottom: "map16_16_eltsum"
  bottom: "map16_17_conv_end"
  top: "map16_17_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_18_bn_pre"
  type: "BatchNorm"
  bottom: "map16_17_eltsum"
  top: "map16_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_18_bn_pre"
  type: "BatchNorm"
  bottom: "map16_17_eltsum"
  top: "map16_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_18_pre_scale"
  type: "Scale"
  bottom: "map16_18_bn_pre"
  top: "map16_18_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_18_pre_relu"
  type: "ReLU"
  bottom: "map16_18_bn_pre"
  top: "map16_18_bn_pre"
}
layer {
  name: "map16_18_conv1"
  type: "Convolution"
  bottom: "map16_18_bn_pre"
  top: "map16_18_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_18_bn1"
  type: "BatchNorm"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_18_bn1"
  type: "BatchNorm"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_18_scale1"
  type: "Scale"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_18_relu1"
  type: "ReLU"
  bottom: "map16_18_conv1"
  top: "map16_18_conv1"
}
layer {
  name: "map16_18_conv2"
  type: "Convolution"
  bottom: "map16_18_conv1"
  top: "map16_18_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_18_bn2"
  type: "BatchNorm"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_18_bn2"
  type: "BatchNorm"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_18_scale2"
  type: "Scale"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_18_relu2"
  type: "ReLU"
  bottom: "map16_18_conv2"
  top: "map16_18_conv2"
}
layer {
  name: "map16_18_conv_end"
  type: "Convolution"
  bottom: "map16_18_conv2"
  top: "map16_18_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_18_eltsum"
  type: "Eltwise"
  bottom: "map16_17_eltsum"
  bottom: "map16_18_conv_end"
  top: "map16_18_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_19_bn_pre"
  type: "BatchNorm"
  bottom: "map16_18_eltsum"
  top: "map16_19_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_19_bn_pre"
  type: "BatchNorm"
  bottom: "map16_18_eltsum"
  top: "map16_19_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_19_pre_scale"
  type: "Scale"
  bottom: "map16_19_bn_pre"
  top: "map16_19_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_19_pre_relu"
  type: "ReLU"
  bottom: "map16_19_bn_pre"
  top: "map16_19_bn_pre"
}
layer {
  name: "map16_19_conv1"
  type: "Convolution"
  bottom: "map16_19_bn_pre"
  top: "map16_19_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_19_bn1"
  type: "BatchNorm"
  bottom: "map16_19_conv1"
  top: "map16_19_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_19_bn1"
  type: "BatchNorm"
  bottom: "map16_19_conv1"
  top: "map16_19_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_19_scale1"
  type: "Scale"
  bottom: "map16_19_conv1"
  top: "map16_19_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_19_relu1"
  type: "ReLU"
  bottom: "map16_19_conv1"
  top: "map16_19_conv1"
}
layer {
  name: "map16_19_conv2"
  type: "Convolution"
  bottom: "map16_19_conv1"
  top: "map16_19_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_19_bn2"
  type: "BatchNorm"
  bottom: "map16_19_conv2"
  top: "map16_19_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_19_bn2"
  type: "BatchNorm"
  bottom: "map16_19_conv2"
  top: "map16_19_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_19_scale2"
  type: "Scale"
  bottom: "map16_19_conv2"
  top: "map16_19_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_19_relu2"
  type: "ReLU"
  bottom: "map16_19_conv2"
  top: "map16_19_conv2"
}
layer {
  name: "map16_19_conv_end"
  type: "Convolution"
  bottom: "map16_19_conv2"
  top: "map16_19_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_19_eltsum"
  type: "Eltwise"
  bottom: "map16_18_eltsum"
  bottom: "map16_19_conv_end"
  top: "map16_19_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_20_bn_pre"
  type: "BatchNorm"
  bottom: "map16_19_eltsum"
  top: "map16_20_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_20_bn_pre"
  type: "BatchNorm"
  bottom: "map16_19_eltsum"
  top: "map16_20_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_20_pre_scale"
  type: "Scale"
  bottom: "map16_20_bn_pre"
  top: "map16_20_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_20_pre_relu"
  type: "ReLU"
  bottom: "map16_20_bn_pre"
  top: "map16_20_bn_pre"
}
layer {
  name: "map16_20_conv1"
  type: "Convolution"
  bottom: "map16_20_bn_pre"
  top: "map16_20_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_20_bn1"
  type: "BatchNorm"
  bottom: "map16_20_conv1"
  top: "map16_20_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_20_bn1"
  type: "BatchNorm"
  bottom: "map16_20_conv1"
  top: "map16_20_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_20_scale1"
  type: "Scale"
  bottom: "map16_20_conv1"
  top: "map16_20_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_20_relu1"
  type: "ReLU"
  bottom: "map16_20_conv1"
  top: "map16_20_conv1"
}
layer {
  name: "map16_20_conv2"
  type: "Convolution"
  bottom: "map16_20_conv1"
  top: "map16_20_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_20_bn2"
  type: "BatchNorm"
  bottom: "map16_20_conv2"
  top: "map16_20_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_20_bn2"
  type: "BatchNorm"
  bottom: "map16_20_conv2"
  top: "map16_20_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_20_scale2"
  type: "Scale"
  bottom: "map16_20_conv2"
  top: "map16_20_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_20_relu2"
  type: "ReLU"
  bottom: "map16_20_conv2"
  top: "map16_20_conv2"
}
layer {
  name: "map16_20_conv_end"
  type: "Convolution"
  bottom: "map16_20_conv2"
  top: "map16_20_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_20_eltsum"
  type: "Eltwise"
  bottom: "map16_19_eltsum"
  bottom: "map16_20_conv_end"
  top: "map16_20_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_21_bn_pre"
  type: "BatchNorm"
  bottom: "map16_20_eltsum"
  top: "map16_21_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_21_bn_pre"
  type: "BatchNorm"
  bottom: "map16_20_eltsum"
  top: "map16_21_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_21_pre_scale"
  type: "Scale"
  bottom: "map16_21_bn_pre"
  top: "map16_21_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_21_pre_relu"
  type: "ReLU"
  bottom: "map16_21_bn_pre"
  top: "map16_21_bn_pre"
}
layer {
  name: "map16_21_conv1"
  type: "Convolution"
  bottom: "map16_21_bn_pre"
  top: "map16_21_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_21_bn1"
  type: "BatchNorm"
  bottom: "map16_21_conv1"
  top: "map16_21_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_21_bn1"
  type: "BatchNorm"
  bottom: "map16_21_conv1"
  top: "map16_21_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_21_scale1"
  type: "Scale"
  bottom: "map16_21_conv1"
  top: "map16_21_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_21_relu1"
  type: "ReLU"
  bottom: "map16_21_conv1"
  top: "map16_21_conv1"
}
layer {
  name: "map16_21_conv2"
  type: "Convolution"
  bottom: "map16_21_conv1"
  top: "map16_21_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_21_bn2"
  type: "BatchNorm"
  bottom: "map16_21_conv2"
  top: "map16_21_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_21_bn2"
  type: "BatchNorm"
  bottom: "map16_21_conv2"
  top: "map16_21_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_21_scale2"
  type: "Scale"
  bottom: "map16_21_conv2"
  top: "map16_21_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_21_relu2"
  type: "ReLU"
  bottom: "map16_21_conv2"
  top: "map16_21_conv2"
}
layer {
  name: "map16_21_conv_end"
  type: "Convolution"
  bottom: "map16_21_conv2"
  top: "map16_21_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_21_eltsum"
  type: "Eltwise"
  bottom: "map16_20_eltsum"
  bottom: "map16_21_conv_end"
  top: "map16_21_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_22_bn_pre"
  type: "BatchNorm"
  bottom: "map16_21_eltsum"
  top: "map16_22_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_22_bn_pre"
  type: "BatchNorm"
  bottom: "map16_21_eltsum"
  top: "map16_22_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_22_pre_scale"
  type: "Scale"
  bottom: "map16_22_bn_pre"
  top: "map16_22_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_22_pre_relu"
  type: "ReLU"
  bottom: "map16_22_bn_pre"
  top: "map16_22_bn_pre"
}
layer {
  name: "map16_22_conv1"
  type: "Convolution"
  bottom: "map16_22_bn_pre"
  top: "map16_22_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_22_bn1"
  type: "BatchNorm"
  bottom: "map16_22_conv1"
  top: "map16_22_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_22_bn1"
  type: "BatchNorm"
  bottom: "map16_22_conv1"
  top: "map16_22_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_22_scale1"
  type: "Scale"
  bottom: "map16_22_conv1"
  top: "map16_22_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_22_relu1"
  type: "ReLU"
  bottom: "map16_22_conv1"
  top: "map16_22_conv1"
}
layer {
  name: "map16_22_conv2"
  type: "Convolution"
  bottom: "map16_22_conv1"
  top: "map16_22_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_22_bn2"
  type: "BatchNorm"
  bottom: "map16_22_conv2"
  top: "map16_22_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_22_bn2"
  type: "BatchNorm"
  bottom: "map16_22_conv2"
  top: "map16_22_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_22_scale2"
  type: "Scale"
  bottom: "map16_22_conv2"
  top: "map16_22_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_22_relu2"
  type: "ReLU"
  bottom: "map16_22_conv2"
  top: "map16_22_conv2"
}
layer {
  name: "map16_22_conv_end"
  type: "Convolution"
  bottom: "map16_22_conv2"
  top: "map16_22_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_22_eltsum"
  type: "Eltwise"
  bottom: "map16_21_eltsum"
  bottom: "map16_22_conv_end"
  top: "map16_22_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_23_bn_pre"
  type: "BatchNorm"
  bottom: "map16_22_eltsum"
  top: "map16_23_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_23_bn_pre"
  type: "BatchNorm"
  bottom: "map16_22_eltsum"
  top: "map16_23_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_23_pre_scale"
  type: "Scale"
  bottom: "map16_23_bn_pre"
  top: "map16_23_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_23_pre_relu"
  type: "ReLU"
  bottom: "map16_23_bn_pre"
  top: "map16_23_bn_pre"
}
layer {
  name: "map16_23_conv1"
  type: "Convolution"
  bottom: "map16_23_bn_pre"
  top: "map16_23_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_23_bn1"
  type: "BatchNorm"
  bottom: "map16_23_conv1"
  top: "map16_23_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_23_bn1"
  type: "BatchNorm"
  bottom: "map16_23_conv1"
  top: "map16_23_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_23_scale1"
  type: "Scale"
  bottom: "map16_23_conv1"
  top: "map16_23_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_23_relu1"
  type: "ReLU"
  bottom: "map16_23_conv1"
  top: "map16_23_conv1"
}
layer {
  name: "map16_23_conv2"
  type: "Convolution"
  bottom: "map16_23_conv1"
  top: "map16_23_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_23_bn2"
  type: "BatchNorm"
  bottom: "map16_23_conv2"
  top: "map16_23_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_23_bn2"
  type: "BatchNorm"
  bottom: "map16_23_conv2"
  top: "map16_23_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_23_scale2"
  type: "Scale"
  bottom: "map16_23_conv2"
  top: "map16_23_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_23_relu2"
  type: "ReLU"
  bottom: "map16_23_conv2"
  top: "map16_23_conv2"
}
layer {
  name: "map16_23_conv_end"
  type: "Convolution"
  bottom: "map16_23_conv2"
  top: "map16_23_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_23_eltsum"
  type: "Eltwise"
  bottom: "map16_22_eltsum"
  bottom: "map16_23_conv_end"
  top: "map16_23_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_24_bn_pre"
  type: "BatchNorm"
  bottom: "map16_23_eltsum"
  top: "map16_24_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_24_bn_pre"
  type: "BatchNorm"
  bottom: "map16_23_eltsum"
  top: "map16_24_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_24_pre_scale"
  type: "Scale"
  bottom: "map16_24_bn_pre"
  top: "map16_24_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_24_pre_relu"
  type: "ReLU"
  bottom: "map16_24_bn_pre"
  top: "map16_24_bn_pre"
}
layer {
  name: "map16_24_conv1"
  type: "Convolution"
  bottom: "map16_24_bn_pre"
  top: "map16_24_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_24_bn1"
  type: "BatchNorm"
  bottom: "map16_24_conv1"
  top: "map16_24_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_24_bn1"
  type: "BatchNorm"
  bottom: "map16_24_conv1"
  top: "map16_24_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_24_scale1"
  type: "Scale"
  bottom: "map16_24_conv1"
  top: "map16_24_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_24_relu1"
  type: "ReLU"
  bottom: "map16_24_conv1"
  top: "map16_24_conv1"
}
layer {
  name: "map16_24_conv2"
  type: "Convolution"
  bottom: "map16_24_conv1"
  top: "map16_24_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_24_bn2"
  type: "BatchNorm"
  bottom: "map16_24_conv2"
  top: "map16_24_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_24_bn2"
  type: "BatchNorm"
  bottom: "map16_24_conv2"
  top: "map16_24_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_24_scale2"
  type: "Scale"
  bottom: "map16_24_conv2"
  top: "map16_24_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_24_relu2"
  type: "ReLU"
  bottom: "map16_24_conv2"
  top: "map16_24_conv2"
}
layer {
  name: "map16_24_conv_end"
  type: "Convolution"
  bottom: "map16_24_conv2"
  top: "map16_24_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_24_eltsum"
  type: "Eltwise"
  bottom: "map16_23_eltsum"
  bottom: "map16_24_conv_end"
  top: "map16_24_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_25_bn_pre"
  type: "BatchNorm"
  bottom: "map16_24_eltsum"
  top: "map16_25_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_25_bn_pre"
  type: "BatchNorm"
  bottom: "map16_24_eltsum"
  top: "map16_25_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_25_pre_scale"
  type: "Scale"
  bottom: "map16_25_bn_pre"
  top: "map16_25_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_25_pre_relu"
  type: "ReLU"
  bottom: "map16_25_bn_pre"
  top: "map16_25_bn_pre"
}
layer {
  name: "map16_25_conv1"
  type: "Convolution"
  bottom: "map16_25_bn_pre"
  top: "map16_25_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_25_bn1"
  type: "BatchNorm"
  bottom: "map16_25_conv1"
  top: "map16_25_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_25_bn1"
  type: "BatchNorm"
  bottom: "map16_25_conv1"
  top: "map16_25_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_25_scale1"
  type: "Scale"
  bottom: "map16_25_conv1"
  top: "map16_25_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_25_relu1"
  type: "ReLU"
  bottom: "map16_25_conv1"
  top: "map16_25_conv1"
}
layer {
  name: "map16_25_conv2"
  type: "Convolution"
  bottom: "map16_25_conv1"
  top: "map16_25_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_25_bn2"
  type: "BatchNorm"
  bottom: "map16_25_conv2"
  top: "map16_25_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_25_bn2"
  type: "BatchNorm"
  bottom: "map16_25_conv2"
  top: "map16_25_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_25_scale2"
  type: "Scale"
  bottom: "map16_25_conv2"
  top: "map16_25_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_25_relu2"
  type: "ReLU"
  bottom: "map16_25_conv2"
  top: "map16_25_conv2"
}
layer {
  name: "map16_25_conv_end"
  type: "Convolution"
  bottom: "map16_25_conv2"
  top: "map16_25_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_25_eltsum"
  type: "Eltwise"
  bottom: "map16_24_eltsum"
  bottom: "map16_25_conv_end"
  top: "map16_25_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_26_bn_pre"
  type: "BatchNorm"
  bottom: "map16_25_eltsum"
  top: "map16_26_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_26_bn_pre"
  type: "BatchNorm"
  bottom: "map16_25_eltsum"
  top: "map16_26_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_26_pre_scale"
  type: "Scale"
  bottom: "map16_26_bn_pre"
  top: "map16_26_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_26_pre_relu"
  type: "ReLU"
  bottom: "map16_26_bn_pre"
  top: "map16_26_bn_pre"
}
layer {
  name: "map16_26_conv1"
  type: "Convolution"
  bottom: "map16_26_bn_pre"
  top: "map16_26_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_26_bn1"
  type: "BatchNorm"
  bottom: "map16_26_conv1"
  top: "map16_26_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_26_bn1"
  type: "BatchNorm"
  bottom: "map16_26_conv1"
  top: "map16_26_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_26_scale1"
  type: "Scale"
  bottom: "map16_26_conv1"
  top: "map16_26_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_26_relu1"
  type: "ReLU"
  bottom: "map16_26_conv1"
  top: "map16_26_conv1"
}
layer {
  name: "map16_26_conv2"
  type: "Convolution"
  bottom: "map16_26_conv1"
  top: "map16_26_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_26_bn2"
  type: "BatchNorm"
  bottom: "map16_26_conv2"
  top: "map16_26_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_26_bn2"
  type: "BatchNorm"
  bottom: "map16_26_conv2"
  top: "map16_26_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_26_scale2"
  type: "Scale"
  bottom: "map16_26_conv2"
  top: "map16_26_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_26_relu2"
  type: "ReLU"
  bottom: "map16_26_conv2"
  top: "map16_26_conv2"
}
layer {
  name: "map16_26_conv_end"
  type: "Convolution"
  bottom: "map16_26_conv2"
  top: "map16_26_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_26_eltsum"
  type: "Eltwise"
  bottom: "map16_25_eltsum"
  bottom: "map16_26_conv_end"
  top: "map16_26_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_27_bn_pre"
  type: "BatchNorm"
  bottom: "map16_26_eltsum"
  top: "map16_27_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_27_bn_pre"
  type: "BatchNorm"
  bottom: "map16_26_eltsum"
  top: "map16_27_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_27_pre_scale"
  type: "Scale"
  bottom: "map16_27_bn_pre"
  top: "map16_27_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_27_pre_relu"
  type: "ReLU"
  bottom: "map16_27_bn_pre"
  top: "map16_27_bn_pre"
}
layer {
  name: "map16_27_conv1"
  type: "Convolution"
  bottom: "map16_27_bn_pre"
  top: "map16_27_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_27_bn1"
  type: "BatchNorm"
  bottom: "map16_27_conv1"
  top: "map16_27_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_27_bn1"
  type: "BatchNorm"
  bottom: "map16_27_conv1"
  top: "map16_27_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_27_scale1"
  type: "Scale"
  bottom: "map16_27_conv1"
  top: "map16_27_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_27_relu1"
  type: "ReLU"
  bottom: "map16_27_conv1"
  top: "map16_27_conv1"
}
layer {
  name: "map16_27_conv2"
  type: "Convolution"
  bottom: "map16_27_conv1"
  top: "map16_27_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_27_bn2"
  type: "BatchNorm"
  bottom: "map16_27_conv2"
  top: "map16_27_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_27_bn2"
  type: "BatchNorm"
  bottom: "map16_27_conv2"
  top: "map16_27_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_27_scale2"
  type: "Scale"
  bottom: "map16_27_conv2"
  top: "map16_27_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_27_relu2"
  type: "ReLU"
  bottom: "map16_27_conv2"
  top: "map16_27_conv2"
}
layer {
  name: "map16_27_conv_end"
  type: "Convolution"
  bottom: "map16_27_conv2"
  top: "map16_27_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_27_eltsum"
  type: "Eltwise"
  bottom: "map16_26_eltsum"
  bottom: "map16_27_conv_end"
  top: "map16_27_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_28_bn_pre"
  type: "BatchNorm"
  bottom: "map16_27_eltsum"
  top: "map16_28_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_28_bn_pre"
  type: "BatchNorm"
  bottom: "map16_27_eltsum"
  top: "map16_28_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_28_pre_scale"
  type: "Scale"
  bottom: "map16_28_bn_pre"
  top: "map16_28_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_28_pre_relu"
  type: "ReLU"
  bottom: "map16_28_bn_pre"
  top: "map16_28_bn_pre"
}
layer {
  name: "map16_28_conv1"
  type: "Convolution"
  bottom: "map16_28_bn_pre"
  top: "map16_28_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_28_bn1"
  type: "BatchNorm"
  bottom: "map16_28_conv1"
  top: "map16_28_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_28_bn1"
  type: "BatchNorm"
  bottom: "map16_28_conv1"
  top: "map16_28_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_28_scale1"
  type: "Scale"
  bottom: "map16_28_conv1"
  top: "map16_28_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_28_relu1"
  type: "ReLU"
  bottom: "map16_28_conv1"
  top: "map16_28_conv1"
}
layer {
  name: "map16_28_conv2"
  type: "Convolution"
  bottom: "map16_28_conv1"
  top: "map16_28_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_28_bn2"
  type: "BatchNorm"
  bottom: "map16_28_conv2"
  top: "map16_28_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_28_bn2"
  type: "BatchNorm"
  bottom: "map16_28_conv2"
  top: "map16_28_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_28_scale2"
  type: "Scale"
  bottom: "map16_28_conv2"
  top: "map16_28_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_28_relu2"
  type: "ReLU"
  bottom: "map16_28_conv2"
  top: "map16_28_conv2"
}
layer {
  name: "map16_28_conv_end"
  type: "Convolution"
  bottom: "map16_28_conv2"
  top: "map16_28_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_28_eltsum"
  type: "Eltwise"
  bottom: "map16_27_eltsum"
  bottom: "map16_28_conv_end"
  top: "map16_28_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_29_bn_pre"
  type: "BatchNorm"
  bottom: "map16_28_eltsum"
  top: "map16_29_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_29_bn_pre"
  type: "BatchNorm"
  bottom: "map16_28_eltsum"
  top: "map16_29_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_29_pre_scale"
  type: "Scale"
  bottom: "map16_29_bn_pre"
  top: "map16_29_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_29_pre_relu"
  type: "ReLU"
  bottom: "map16_29_bn_pre"
  top: "map16_29_bn_pre"
}
layer {
  name: "map16_29_conv1"
  type: "Convolution"
  bottom: "map16_29_bn_pre"
  top: "map16_29_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_29_bn1"
  type: "BatchNorm"
  bottom: "map16_29_conv1"
  top: "map16_29_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_29_bn1"
  type: "BatchNorm"
  bottom: "map16_29_conv1"
  top: "map16_29_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_29_scale1"
  type: "Scale"
  bottom: "map16_29_conv1"
  top: "map16_29_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_29_relu1"
  type: "ReLU"
  bottom: "map16_29_conv1"
  top: "map16_29_conv1"
}
layer {
  name: "map16_29_conv2"
  type: "Convolution"
  bottom: "map16_29_conv1"
  top: "map16_29_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_29_bn2"
  type: "BatchNorm"
  bottom: "map16_29_conv2"
  top: "map16_29_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_29_bn2"
  type: "BatchNorm"
  bottom: "map16_29_conv2"
  top: "map16_29_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_29_scale2"
  type: "Scale"
  bottom: "map16_29_conv2"
  top: "map16_29_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_29_relu2"
  type: "ReLU"
  bottom: "map16_29_conv2"
  top: "map16_29_conv2"
}
layer {
  name: "map16_29_conv_end"
  type: "Convolution"
  bottom: "map16_29_conv2"
  top: "map16_29_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_29_eltsum"
  type: "Eltwise"
  bottom: "map16_28_eltsum"
  bottom: "map16_29_conv_end"
  top: "map16_29_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_30_bn_pre"
  type: "BatchNorm"
  bottom: "map16_29_eltsum"
  top: "map16_30_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_30_bn_pre"
  type: "BatchNorm"
  bottom: "map16_29_eltsum"
  top: "map16_30_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_30_pre_scale"
  type: "Scale"
  bottom: "map16_30_bn_pre"
  top: "map16_30_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_30_pre_relu"
  type: "ReLU"
  bottom: "map16_30_bn_pre"
  top: "map16_30_bn_pre"
}
layer {
  name: "map16_30_conv1"
  type: "Convolution"
  bottom: "map16_30_bn_pre"
  top: "map16_30_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_30_bn1"
  type: "BatchNorm"
  bottom: "map16_30_conv1"
  top: "map16_30_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_30_bn1"
  type: "BatchNorm"
  bottom: "map16_30_conv1"
  top: "map16_30_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_30_scale1"
  type: "Scale"
  bottom: "map16_30_conv1"
  top: "map16_30_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_30_relu1"
  type: "ReLU"
  bottom: "map16_30_conv1"
  top: "map16_30_conv1"
}
layer {
  name: "map16_30_conv2"
  type: "Convolution"
  bottom: "map16_30_conv1"
  top: "map16_30_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_30_bn2"
  type: "BatchNorm"
  bottom: "map16_30_conv2"
  top: "map16_30_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_30_bn2"
  type: "BatchNorm"
  bottom: "map16_30_conv2"
  top: "map16_30_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_30_scale2"
  type: "Scale"
  bottom: "map16_30_conv2"
  top: "map16_30_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_30_relu2"
  type: "ReLU"
  bottom: "map16_30_conv2"
  top: "map16_30_conv2"
}
layer {
  name: "map16_30_conv_end"
  type: "Convolution"
  bottom: "map16_30_conv2"
  top: "map16_30_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_30_eltsum"
  type: "Eltwise"
  bottom: "map16_29_eltsum"
  bottom: "map16_30_conv_end"
  top: "map16_30_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_31_bn_pre"
  type: "BatchNorm"
  bottom: "map16_30_eltsum"
  top: "map16_31_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_31_bn_pre"
  type: "BatchNorm"
  bottom: "map16_30_eltsum"
  top: "map16_31_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_31_pre_scale"
  type: "Scale"
  bottom: "map16_31_bn_pre"
  top: "map16_31_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_31_pre_relu"
  type: "ReLU"
  bottom: "map16_31_bn_pre"
  top: "map16_31_bn_pre"
}
layer {
  name: "map16_31_conv1"
  type: "Convolution"
  bottom: "map16_31_bn_pre"
  top: "map16_31_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_31_bn1"
  type: "BatchNorm"
  bottom: "map16_31_conv1"
  top: "map16_31_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_31_bn1"
  type: "BatchNorm"
  bottom: "map16_31_conv1"
  top: "map16_31_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_31_scale1"
  type: "Scale"
  bottom: "map16_31_conv1"
  top: "map16_31_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_31_relu1"
  type: "ReLU"
  bottom: "map16_31_conv1"
  top: "map16_31_conv1"
}
layer {
  name: "map16_31_conv2"
  type: "Convolution"
  bottom: "map16_31_conv1"
  top: "map16_31_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_31_bn2"
  type: "BatchNorm"
  bottom: "map16_31_conv2"
  top: "map16_31_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_31_bn2"
  type: "BatchNorm"
  bottom: "map16_31_conv2"
  top: "map16_31_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_31_scale2"
  type: "Scale"
  bottom: "map16_31_conv2"
  top: "map16_31_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_31_relu2"
  type: "ReLU"
  bottom: "map16_31_conv2"
  top: "map16_31_conv2"
}
layer {
  name: "map16_31_conv_end"
  type: "Convolution"
  bottom: "map16_31_conv2"
  top: "map16_31_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_31_eltsum"
  type: "Eltwise"
  bottom: "map16_30_eltsum"
  bottom: "map16_31_conv_end"
  top: "map16_31_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_32_bn_pre"
  type: "BatchNorm"
  bottom: "map16_31_eltsum"
  top: "map16_32_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_32_bn_pre"
  type: "BatchNorm"
  bottom: "map16_31_eltsum"
  top: "map16_32_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_32_pre_scale"
  type: "Scale"
  bottom: "map16_32_bn_pre"
  top: "map16_32_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_32_pre_relu"
  type: "ReLU"
  bottom: "map16_32_bn_pre"
  top: "map16_32_bn_pre"
}
layer {
  name: "map16_32_conv1"
  type: "Convolution"
  bottom: "map16_32_bn_pre"
  top: "map16_32_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_32_bn1"
  type: "BatchNorm"
  bottom: "map16_32_conv1"
  top: "map16_32_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_32_bn1"
  type: "BatchNorm"
  bottom: "map16_32_conv1"
  top: "map16_32_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_32_scale1"
  type: "Scale"
  bottom: "map16_32_conv1"
  top: "map16_32_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_32_relu1"
  type: "ReLU"
  bottom: "map16_32_conv1"
  top: "map16_32_conv1"
}
layer {
  name: "map16_32_conv2"
  type: "Convolution"
  bottom: "map16_32_conv1"
  top: "map16_32_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_32_bn2"
  type: "BatchNorm"
  bottom: "map16_32_conv2"
  top: "map16_32_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_32_bn2"
  type: "BatchNorm"
  bottom: "map16_32_conv2"
  top: "map16_32_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_32_scale2"
  type: "Scale"
  bottom: "map16_32_conv2"
  top: "map16_32_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_32_relu2"
  type: "ReLU"
  bottom: "map16_32_conv2"
  top: "map16_32_conv2"
}
layer {
  name: "map16_32_conv_end"
  type: "Convolution"
  bottom: "map16_32_conv2"
  top: "map16_32_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_32_eltsum"
  type: "Eltwise"
  bottom: "map16_31_eltsum"
  bottom: "map16_32_conv_end"
  top: "map16_32_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_33_bn_pre"
  type: "BatchNorm"
  bottom: "map16_32_eltsum"
  top: "map16_33_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_33_bn_pre"
  type: "BatchNorm"
  bottom: "map16_32_eltsum"
  top: "map16_33_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_33_pre_scale"
  type: "Scale"
  bottom: "map16_33_bn_pre"
  top: "map16_33_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_33_pre_relu"
  type: "ReLU"
  bottom: "map16_33_bn_pre"
  top: "map16_33_bn_pre"
}
layer {
  name: "map16_33_conv1"
  type: "Convolution"
  bottom: "map16_33_bn_pre"
  top: "map16_33_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_33_bn1"
  type: "BatchNorm"
  bottom: "map16_33_conv1"
  top: "map16_33_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_33_bn1"
  type: "BatchNorm"
  bottom: "map16_33_conv1"
  top: "map16_33_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_33_scale1"
  type: "Scale"
  bottom: "map16_33_conv1"
  top: "map16_33_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_33_relu1"
  type: "ReLU"
  bottom: "map16_33_conv1"
  top: "map16_33_conv1"
}
layer {
  name: "map16_33_conv2"
  type: "Convolution"
  bottom: "map16_33_conv1"
  top: "map16_33_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_33_bn2"
  type: "BatchNorm"
  bottom: "map16_33_conv2"
  top: "map16_33_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_33_bn2"
  type: "BatchNorm"
  bottom: "map16_33_conv2"
  top: "map16_33_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_33_scale2"
  type: "Scale"
  bottom: "map16_33_conv2"
  top: "map16_33_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_33_relu2"
  type: "ReLU"
  bottom: "map16_33_conv2"
  top: "map16_33_conv2"
}
layer {
  name: "map16_33_conv_end"
  type: "Convolution"
  bottom: "map16_33_conv2"
  top: "map16_33_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_33_eltsum"
  type: "Eltwise"
  bottom: "map16_32_eltsum"
  bottom: "map16_33_conv_end"
  top: "map16_33_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_34_bn_pre"
  type: "BatchNorm"
  bottom: "map16_33_eltsum"
  top: "map16_34_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_34_bn_pre"
  type: "BatchNorm"
  bottom: "map16_33_eltsum"
  top: "map16_34_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_34_pre_scale"
  type: "Scale"
  bottom: "map16_34_bn_pre"
  top: "map16_34_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_34_pre_relu"
  type: "ReLU"
  bottom: "map16_34_bn_pre"
  top: "map16_34_bn_pre"
}
layer {
  name: "map16_34_conv1"
  type: "Convolution"
  bottom: "map16_34_bn_pre"
  top: "map16_34_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_34_bn1"
  type: "BatchNorm"
  bottom: "map16_34_conv1"
  top: "map16_34_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_34_bn1"
  type: "BatchNorm"
  bottom: "map16_34_conv1"
  top: "map16_34_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_34_scale1"
  type: "Scale"
  bottom: "map16_34_conv1"
  top: "map16_34_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_34_relu1"
  type: "ReLU"
  bottom: "map16_34_conv1"
  top: "map16_34_conv1"
}
layer {
  name: "map16_34_conv2"
  type: "Convolution"
  bottom: "map16_34_conv1"
  top: "map16_34_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_34_bn2"
  type: "BatchNorm"
  bottom: "map16_34_conv2"
  top: "map16_34_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_34_bn2"
  type: "BatchNorm"
  bottom: "map16_34_conv2"
  top: "map16_34_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_34_scale2"
  type: "Scale"
  bottom: "map16_34_conv2"
  top: "map16_34_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_34_relu2"
  type: "ReLU"
  bottom: "map16_34_conv2"
  top: "map16_34_conv2"
}
layer {
  name: "map16_34_conv_end"
  type: "Convolution"
  bottom: "map16_34_conv2"
  top: "map16_34_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_34_eltsum"
  type: "Eltwise"
  bottom: "map16_33_eltsum"
  bottom: "map16_34_conv_end"
  top: "map16_34_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_35_bn_pre"
  type: "BatchNorm"
  bottom: "map16_34_eltsum"
  top: "map16_35_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_35_bn_pre"
  type: "BatchNorm"
  bottom: "map16_34_eltsum"
  top: "map16_35_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_35_pre_scale"
  type: "Scale"
  bottom: "map16_35_bn_pre"
  top: "map16_35_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_35_pre_relu"
  type: "ReLU"
  bottom: "map16_35_bn_pre"
  top: "map16_35_bn_pre"
}
layer {
  name: "map16_35_conv1"
  type: "Convolution"
  bottom: "map16_35_bn_pre"
  top: "map16_35_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_35_bn1"
  type: "BatchNorm"
  bottom: "map16_35_conv1"
  top: "map16_35_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_35_bn1"
  type: "BatchNorm"
  bottom: "map16_35_conv1"
  top: "map16_35_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_35_scale1"
  type: "Scale"
  bottom: "map16_35_conv1"
  top: "map16_35_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_35_relu1"
  type: "ReLU"
  bottom: "map16_35_conv1"
  top: "map16_35_conv1"
}
layer {
  name: "map16_35_conv2"
  type: "Convolution"
  bottom: "map16_35_conv1"
  top: "map16_35_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_35_bn2"
  type: "BatchNorm"
  bottom: "map16_35_conv2"
  top: "map16_35_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_35_bn2"
  type: "BatchNorm"
  bottom: "map16_35_conv2"
  top: "map16_35_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_35_scale2"
  type: "Scale"
  bottom: "map16_35_conv2"
  top: "map16_35_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_35_relu2"
  type: "ReLU"
  bottom: "map16_35_conv2"
  top: "map16_35_conv2"
}
layer {
  name: "map16_35_conv_end"
  type: "Convolution"
  bottom: "map16_35_conv2"
  top: "map16_35_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_35_eltsum"
  type: "Eltwise"
  bottom: "map16_34_eltsum"
  bottom: "map16_35_conv_end"
  top: "map16_35_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_36_bn_pre"
  type: "BatchNorm"
  bottom: "map16_35_eltsum"
  top: "map16_36_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_36_bn_pre"
  type: "BatchNorm"
  bottom: "map16_35_eltsum"
  top: "map16_36_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_36_pre_scale"
  type: "Scale"
  bottom: "map16_36_bn_pre"
  top: "map16_36_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_36_pre_relu"
  type: "ReLU"
  bottom: "map16_36_bn_pre"
  top: "map16_36_bn_pre"
}
layer {
  name: "map16_36_conv1"
  type: "Convolution"
  bottom: "map16_36_bn_pre"
  top: "map16_36_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_36_bn1"
  type: "BatchNorm"
  bottom: "map16_36_conv1"
  top: "map16_36_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_36_bn1"
  type: "BatchNorm"
  bottom: "map16_36_conv1"
  top: "map16_36_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_36_scale1"
  type: "Scale"
  bottom: "map16_36_conv1"
  top: "map16_36_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_36_relu1"
  type: "ReLU"
  bottom: "map16_36_conv1"
  top: "map16_36_conv1"
}
layer {
  name: "map16_36_conv2"
  type: "Convolution"
  bottom: "map16_36_conv1"
  top: "map16_36_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_36_bn2"
  type: "BatchNorm"
  bottom: "map16_36_conv2"
  top: "map16_36_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_36_bn2"
  type: "BatchNorm"
  bottom: "map16_36_conv2"
  top: "map16_36_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_36_scale2"
  type: "Scale"
  bottom: "map16_36_conv2"
  top: "map16_36_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_36_relu2"
  type: "ReLU"
  bottom: "map16_36_conv2"
  top: "map16_36_conv2"
}
layer {
  name: "map16_36_conv_end"
  type: "Convolution"
  bottom: "map16_36_conv2"
  top: "map16_36_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_36_eltsum"
  type: "Eltwise"
  bottom: "map16_35_eltsum"
  bottom: "map16_36_conv_end"
  top: "map16_36_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_37_bn_pre"
  type: "BatchNorm"
  bottom: "map16_36_eltsum"
  top: "map16_37_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_37_bn_pre"
  type: "BatchNorm"
  bottom: "map16_36_eltsum"
  top: "map16_37_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_37_pre_scale"
  type: "Scale"
  bottom: "map16_37_bn_pre"
  top: "map16_37_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_37_pre_relu"
  type: "ReLU"
  bottom: "map16_37_bn_pre"
  top: "map16_37_bn_pre"
}
layer {
  name: "map16_37_conv1"
  type: "Convolution"
  bottom: "map16_37_bn_pre"
  top: "map16_37_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_37_bn1"
  type: "BatchNorm"
  bottom: "map16_37_conv1"
  top: "map16_37_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_37_bn1"
  type: "BatchNorm"
  bottom: "map16_37_conv1"
  top: "map16_37_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_37_scale1"
  type: "Scale"
  bottom: "map16_37_conv1"
  top: "map16_37_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_37_relu1"
  type: "ReLU"
  bottom: "map16_37_conv1"
  top: "map16_37_conv1"
}
layer {
  name: "map16_37_conv2"
  type: "Convolution"
  bottom: "map16_37_conv1"
  top: "map16_37_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_37_bn2"
  type: "BatchNorm"
  bottom: "map16_37_conv2"
  top: "map16_37_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_37_bn2"
  type: "BatchNorm"
  bottom: "map16_37_conv2"
  top: "map16_37_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_37_scale2"
  type: "Scale"
  bottom: "map16_37_conv2"
  top: "map16_37_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_37_relu2"
  type: "ReLU"
  bottom: "map16_37_conv2"
  top: "map16_37_conv2"
}
layer {
  name: "map16_37_conv_end"
  type: "Convolution"
  bottom: "map16_37_conv2"
  top: "map16_37_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_37_eltsum"
  type: "Eltwise"
  bottom: "map16_36_eltsum"
  bottom: "map16_37_conv_end"
  top: "map16_37_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_38_bn_pre"
  type: "BatchNorm"
  bottom: "map16_37_eltsum"
  top: "map16_38_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_38_bn_pre"
  type: "BatchNorm"
  bottom: "map16_37_eltsum"
  top: "map16_38_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_38_pre_scale"
  type: "Scale"
  bottom: "map16_38_bn_pre"
  top: "map16_38_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_38_pre_relu"
  type: "ReLU"
  bottom: "map16_38_bn_pre"
  top: "map16_38_bn_pre"
}
layer {
  name: "map16_38_conv1"
  type: "Convolution"
  bottom: "map16_38_bn_pre"
  top: "map16_38_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_38_bn1"
  type: "BatchNorm"
  bottom: "map16_38_conv1"
  top: "map16_38_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_38_bn1"
  type: "BatchNorm"
  bottom: "map16_38_conv1"
  top: "map16_38_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_38_scale1"
  type: "Scale"
  bottom: "map16_38_conv1"
  top: "map16_38_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_38_relu1"
  type: "ReLU"
  bottom: "map16_38_conv1"
  top: "map16_38_conv1"
}
layer {
  name: "map16_38_conv2"
  type: "Convolution"
  bottom: "map16_38_conv1"
  top: "map16_38_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_38_bn2"
  type: "BatchNorm"
  bottom: "map16_38_conv2"
  top: "map16_38_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_38_bn2"
  type: "BatchNorm"
  bottom: "map16_38_conv2"
  top: "map16_38_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_38_scale2"
  type: "Scale"
  bottom: "map16_38_conv2"
  top: "map16_38_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_38_relu2"
  type: "ReLU"
  bottom: "map16_38_conv2"
  top: "map16_38_conv2"
}
layer {
  name: "map16_38_conv_end"
  type: "Convolution"
  bottom: "map16_38_conv2"
  top: "map16_38_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_38_eltsum"
  type: "Eltwise"
  bottom: "map16_37_eltsum"
  bottom: "map16_38_conv_end"
  top: "map16_38_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_39_bn_pre"
  type: "BatchNorm"
  bottom: "map16_38_eltsum"
  top: "map16_39_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_39_bn_pre"
  type: "BatchNorm"
  bottom: "map16_38_eltsum"
  top: "map16_39_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_39_pre_scale"
  type: "Scale"
  bottom: "map16_39_bn_pre"
  top: "map16_39_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_39_pre_relu"
  type: "ReLU"
  bottom: "map16_39_bn_pre"
  top: "map16_39_bn_pre"
}
layer {
  name: "map16_39_conv1"
  type: "Convolution"
  bottom: "map16_39_bn_pre"
  top: "map16_39_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_39_bn1"
  type: "BatchNorm"
  bottom: "map16_39_conv1"
  top: "map16_39_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_39_bn1"
  type: "BatchNorm"
  bottom: "map16_39_conv1"
  top: "map16_39_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_39_scale1"
  type: "Scale"
  bottom: "map16_39_conv1"
  top: "map16_39_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_39_relu1"
  type: "ReLU"
  bottom: "map16_39_conv1"
  top: "map16_39_conv1"
}
layer {
  name: "map16_39_conv2"
  type: "Convolution"
  bottom: "map16_39_conv1"
  top: "map16_39_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_39_bn2"
  type: "BatchNorm"
  bottom: "map16_39_conv2"
  top: "map16_39_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_39_bn2"
  type: "BatchNorm"
  bottom: "map16_39_conv2"
  top: "map16_39_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_39_scale2"
  type: "Scale"
  bottom: "map16_39_conv2"
  top: "map16_39_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_39_relu2"
  type: "ReLU"
  bottom: "map16_39_conv2"
  top: "map16_39_conv2"
}
layer {
  name: "map16_39_conv_end"
  type: "Convolution"
  bottom: "map16_39_conv2"
  top: "map16_39_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_39_eltsum"
  type: "Eltwise"
  bottom: "map16_38_eltsum"
  bottom: "map16_39_conv_end"
  top: "map16_39_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_40_bn_pre"
  type: "BatchNorm"
  bottom: "map16_39_eltsum"
  top: "map16_40_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_40_bn_pre"
  type: "BatchNorm"
  bottom: "map16_39_eltsum"
  top: "map16_40_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_40_pre_scale"
  type: "Scale"
  bottom: "map16_40_bn_pre"
  top: "map16_40_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_40_pre_relu"
  type: "ReLU"
  bottom: "map16_40_bn_pre"
  top: "map16_40_bn_pre"
}
layer {
  name: "map16_40_conv1"
  type: "Convolution"
  bottom: "map16_40_bn_pre"
  top: "map16_40_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_40_bn1"
  type: "BatchNorm"
  bottom: "map16_40_conv1"
  top: "map16_40_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_40_bn1"
  type: "BatchNorm"
  bottom: "map16_40_conv1"
  top: "map16_40_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_40_scale1"
  type: "Scale"
  bottom: "map16_40_conv1"
  top: "map16_40_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_40_relu1"
  type: "ReLU"
  bottom: "map16_40_conv1"
  top: "map16_40_conv1"
}
layer {
  name: "map16_40_conv2"
  type: "Convolution"
  bottom: "map16_40_conv1"
  top: "map16_40_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_40_bn2"
  type: "BatchNorm"
  bottom: "map16_40_conv2"
  top: "map16_40_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_40_bn2"
  type: "BatchNorm"
  bottom: "map16_40_conv2"
  top: "map16_40_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_40_scale2"
  type: "Scale"
  bottom: "map16_40_conv2"
  top: "map16_40_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_40_relu2"
  type: "ReLU"
  bottom: "map16_40_conv2"
  top: "map16_40_conv2"
}
layer {
  name: "map16_40_conv_end"
  type: "Convolution"
  bottom: "map16_40_conv2"
  top: "map16_40_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_40_eltsum"
  type: "Eltwise"
  bottom: "map16_39_eltsum"
  bottom: "map16_40_conv_end"
  top: "map16_40_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_41_bn_pre"
  type: "BatchNorm"
  bottom: "map16_40_eltsum"
  top: "map16_41_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_41_bn_pre"
  type: "BatchNorm"
  bottom: "map16_40_eltsum"
  top: "map16_41_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_41_pre_scale"
  type: "Scale"
  bottom: "map16_41_bn_pre"
  top: "map16_41_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_41_pre_relu"
  type: "ReLU"
  bottom: "map16_41_bn_pre"
  top: "map16_41_bn_pre"
}
layer {
  name: "map16_41_conv1"
  type: "Convolution"
  bottom: "map16_41_bn_pre"
  top: "map16_41_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_41_bn1"
  type: "BatchNorm"
  bottom: "map16_41_conv1"
  top: "map16_41_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_41_bn1"
  type: "BatchNorm"
  bottom: "map16_41_conv1"
  top: "map16_41_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_41_scale1"
  type: "Scale"
  bottom: "map16_41_conv1"
  top: "map16_41_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_41_relu1"
  type: "ReLU"
  bottom: "map16_41_conv1"
  top: "map16_41_conv1"
}
layer {
  name: "map16_41_conv2"
  type: "Convolution"
  bottom: "map16_41_conv1"
  top: "map16_41_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_41_bn2"
  type: "BatchNorm"
  bottom: "map16_41_conv2"
  top: "map16_41_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_41_bn2"
  type: "BatchNorm"
  bottom: "map16_41_conv2"
  top: "map16_41_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_41_scale2"
  type: "Scale"
  bottom: "map16_41_conv2"
  top: "map16_41_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_41_relu2"
  type: "ReLU"
  bottom: "map16_41_conv2"
  top: "map16_41_conv2"
}
layer {
  name: "map16_41_conv_end"
  type: "Convolution"
  bottom: "map16_41_conv2"
  top: "map16_41_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_41_eltsum"
  type: "Eltwise"
  bottom: "map16_40_eltsum"
  bottom: "map16_41_conv_end"
  top: "map16_41_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_42_bn_pre"
  type: "BatchNorm"
  bottom: "map16_41_eltsum"
  top: "map16_42_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_42_bn_pre"
  type: "BatchNorm"
  bottom: "map16_41_eltsum"
  top: "map16_42_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_42_pre_scale"
  type: "Scale"
  bottom: "map16_42_bn_pre"
  top: "map16_42_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_42_pre_relu"
  type: "ReLU"
  bottom: "map16_42_bn_pre"
  top: "map16_42_bn_pre"
}
layer {
  name: "map16_42_conv1"
  type: "Convolution"
  bottom: "map16_42_bn_pre"
  top: "map16_42_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_42_bn1"
  type: "BatchNorm"
  bottom: "map16_42_conv1"
  top: "map16_42_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_42_bn1"
  type: "BatchNorm"
  bottom: "map16_42_conv1"
  top: "map16_42_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_42_scale1"
  type: "Scale"
  bottom: "map16_42_conv1"
  top: "map16_42_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_42_relu1"
  type: "ReLU"
  bottom: "map16_42_conv1"
  top: "map16_42_conv1"
}
layer {
  name: "map16_42_conv2"
  type: "Convolution"
  bottom: "map16_42_conv1"
  top: "map16_42_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_42_bn2"
  type: "BatchNorm"
  bottom: "map16_42_conv2"
  top: "map16_42_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_42_bn2"
  type: "BatchNorm"
  bottom: "map16_42_conv2"
  top: "map16_42_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_42_scale2"
  type: "Scale"
  bottom: "map16_42_conv2"
  top: "map16_42_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_42_relu2"
  type: "ReLU"
  bottom: "map16_42_conv2"
  top: "map16_42_conv2"
}
layer {
  name: "map16_42_conv_end"
  type: "Convolution"
  bottom: "map16_42_conv2"
  top: "map16_42_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_42_eltsum"
  type: "Eltwise"
  bottom: "map16_41_eltsum"
  bottom: "map16_42_conv_end"
  top: "map16_42_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_43_bn_pre"
  type: "BatchNorm"
  bottom: "map16_42_eltsum"
  top: "map16_43_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_43_bn_pre"
  type: "BatchNorm"
  bottom: "map16_42_eltsum"
  top: "map16_43_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_43_pre_scale"
  type: "Scale"
  bottom: "map16_43_bn_pre"
  top: "map16_43_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_43_pre_relu"
  type: "ReLU"
  bottom: "map16_43_bn_pre"
  top: "map16_43_bn_pre"
}
layer {
  name: "map16_43_conv1"
  type: "Convolution"
  bottom: "map16_43_bn_pre"
  top: "map16_43_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_43_bn1"
  type: "BatchNorm"
  bottom: "map16_43_conv1"
  top: "map16_43_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_43_bn1"
  type: "BatchNorm"
  bottom: "map16_43_conv1"
  top: "map16_43_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_43_scale1"
  type: "Scale"
  bottom: "map16_43_conv1"
  top: "map16_43_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_43_relu1"
  type: "ReLU"
  bottom: "map16_43_conv1"
  top: "map16_43_conv1"
}
layer {
  name: "map16_43_conv2"
  type: "Convolution"
  bottom: "map16_43_conv1"
  top: "map16_43_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_43_bn2"
  type: "BatchNorm"
  bottom: "map16_43_conv2"
  top: "map16_43_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_43_bn2"
  type: "BatchNorm"
  bottom: "map16_43_conv2"
  top: "map16_43_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_43_scale2"
  type: "Scale"
  bottom: "map16_43_conv2"
  top: "map16_43_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_43_relu2"
  type: "ReLU"
  bottom: "map16_43_conv2"
  top: "map16_43_conv2"
}
layer {
  name: "map16_43_conv_end"
  type: "Convolution"
  bottom: "map16_43_conv2"
  top: "map16_43_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_43_eltsum"
  type: "Eltwise"
  bottom: "map16_42_eltsum"
  bottom: "map16_43_conv_end"
  top: "map16_43_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_44_bn_pre"
  type: "BatchNorm"
  bottom: "map16_43_eltsum"
  top: "map16_44_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_44_bn_pre"
  type: "BatchNorm"
  bottom: "map16_43_eltsum"
  top: "map16_44_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_44_pre_scale"
  type: "Scale"
  bottom: "map16_44_bn_pre"
  top: "map16_44_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_44_pre_relu"
  type: "ReLU"
  bottom: "map16_44_bn_pre"
  top: "map16_44_bn_pre"
}
layer {
  name: "map16_44_conv1"
  type: "Convolution"
  bottom: "map16_44_bn_pre"
  top: "map16_44_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_44_bn1"
  type: "BatchNorm"
  bottom: "map16_44_conv1"
  top: "map16_44_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_44_bn1"
  type: "BatchNorm"
  bottom: "map16_44_conv1"
  top: "map16_44_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_44_scale1"
  type: "Scale"
  bottom: "map16_44_conv1"
  top: "map16_44_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_44_relu1"
  type: "ReLU"
  bottom: "map16_44_conv1"
  top: "map16_44_conv1"
}
layer {
  name: "map16_44_conv2"
  type: "Convolution"
  bottom: "map16_44_conv1"
  top: "map16_44_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_44_bn2"
  type: "BatchNorm"
  bottom: "map16_44_conv2"
  top: "map16_44_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_44_bn2"
  type: "BatchNorm"
  bottom: "map16_44_conv2"
  top: "map16_44_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_44_scale2"
  type: "Scale"
  bottom: "map16_44_conv2"
  top: "map16_44_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_44_relu2"
  type: "ReLU"
  bottom: "map16_44_conv2"
  top: "map16_44_conv2"
}
layer {
  name: "map16_44_conv_end"
  type: "Convolution"
  bottom: "map16_44_conv2"
  top: "map16_44_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_44_eltsum"
  type: "Eltwise"
  bottom: "map16_43_eltsum"
  bottom: "map16_44_conv_end"
  top: "map16_44_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_45_bn_pre"
  type: "BatchNorm"
  bottom: "map16_44_eltsum"
  top: "map16_45_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_45_bn_pre"
  type: "BatchNorm"
  bottom: "map16_44_eltsum"
  top: "map16_45_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_45_pre_scale"
  type: "Scale"
  bottom: "map16_45_bn_pre"
  top: "map16_45_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_45_pre_relu"
  type: "ReLU"
  bottom: "map16_45_bn_pre"
  top: "map16_45_bn_pre"
}
layer {
  name: "map16_45_conv1"
  type: "Convolution"
  bottom: "map16_45_bn_pre"
  top: "map16_45_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_45_bn1"
  type: "BatchNorm"
  bottom: "map16_45_conv1"
  top: "map16_45_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_45_bn1"
  type: "BatchNorm"
  bottom: "map16_45_conv1"
  top: "map16_45_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_45_scale1"
  type: "Scale"
  bottom: "map16_45_conv1"
  top: "map16_45_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_45_relu1"
  type: "ReLU"
  bottom: "map16_45_conv1"
  top: "map16_45_conv1"
}
layer {
  name: "map16_45_conv2"
  type: "Convolution"
  bottom: "map16_45_conv1"
  top: "map16_45_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_45_bn2"
  type: "BatchNorm"
  bottom: "map16_45_conv2"
  top: "map16_45_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_45_bn2"
  type: "BatchNorm"
  bottom: "map16_45_conv2"
  top: "map16_45_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_45_scale2"
  type: "Scale"
  bottom: "map16_45_conv2"
  top: "map16_45_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_45_relu2"
  type: "ReLU"
  bottom: "map16_45_conv2"
  top: "map16_45_conv2"
}
layer {
  name: "map16_45_conv_end"
  type: "Convolution"
  bottom: "map16_45_conv2"
  top: "map16_45_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_45_eltsum"
  type: "Eltwise"
  bottom: "map16_44_eltsum"
  bottom: "map16_45_conv_end"
  top: "map16_45_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_46_bn_pre"
  type: "BatchNorm"
  bottom: "map16_45_eltsum"
  top: "map16_46_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_46_bn_pre"
  type: "BatchNorm"
  bottom: "map16_45_eltsum"
  top: "map16_46_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_46_pre_scale"
  type: "Scale"
  bottom: "map16_46_bn_pre"
  top: "map16_46_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_46_pre_relu"
  type: "ReLU"
  bottom: "map16_46_bn_pre"
  top: "map16_46_bn_pre"
}
layer {
  name: "map16_46_conv1"
  type: "Convolution"
  bottom: "map16_46_bn_pre"
  top: "map16_46_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_46_bn1"
  type: "BatchNorm"
  bottom: "map16_46_conv1"
  top: "map16_46_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_46_bn1"
  type: "BatchNorm"
  bottom: "map16_46_conv1"
  top: "map16_46_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_46_scale1"
  type: "Scale"
  bottom: "map16_46_conv1"
  top: "map16_46_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_46_relu1"
  type: "ReLU"
  bottom: "map16_46_conv1"
  top: "map16_46_conv1"
}
layer {
  name: "map16_46_conv2"
  type: "Convolution"
  bottom: "map16_46_conv1"
  top: "map16_46_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_46_bn2"
  type: "BatchNorm"
  bottom: "map16_46_conv2"
  top: "map16_46_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_46_bn2"
  type: "BatchNorm"
  bottom: "map16_46_conv2"
  top: "map16_46_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_46_scale2"
  type: "Scale"
  bottom: "map16_46_conv2"
  top: "map16_46_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_46_relu2"
  type: "ReLU"
  bottom: "map16_46_conv2"
  top: "map16_46_conv2"
}
layer {
  name: "map16_46_conv_end"
  type: "Convolution"
  bottom: "map16_46_conv2"
  top: "map16_46_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_46_eltsum"
  type: "Eltwise"
  bottom: "map16_45_eltsum"
  bottom: "map16_46_conv_end"
  top: "map16_46_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_47_bn_pre"
  type: "BatchNorm"
  bottom: "map16_46_eltsum"
  top: "map16_47_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_47_bn_pre"
  type: "BatchNorm"
  bottom: "map16_46_eltsum"
  top: "map16_47_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_47_pre_scale"
  type: "Scale"
  bottom: "map16_47_bn_pre"
  top: "map16_47_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_47_pre_relu"
  type: "ReLU"
  bottom: "map16_47_bn_pre"
  top: "map16_47_bn_pre"
}
layer {
  name: "map16_47_conv1"
  type: "Convolution"
  bottom: "map16_47_bn_pre"
  top: "map16_47_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_47_bn1"
  type: "BatchNorm"
  bottom: "map16_47_conv1"
  top: "map16_47_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_47_bn1"
  type: "BatchNorm"
  bottom: "map16_47_conv1"
  top: "map16_47_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_47_scale1"
  type: "Scale"
  bottom: "map16_47_conv1"
  top: "map16_47_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_47_relu1"
  type: "ReLU"
  bottom: "map16_47_conv1"
  top: "map16_47_conv1"
}
layer {
  name: "map16_47_conv2"
  type: "Convolution"
  bottom: "map16_47_conv1"
  top: "map16_47_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_47_bn2"
  type: "BatchNorm"
  bottom: "map16_47_conv2"
  top: "map16_47_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_47_bn2"
  type: "BatchNorm"
  bottom: "map16_47_conv2"
  top: "map16_47_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_47_scale2"
  type: "Scale"
  bottom: "map16_47_conv2"
  top: "map16_47_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_47_relu2"
  type: "ReLU"
  bottom: "map16_47_conv2"
  top: "map16_47_conv2"
}
layer {
  name: "map16_47_conv_end"
  type: "Convolution"
  bottom: "map16_47_conv2"
  top: "map16_47_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_47_eltsum"
  type: "Eltwise"
  bottom: "map16_46_eltsum"
  bottom: "map16_47_conv_end"
  top: "map16_47_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_48_bn_pre"
  type: "BatchNorm"
  bottom: "map16_47_eltsum"
  top: "map16_48_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_48_bn_pre"
  type: "BatchNorm"
  bottom: "map16_47_eltsum"
  top: "map16_48_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_48_pre_scale"
  type: "Scale"
  bottom: "map16_48_bn_pre"
  top: "map16_48_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_48_pre_relu"
  type: "ReLU"
  bottom: "map16_48_bn_pre"
  top: "map16_48_bn_pre"
}
layer {
  name: "map16_48_conv1"
  type: "Convolution"
  bottom: "map16_48_bn_pre"
  top: "map16_48_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_48_bn1"
  type: "BatchNorm"
  bottom: "map16_48_conv1"
  top: "map16_48_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_48_bn1"
  type: "BatchNorm"
  bottom: "map16_48_conv1"
  top: "map16_48_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_48_scale1"
  type: "Scale"
  bottom: "map16_48_conv1"
  top: "map16_48_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_48_relu1"
  type: "ReLU"
  bottom: "map16_48_conv1"
  top: "map16_48_conv1"
}
layer {
  name: "map16_48_conv2"
  type: "Convolution"
  bottom: "map16_48_conv1"
  top: "map16_48_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_48_bn2"
  type: "BatchNorm"
  bottom: "map16_48_conv2"
  top: "map16_48_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_48_bn2"
  type: "BatchNorm"
  bottom: "map16_48_conv2"
  top: "map16_48_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_48_scale2"
  type: "Scale"
  bottom: "map16_48_conv2"
  top: "map16_48_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_48_relu2"
  type: "ReLU"
  bottom: "map16_48_conv2"
  top: "map16_48_conv2"
}
layer {
  name: "map16_48_conv_end"
  type: "Convolution"
  bottom: "map16_48_conv2"
  top: "map16_48_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_48_eltsum"
  type: "Eltwise"
  bottom: "map16_47_eltsum"
  bottom: "map16_48_conv_end"
  top: "map16_48_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_49_bn_pre"
  type: "BatchNorm"
  bottom: "map16_48_eltsum"
  top: "map16_49_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_49_bn_pre"
  type: "BatchNorm"
  bottom: "map16_48_eltsum"
  top: "map16_49_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_49_pre_scale"
  type: "Scale"
  bottom: "map16_49_bn_pre"
  top: "map16_49_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_49_pre_relu"
  type: "ReLU"
  bottom: "map16_49_bn_pre"
  top: "map16_49_bn_pre"
}
layer {
  name: "map16_49_conv1"
  type: "Convolution"
  bottom: "map16_49_bn_pre"
  top: "map16_49_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_49_bn1"
  type: "BatchNorm"
  bottom: "map16_49_conv1"
  top: "map16_49_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_49_bn1"
  type: "BatchNorm"
  bottom: "map16_49_conv1"
  top: "map16_49_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_49_scale1"
  type: "Scale"
  bottom: "map16_49_conv1"
  top: "map16_49_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_49_relu1"
  type: "ReLU"
  bottom: "map16_49_conv1"
  top: "map16_49_conv1"
}
layer {
  name: "map16_49_conv2"
  type: "Convolution"
  bottom: "map16_49_conv1"
  top: "map16_49_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_49_bn2"
  type: "BatchNorm"
  bottom: "map16_49_conv2"
  top: "map16_49_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_49_bn2"
  type: "BatchNorm"
  bottom: "map16_49_conv2"
  top: "map16_49_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_49_scale2"
  type: "Scale"
  bottom: "map16_49_conv2"
  top: "map16_49_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_49_relu2"
  type: "ReLU"
  bottom: "map16_49_conv2"
  top: "map16_49_conv2"
}
layer {
  name: "map16_49_conv_end"
  type: "Convolution"
  bottom: "map16_49_conv2"
  top: "map16_49_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_49_eltsum"
  type: "Eltwise"
  bottom: "map16_48_eltsum"
  bottom: "map16_49_conv_end"
  top: "map16_49_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_50_bn_pre"
  type: "BatchNorm"
  bottom: "map16_49_eltsum"
  top: "map16_50_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_50_bn_pre"
  type: "BatchNorm"
  bottom: "map16_49_eltsum"
  top: "map16_50_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_50_pre_scale"
  type: "Scale"
  bottom: "map16_50_bn_pre"
  top: "map16_50_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_50_pre_relu"
  type: "ReLU"
  bottom: "map16_50_bn_pre"
  top: "map16_50_bn_pre"
}
layer {
  name: "map16_50_conv1"
  type: "Convolution"
  bottom: "map16_50_bn_pre"
  top: "map16_50_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_50_bn1"
  type: "BatchNorm"
  bottom: "map16_50_conv1"
  top: "map16_50_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_50_bn1"
  type: "BatchNorm"
  bottom: "map16_50_conv1"
  top: "map16_50_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_50_scale1"
  type: "Scale"
  bottom: "map16_50_conv1"
  top: "map16_50_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_50_relu1"
  type: "ReLU"
  bottom: "map16_50_conv1"
  top: "map16_50_conv1"
}
layer {
  name: "map16_50_conv2"
  type: "Convolution"
  bottom: "map16_50_conv1"
  top: "map16_50_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_50_bn2"
  type: "BatchNorm"
  bottom: "map16_50_conv2"
  top: "map16_50_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_50_bn2"
  type: "BatchNorm"
  bottom: "map16_50_conv2"
  top: "map16_50_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_50_scale2"
  type: "Scale"
  bottom: "map16_50_conv2"
  top: "map16_50_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_50_relu2"
  type: "ReLU"
  bottom: "map16_50_conv2"
  top: "map16_50_conv2"
}
layer {
  name: "map16_50_conv_end"
  type: "Convolution"
  bottom: "map16_50_conv2"
  top: "map16_50_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_50_eltsum"
  type: "Eltwise"
  bottom: "map16_49_eltsum"
  bottom: "map16_50_conv_end"
  top: "map16_50_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_51_bn_pre"
  type: "BatchNorm"
  bottom: "map16_50_eltsum"
  top: "map16_51_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_51_bn_pre"
  type: "BatchNorm"
  bottom: "map16_50_eltsum"
  top: "map16_51_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_51_pre_scale"
  type: "Scale"
  bottom: "map16_51_bn_pre"
  top: "map16_51_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_51_pre_relu"
  type: "ReLU"
  bottom: "map16_51_bn_pre"
  top: "map16_51_bn_pre"
}
layer {
  name: "map16_51_conv1"
  type: "Convolution"
  bottom: "map16_51_bn_pre"
  top: "map16_51_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_51_bn1"
  type: "BatchNorm"
  bottom: "map16_51_conv1"
  top: "map16_51_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_51_bn1"
  type: "BatchNorm"
  bottom: "map16_51_conv1"
  top: "map16_51_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_51_scale1"
  type: "Scale"
  bottom: "map16_51_conv1"
  top: "map16_51_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_51_relu1"
  type: "ReLU"
  bottom: "map16_51_conv1"
  top: "map16_51_conv1"
}
layer {
  name: "map16_51_conv2"
  type: "Convolution"
  bottom: "map16_51_conv1"
  top: "map16_51_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_51_bn2"
  type: "BatchNorm"
  bottom: "map16_51_conv2"
  top: "map16_51_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_51_bn2"
  type: "BatchNorm"
  bottom: "map16_51_conv2"
  top: "map16_51_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_51_scale2"
  type: "Scale"
  bottom: "map16_51_conv2"
  top: "map16_51_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_51_relu2"
  type: "ReLU"
  bottom: "map16_51_conv2"
  top: "map16_51_conv2"
}
layer {
  name: "map16_51_conv_end"
  type: "Convolution"
  bottom: "map16_51_conv2"
  top: "map16_51_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_51_eltsum"
  type: "Eltwise"
  bottom: "map16_50_eltsum"
  bottom: "map16_51_conv_end"
  top: "map16_51_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_52_bn_pre"
  type: "BatchNorm"
  bottom: "map16_51_eltsum"
  top: "map16_52_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_52_bn_pre"
  type: "BatchNorm"
  bottom: "map16_51_eltsum"
  top: "map16_52_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_52_pre_scale"
  type: "Scale"
  bottom: "map16_52_bn_pre"
  top: "map16_52_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_52_pre_relu"
  type: "ReLU"
  bottom: "map16_52_bn_pre"
  top: "map16_52_bn_pre"
}
layer {
  name: "map16_52_conv1"
  type: "Convolution"
  bottom: "map16_52_bn_pre"
  top: "map16_52_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_52_bn1"
  type: "BatchNorm"
  bottom: "map16_52_conv1"
  top: "map16_52_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_52_bn1"
  type: "BatchNorm"
  bottom: "map16_52_conv1"
  top: "map16_52_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_52_scale1"
  type: "Scale"
  bottom: "map16_52_conv1"
  top: "map16_52_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_52_relu1"
  type: "ReLU"
  bottom: "map16_52_conv1"
  top: "map16_52_conv1"
}
layer {
  name: "map16_52_conv2"
  type: "Convolution"
  bottom: "map16_52_conv1"
  top: "map16_52_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_52_bn2"
  type: "BatchNorm"
  bottom: "map16_52_conv2"
  top: "map16_52_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_52_bn2"
  type: "BatchNorm"
  bottom: "map16_52_conv2"
  top: "map16_52_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_52_scale2"
  type: "Scale"
  bottom: "map16_52_conv2"
  top: "map16_52_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_52_relu2"
  type: "ReLU"
  bottom: "map16_52_conv2"
  top: "map16_52_conv2"
}
layer {
  name: "map16_52_conv_end"
  type: "Convolution"
  bottom: "map16_52_conv2"
  top: "map16_52_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_52_eltsum"
  type: "Eltwise"
  bottom: "map16_51_eltsum"
  bottom: "map16_52_conv_end"
  top: "map16_52_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_53_bn_pre"
  type: "BatchNorm"
  bottom: "map16_52_eltsum"
  top: "map16_53_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_53_bn_pre"
  type: "BatchNorm"
  bottom: "map16_52_eltsum"
  top: "map16_53_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_53_pre_scale"
  type: "Scale"
  bottom: "map16_53_bn_pre"
  top: "map16_53_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_53_pre_relu"
  type: "ReLU"
  bottom: "map16_53_bn_pre"
  top: "map16_53_bn_pre"
}
layer {
  name: "map16_53_conv1"
  type: "Convolution"
  bottom: "map16_53_bn_pre"
  top: "map16_53_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_53_bn1"
  type: "BatchNorm"
  bottom: "map16_53_conv1"
  top: "map16_53_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_53_bn1"
  type: "BatchNorm"
  bottom: "map16_53_conv1"
  top: "map16_53_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_53_scale1"
  type: "Scale"
  bottom: "map16_53_conv1"
  top: "map16_53_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_53_relu1"
  type: "ReLU"
  bottom: "map16_53_conv1"
  top: "map16_53_conv1"
}
layer {
  name: "map16_53_conv2"
  type: "Convolution"
  bottom: "map16_53_conv1"
  top: "map16_53_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_53_bn2"
  type: "BatchNorm"
  bottom: "map16_53_conv2"
  top: "map16_53_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_53_bn2"
  type: "BatchNorm"
  bottom: "map16_53_conv2"
  top: "map16_53_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_53_scale2"
  type: "Scale"
  bottom: "map16_53_conv2"
  top: "map16_53_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_53_relu2"
  type: "ReLU"
  bottom: "map16_53_conv2"
  top: "map16_53_conv2"
}
layer {
  name: "map16_53_conv_end"
  type: "Convolution"
  bottom: "map16_53_conv2"
  top: "map16_53_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_53_eltsum"
  type: "Eltwise"
  bottom: "map16_52_eltsum"
  bottom: "map16_53_conv_end"
  top: "map16_53_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_54_bn_pre"
  type: "BatchNorm"
  bottom: "map16_53_eltsum"
  top: "map16_54_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_54_bn_pre"
  type: "BatchNorm"
  bottom: "map16_53_eltsum"
  top: "map16_54_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_54_pre_scale"
  type: "Scale"
  bottom: "map16_54_bn_pre"
  top: "map16_54_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_54_pre_relu"
  type: "ReLU"
  bottom: "map16_54_bn_pre"
  top: "map16_54_bn_pre"
}
layer {
  name: "map16_54_conv1"
  type: "Convolution"
  bottom: "map16_54_bn_pre"
  top: "map16_54_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_54_bn1"
  type: "BatchNorm"
  bottom: "map16_54_conv1"
  top: "map16_54_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_54_bn1"
  type: "BatchNorm"
  bottom: "map16_54_conv1"
  top: "map16_54_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_54_scale1"
  type: "Scale"
  bottom: "map16_54_conv1"
  top: "map16_54_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_54_relu1"
  type: "ReLU"
  bottom: "map16_54_conv1"
  top: "map16_54_conv1"
}
layer {
  name: "map16_54_conv2"
  type: "Convolution"
  bottom: "map16_54_conv1"
  top: "map16_54_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_54_bn2"
  type: "BatchNorm"
  bottom: "map16_54_conv2"
  top: "map16_54_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_54_bn2"
  type: "BatchNorm"
  bottom: "map16_54_conv2"
  top: "map16_54_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_54_scale2"
  type: "Scale"
  bottom: "map16_54_conv2"
  top: "map16_54_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_54_relu2"
  type: "ReLU"
  bottom: "map16_54_conv2"
  top: "map16_54_conv2"
}
layer {
  name: "map16_54_conv_end"
  type: "Convolution"
  bottom: "map16_54_conv2"
  top: "map16_54_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_54_eltsum"
  type: "Eltwise"
  bottom: "map16_53_eltsum"
  bottom: "map16_54_conv_end"
  top: "map16_54_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_55_bn_pre"
  type: "BatchNorm"
  bottom: "map16_54_eltsum"
  top: "map16_55_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_55_bn_pre"
  type: "BatchNorm"
  bottom: "map16_54_eltsum"
  top: "map16_55_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_55_pre_scale"
  type: "Scale"
  bottom: "map16_55_bn_pre"
  top: "map16_55_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_55_pre_relu"
  type: "ReLU"
  bottom: "map16_55_bn_pre"
  top: "map16_55_bn_pre"
}
layer {
  name: "map16_55_conv1"
  type: "Convolution"
  bottom: "map16_55_bn_pre"
  top: "map16_55_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_55_bn1"
  type: "BatchNorm"
  bottom: "map16_55_conv1"
  top: "map16_55_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_55_bn1"
  type: "BatchNorm"
  bottom: "map16_55_conv1"
  top: "map16_55_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_55_scale1"
  type: "Scale"
  bottom: "map16_55_conv1"
  top: "map16_55_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_55_relu1"
  type: "ReLU"
  bottom: "map16_55_conv1"
  top: "map16_55_conv1"
}
layer {
  name: "map16_55_conv2"
  type: "Convolution"
  bottom: "map16_55_conv1"
  top: "map16_55_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_55_bn2"
  type: "BatchNorm"
  bottom: "map16_55_conv2"
  top: "map16_55_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_55_bn2"
  type: "BatchNorm"
  bottom: "map16_55_conv2"
  top: "map16_55_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_55_scale2"
  type: "Scale"
  bottom: "map16_55_conv2"
  top: "map16_55_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_55_relu2"
  type: "ReLU"
  bottom: "map16_55_conv2"
  top: "map16_55_conv2"
}
layer {
  name: "map16_55_conv_end"
  type: "Convolution"
  bottom: "map16_55_conv2"
  top: "map16_55_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_55_eltsum"
  type: "Eltwise"
  bottom: "map16_54_eltsum"
  bottom: "map16_55_conv_end"
  top: "map16_55_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_56_bn_pre"
  type: "BatchNorm"
  bottom: "map16_55_eltsum"
  top: "map16_56_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_56_bn_pre"
  type: "BatchNorm"
  bottom: "map16_55_eltsum"
  top: "map16_56_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_56_pre_scale"
  type: "Scale"
  bottom: "map16_56_bn_pre"
  top: "map16_56_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_56_pre_relu"
  type: "ReLU"
  bottom: "map16_56_bn_pre"
  top: "map16_56_bn_pre"
}
layer {
  name: "map16_56_conv1"
  type: "Convolution"
  bottom: "map16_56_bn_pre"
  top: "map16_56_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_56_bn1"
  type: "BatchNorm"
  bottom: "map16_56_conv1"
  top: "map16_56_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_56_bn1"
  type: "BatchNorm"
  bottom: "map16_56_conv1"
  top: "map16_56_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_56_scale1"
  type: "Scale"
  bottom: "map16_56_conv1"
  top: "map16_56_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_56_relu1"
  type: "ReLU"
  bottom: "map16_56_conv1"
  top: "map16_56_conv1"
}
layer {
  name: "map16_56_conv2"
  type: "Convolution"
  bottom: "map16_56_conv1"
  top: "map16_56_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_56_bn2"
  type: "BatchNorm"
  bottom: "map16_56_conv2"
  top: "map16_56_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_56_bn2"
  type: "BatchNorm"
  bottom: "map16_56_conv2"
  top: "map16_56_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_56_scale2"
  type: "Scale"
  bottom: "map16_56_conv2"
  top: "map16_56_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_56_relu2"
  type: "ReLU"
  bottom: "map16_56_conv2"
  top: "map16_56_conv2"
}
layer {
  name: "map16_56_conv_end"
  type: "Convolution"
  bottom: "map16_56_conv2"
  top: "map16_56_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_56_eltsum"
  type: "Eltwise"
  bottom: "map16_55_eltsum"
  bottom: "map16_56_conv_end"
  top: "map16_56_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_57_bn_pre"
  type: "BatchNorm"
  bottom: "map16_56_eltsum"
  top: "map16_57_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_57_bn_pre"
  type: "BatchNorm"
  bottom: "map16_56_eltsum"
  top: "map16_57_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_57_pre_scale"
  type: "Scale"
  bottom: "map16_57_bn_pre"
  top: "map16_57_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_57_pre_relu"
  type: "ReLU"
  bottom: "map16_57_bn_pre"
  top: "map16_57_bn_pre"
}
layer {
  name: "map16_57_conv1"
  type: "Convolution"
  bottom: "map16_57_bn_pre"
  top: "map16_57_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_57_bn1"
  type: "BatchNorm"
  bottom: "map16_57_conv1"
  top: "map16_57_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_57_bn1"
  type: "BatchNorm"
  bottom: "map16_57_conv1"
  top: "map16_57_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_57_scale1"
  type: "Scale"
  bottom: "map16_57_conv1"
  top: "map16_57_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_57_relu1"
  type: "ReLU"
  bottom: "map16_57_conv1"
  top: "map16_57_conv1"
}
layer {
  name: "map16_57_conv2"
  type: "Convolution"
  bottom: "map16_57_conv1"
  top: "map16_57_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_57_bn2"
  type: "BatchNorm"
  bottom: "map16_57_conv2"
  top: "map16_57_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_57_bn2"
  type: "BatchNorm"
  bottom: "map16_57_conv2"
  top: "map16_57_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_57_scale2"
  type: "Scale"
  bottom: "map16_57_conv2"
  top: "map16_57_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_57_relu2"
  type: "ReLU"
  bottom: "map16_57_conv2"
  top: "map16_57_conv2"
}
layer {
  name: "map16_57_conv_end"
  type: "Convolution"
  bottom: "map16_57_conv2"
  top: "map16_57_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_57_eltsum"
  type: "Eltwise"
  bottom: "map16_56_eltsum"
  bottom: "map16_57_conv_end"
  top: "map16_57_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_58_bn_pre"
  type: "BatchNorm"
  bottom: "map16_57_eltsum"
  top: "map16_58_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_58_bn_pre"
  type: "BatchNorm"
  bottom: "map16_57_eltsum"
  top: "map16_58_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_58_pre_scale"
  type: "Scale"
  bottom: "map16_58_bn_pre"
  top: "map16_58_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_58_pre_relu"
  type: "ReLU"
  bottom: "map16_58_bn_pre"
  top: "map16_58_bn_pre"
}
layer {
  name: "map16_58_conv1"
  type: "Convolution"
  bottom: "map16_58_bn_pre"
  top: "map16_58_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_58_bn1"
  type: "BatchNorm"
  bottom: "map16_58_conv1"
  top: "map16_58_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_58_bn1"
  type: "BatchNorm"
  bottom: "map16_58_conv1"
  top: "map16_58_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_58_scale1"
  type: "Scale"
  bottom: "map16_58_conv1"
  top: "map16_58_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_58_relu1"
  type: "ReLU"
  bottom: "map16_58_conv1"
  top: "map16_58_conv1"
}
layer {
  name: "map16_58_conv2"
  type: "Convolution"
  bottom: "map16_58_conv1"
  top: "map16_58_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_58_bn2"
  type: "BatchNorm"
  bottom: "map16_58_conv2"
  top: "map16_58_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_58_bn2"
  type: "BatchNorm"
  bottom: "map16_58_conv2"
  top: "map16_58_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_58_scale2"
  type: "Scale"
  bottom: "map16_58_conv2"
  top: "map16_58_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_58_relu2"
  type: "ReLU"
  bottom: "map16_58_conv2"
  top: "map16_58_conv2"
}
layer {
  name: "map16_58_conv_end"
  type: "Convolution"
  bottom: "map16_58_conv2"
  top: "map16_58_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_58_eltsum"
  type: "Eltwise"
  bottom: "map16_57_eltsum"
  bottom: "map16_58_conv_end"
  top: "map16_58_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_59_bn_pre"
  type: "BatchNorm"
  bottom: "map16_58_eltsum"
  top: "map16_59_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_59_bn_pre"
  type: "BatchNorm"
  bottom: "map16_58_eltsum"
  top: "map16_59_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_59_pre_scale"
  type: "Scale"
  bottom: "map16_59_bn_pre"
  top: "map16_59_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_59_pre_relu"
  type: "ReLU"
  bottom: "map16_59_bn_pre"
  top: "map16_59_bn_pre"
}
layer {
  name: "map16_59_conv1"
  type: "Convolution"
  bottom: "map16_59_bn_pre"
  top: "map16_59_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_59_bn1"
  type: "BatchNorm"
  bottom: "map16_59_conv1"
  top: "map16_59_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_59_bn1"
  type: "BatchNorm"
  bottom: "map16_59_conv1"
  top: "map16_59_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_59_scale1"
  type: "Scale"
  bottom: "map16_59_conv1"
  top: "map16_59_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_59_relu1"
  type: "ReLU"
  bottom: "map16_59_conv1"
  top: "map16_59_conv1"
}
layer {
  name: "map16_59_conv2"
  type: "Convolution"
  bottom: "map16_59_conv1"
  top: "map16_59_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_59_bn2"
  type: "BatchNorm"
  bottom: "map16_59_conv2"
  top: "map16_59_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_59_bn2"
  type: "BatchNorm"
  bottom: "map16_59_conv2"
  top: "map16_59_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_59_scale2"
  type: "Scale"
  bottom: "map16_59_conv2"
  top: "map16_59_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_59_relu2"
  type: "ReLU"
  bottom: "map16_59_conv2"
  top: "map16_59_conv2"
}
layer {
  name: "map16_59_conv_end"
  type: "Convolution"
  bottom: "map16_59_conv2"
  top: "map16_59_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_59_eltsum"
  type: "Eltwise"
  bottom: "map16_58_eltsum"
  bottom: "map16_59_conv_end"
  top: "map16_59_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_60_bn_pre"
  type: "BatchNorm"
  bottom: "map16_59_eltsum"
  top: "map16_60_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_60_bn_pre"
  type: "BatchNorm"
  bottom: "map16_59_eltsum"
  top: "map16_60_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_60_pre_scale"
  type: "Scale"
  bottom: "map16_60_bn_pre"
  top: "map16_60_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_60_pre_relu"
  type: "ReLU"
  bottom: "map16_60_bn_pre"
  top: "map16_60_bn_pre"
}
layer {
  name: "map16_60_conv1"
  type: "Convolution"
  bottom: "map16_60_bn_pre"
  top: "map16_60_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_60_bn1"
  type: "BatchNorm"
  bottom: "map16_60_conv1"
  top: "map16_60_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_60_bn1"
  type: "BatchNorm"
  bottom: "map16_60_conv1"
  top: "map16_60_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_60_scale1"
  type: "Scale"
  bottom: "map16_60_conv1"
  top: "map16_60_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_60_relu1"
  type: "ReLU"
  bottom: "map16_60_conv1"
  top: "map16_60_conv1"
}
layer {
  name: "map16_60_conv2"
  type: "Convolution"
  bottom: "map16_60_conv1"
  top: "map16_60_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_60_bn2"
  type: "BatchNorm"
  bottom: "map16_60_conv2"
  top: "map16_60_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_60_bn2"
  type: "BatchNorm"
  bottom: "map16_60_conv2"
  top: "map16_60_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_60_scale2"
  type: "Scale"
  bottom: "map16_60_conv2"
  top: "map16_60_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_60_relu2"
  type: "ReLU"
  bottom: "map16_60_conv2"
  top: "map16_60_conv2"
}
layer {
  name: "map16_60_conv_end"
  type: "Convolution"
  bottom: "map16_60_conv2"
  top: "map16_60_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_60_eltsum"
  type: "Eltwise"
  bottom: "map16_59_eltsum"
  bottom: "map16_60_conv_end"
  top: "map16_60_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_61_bn_pre"
  type: "BatchNorm"
  bottom: "map16_60_eltsum"
  top: "map16_61_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_61_bn_pre"
  type: "BatchNorm"
  bottom: "map16_60_eltsum"
  top: "map16_61_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_61_pre_scale"
  type: "Scale"
  bottom: "map16_61_bn_pre"
  top: "map16_61_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_61_pre_relu"
  type: "ReLU"
  bottom: "map16_61_bn_pre"
  top: "map16_61_bn_pre"
}
layer {
  name: "map16_61_conv1"
  type: "Convolution"
  bottom: "map16_61_bn_pre"
  top: "map16_61_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_61_bn1"
  type: "BatchNorm"
  bottom: "map16_61_conv1"
  top: "map16_61_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_61_bn1"
  type: "BatchNorm"
  bottom: "map16_61_conv1"
  top: "map16_61_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_61_scale1"
  type: "Scale"
  bottom: "map16_61_conv1"
  top: "map16_61_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_61_relu1"
  type: "ReLU"
  bottom: "map16_61_conv1"
  top: "map16_61_conv1"
}
layer {
  name: "map16_61_conv2"
  type: "Convolution"
  bottom: "map16_61_conv1"
  top: "map16_61_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_61_bn2"
  type: "BatchNorm"
  bottom: "map16_61_conv2"
  top: "map16_61_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_61_bn2"
  type: "BatchNorm"
  bottom: "map16_61_conv2"
  top: "map16_61_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_61_scale2"
  type: "Scale"
  bottom: "map16_61_conv2"
  top: "map16_61_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_61_relu2"
  type: "ReLU"
  bottom: "map16_61_conv2"
  top: "map16_61_conv2"
}
layer {
  name: "map16_61_conv_end"
  type: "Convolution"
  bottom: "map16_61_conv2"
  top: "map16_61_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_61_eltsum"
  type: "Eltwise"
  bottom: "map16_60_eltsum"
  bottom: "map16_61_conv_end"
  top: "map16_61_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_62_bn_pre"
  type: "BatchNorm"
  bottom: "map16_61_eltsum"
  top: "map16_62_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_62_bn_pre"
  type: "BatchNorm"
  bottom: "map16_61_eltsum"
  top: "map16_62_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_62_pre_scale"
  type: "Scale"
  bottom: "map16_62_bn_pre"
  top: "map16_62_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_62_pre_relu"
  type: "ReLU"
  bottom: "map16_62_bn_pre"
  top: "map16_62_bn_pre"
}
layer {
  name: "map16_62_conv1"
  type: "Convolution"
  bottom: "map16_62_bn_pre"
  top: "map16_62_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_62_bn1"
  type: "BatchNorm"
  bottom: "map16_62_conv1"
  top: "map16_62_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_62_bn1"
  type: "BatchNorm"
  bottom: "map16_62_conv1"
  top: "map16_62_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_62_scale1"
  type: "Scale"
  bottom: "map16_62_conv1"
  top: "map16_62_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_62_relu1"
  type: "ReLU"
  bottom: "map16_62_conv1"
  top: "map16_62_conv1"
}
layer {
  name: "map16_62_conv2"
  type: "Convolution"
  bottom: "map16_62_conv1"
  top: "map16_62_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_62_bn2"
  type: "BatchNorm"
  bottom: "map16_62_conv2"
  top: "map16_62_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_62_bn2"
  type: "BatchNorm"
  bottom: "map16_62_conv2"
  top: "map16_62_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_62_scale2"
  type: "Scale"
  bottom: "map16_62_conv2"
  top: "map16_62_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_62_relu2"
  type: "ReLU"
  bottom: "map16_62_conv2"
  top: "map16_62_conv2"
}
layer {
  name: "map16_62_conv_end"
  type: "Convolution"
  bottom: "map16_62_conv2"
  top: "map16_62_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_62_eltsum"
  type: "Eltwise"
  bottom: "map16_61_eltsum"
  bottom: "map16_62_conv_end"
  top: "map16_62_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_63_bn_pre"
  type: "BatchNorm"
  bottom: "map16_62_eltsum"
  top: "map16_63_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_63_bn_pre"
  type: "BatchNorm"
  bottom: "map16_62_eltsum"
  top: "map16_63_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_63_pre_scale"
  type: "Scale"
  bottom: "map16_63_bn_pre"
  top: "map16_63_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_63_pre_relu"
  type: "ReLU"
  bottom: "map16_63_bn_pre"
  top: "map16_63_bn_pre"
}
layer {
  name: "map16_63_conv1"
  type: "Convolution"
  bottom: "map16_63_bn_pre"
  top: "map16_63_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_63_bn1"
  type: "BatchNorm"
  bottom: "map16_63_conv1"
  top: "map16_63_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_63_bn1"
  type: "BatchNorm"
  bottom: "map16_63_conv1"
  top: "map16_63_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_63_scale1"
  type: "Scale"
  bottom: "map16_63_conv1"
  top: "map16_63_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_63_relu1"
  type: "ReLU"
  bottom: "map16_63_conv1"
  top: "map16_63_conv1"
}
layer {
  name: "map16_63_conv2"
  type: "Convolution"
  bottom: "map16_63_conv1"
  top: "map16_63_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_63_bn2"
  type: "BatchNorm"
  bottom: "map16_63_conv2"
  top: "map16_63_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_63_bn2"
  type: "BatchNorm"
  bottom: "map16_63_conv2"
  top: "map16_63_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_63_scale2"
  type: "Scale"
  bottom: "map16_63_conv2"
  top: "map16_63_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_63_relu2"
  type: "ReLU"
  bottom: "map16_63_conv2"
  top: "map16_63_conv2"
}
layer {
  name: "map16_63_conv_end"
  type: "Convolution"
  bottom: "map16_63_conv2"
  top: "map16_63_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_63_eltsum"
  type: "Eltwise"
  bottom: "map16_62_eltsum"
  bottom: "map16_63_conv_end"
  top: "map16_63_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_64_bn_pre"
  type: "BatchNorm"
  bottom: "map16_63_eltsum"
  top: "map16_64_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_64_bn_pre"
  type: "BatchNorm"
  bottom: "map16_63_eltsum"
  top: "map16_64_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_64_pre_scale"
  type: "Scale"
  bottom: "map16_64_bn_pre"
  top: "map16_64_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_64_pre_relu"
  type: "ReLU"
  bottom: "map16_64_bn_pre"
  top: "map16_64_bn_pre"
}
layer {
  name: "map16_64_conv1"
  type: "Convolution"
  bottom: "map16_64_bn_pre"
  top: "map16_64_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_64_bn1"
  type: "BatchNorm"
  bottom: "map16_64_conv1"
  top: "map16_64_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_64_bn1"
  type: "BatchNorm"
  bottom: "map16_64_conv1"
  top: "map16_64_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_64_scale1"
  type: "Scale"
  bottom: "map16_64_conv1"
  top: "map16_64_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_64_relu1"
  type: "ReLU"
  bottom: "map16_64_conv1"
  top: "map16_64_conv1"
}
layer {
  name: "map16_64_conv2"
  type: "Convolution"
  bottom: "map16_64_conv1"
  top: "map16_64_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_64_bn2"
  type: "BatchNorm"
  bottom: "map16_64_conv2"
  top: "map16_64_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_64_bn2"
  type: "BatchNorm"
  bottom: "map16_64_conv2"
  top: "map16_64_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_64_scale2"
  type: "Scale"
  bottom: "map16_64_conv2"
  top: "map16_64_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_64_relu2"
  type: "ReLU"
  bottom: "map16_64_conv2"
  top: "map16_64_conv2"
}
layer {
  name: "map16_64_conv_end"
  type: "Convolution"
  bottom: "map16_64_conv2"
  top: "map16_64_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_64_eltsum"
  type: "Eltwise"
  bottom: "map16_63_eltsum"
  bottom: "map16_64_conv_end"
  top: "map16_64_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_65_bn_pre"
  type: "BatchNorm"
  bottom: "map16_64_eltsum"
  top: "map16_65_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_65_bn_pre"
  type: "BatchNorm"
  bottom: "map16_64_eltsum"
  top: "map16_65_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_65_pre_scale"
  type: "Scale"
  bottom: "map16_65_bn_pre"
  top: "map16_65_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_65_pre_relu"
  type: "ReLU"
  bottom: "map16_65_bn_pre"
  top: "map16_65_bn_pre"
}
layer {
  name: "map16_65_conv1"
  type: "Convolution"
  bottom: "map16_65_bn_pre"
  top: "map16_65_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_65_bn1"
  type: "BatchNorm"
  bottom: "map16_65_conv1"
  top: "map16_65_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_65_bn1"
  type: "BatchNorm"
  bottom: "map16_65_conv1"
  top: "map16_65_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_65_scale1"
  type: "Scale"
  bottom: "map16_65_conv1"
  top: "map16_65_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_65_relu1"
  type: "ReLU"
  bottom: "map16_65_conv1"
  top: "map16_65_conv1"
}
layer {
  name: "map16_65_conv2"
  type: "Convolution"
  bottom: "map16_65_conv1"
  top: "map16_65_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_65_bn2"
  type: "BatchNorm"
  bottom: "map16_65_conv2"
  top: "map16_65_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_65_bn2"
  type: "BatchNorm"
  bottom: "map16_65_conv2"
  top: "map16_65_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_65_scale2"
  type: "Scale"
  bottom: "map16_65_conv2"
  top: "map16_65_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_65_relu2"
  type: "ReLU"
  bottom: "map16_65_conv2"
  top: "map16_65_conv2"
}
layer {
  name: "map16_65_conv_end"
  type: "Convolution"
  bottom: "map16_65_conv2"
  top: "map16_65_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_65_eltsum"
  type: "Eltwise"
  bottom: "map16_64_eltsum"
  bottom: "map16_65_conv_end"
  top: "map16_65_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_66_bn_pre"
  type: "BatchNorm"
  bottom: "map16_65_eltsum"
  top: "map16_66_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_66_bn_pre"
  type: "BatchNorm"
  bottom: "map16_65_eltsum"
  top: "map16_66_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_66_pre_scale"
  type: "Scale"
  bottom: "map16_66_bn_pre"
  top: "map16_66_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_66_pre_relu"
  type: "ReLU"
  bottom: "map16_66_bn_pre"
  top: "map16_66_bn_pre"
}
layer {
  name: "map16_66_conv1"
  type: "Convolution"
  bottom: "map16_66_bn_pre"
  top: "map16_66_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_66_bn1"
  type: "BatchNorm"
  bottom: "map16_66_conv1"
  top: "map16_66_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_66_bn1"
  type: "BatchNorm"
  bottom: "map16_66_conv1"
  top: "map16_66_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_66_scale1"
  type: "Scale"
  bottom: "map16_66_conv1"
  top: "map16_66_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_66_relu1"
  type: "ReLU"
  bottom: "map16_66_conv1"
  top: "map16_66_conv1"
}
layer {
  name: "map16_66_conv2"
  type: "Convolution"
  bottom: "map16_66_conv1"
  top: "map16_66_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_66_bn2"
  type: "BatchNorm"
  bottom: "map16_66_conv2"
  top: "map16_66_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_66_bn2"
  type: "BatchNorm"
  bottom: "map16_66_conv2"
  top: "map16_66_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_66_scale2"
  type: "Scale"
  bottom: "map16_66_conv2"
  top: "map16_66_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_66_relu2"
  type: "ReLU"
  bottom: "map16_66_conv2"
  top: "map16_66_conv2"
}
layer {
  name: "map16_66_conv_end"
  type: "Convolution"
  bottom: "map16_66_conv2"
  top: "map16_66_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_66_eltsum"
  type: "Eltwise"
  bottom: "map16_65_eltsum"
  bottom: "map16_66_conv_end"
  top: "map16_66_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_67_bn_pre"
  type: "BatchNorm"
  bottom: "map16_66_eltsum"
  top: "map16_67_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_67_bn_pre"
  type: "BatchNorm"
  bottom: "map16_66_eltsum"
  top: "map16_67_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_67_pre_scale"
  type: "Scale"
  bottom: "map16_67_bn_pre"
  top: "map16_67_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_67_pre_relu"
  type: "ReLU"
  bottom: "map16_67_bn_pre"
  top: "map16_67_bn_pre"
}
layer {
  name: "map16_67_conv1"
  type: "Convolution"
  bottom: "map16_67_bn_pre"
  top: "map16_67_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_67_bn1"
  type: "BatchNorm"
  bottom: "map16_67_conv1"
  top: "map16_67_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_67_bn1"
  type: "BatchNorm"
  bottom: "map16_67_conv1"
  top: "map16_67_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_67_scale1"
  type: "Scale"
  bottom: "map16_67_conv1"
  top: "map16_67_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_67_relu1"
  type: "ReLU"
  bottom: "map16_67_conv1"
  top: "map16_67_conv1"
}
layer {
  name: "map16_67_conv2"
  type: "Convolution"
  bottom: "map16_67_conv1"
  top: "map16_67_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_67_bn2"
  type: "BatchNorm"
  bottom: "map16_67_conv2"
  top: "map16_67_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_67_bn2"
  type: "BatchNorm"
  bottom: "map16_67_conv2"
  top: "map16_67_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_67_scale2"
  type: "Scale"
  bottom: "map16_67_conv2"
  top: "map16_67_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_67_relu2"
  type: "ReLU"
  bottom: "map16_67_conv2"
  top: "map16_67_conv2"
}
layer {
  name: "map16_67_conv_end"
  type: "Convolution"
  bottom: "map16_67_conv2"
  top: "map16_67_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_67_eltsum"
  type: "Eltwise"
  bottom: "map16_66_eltsum"
  bottom: "map16_67_conv_end"
  top: "map16_67_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_68_bn_pre"
  type: "BatchNorm"
  bottom: "map16_67_eltsum"
  top: "map16_68_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_68_bn_pre"
  type: "BatchNorm"
  bottom: "map16_67_eltsum"
  top: "map16_68_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_68_pre_scale"
  type: "Scale"
  bottom: "map16_68_bn_pre"
  top: "map16_68_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_68_pre_relu"
  type: "ReLU"
  bottom: "map16_68_bn_pre"
  top: "map16_68_bn_pre"
}
layer {
  name: "map16_68_conv1"
  type: "Convolution"
  bottom: "map16_68_bn_pre"
  top: "map16_68_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_68_bn1"
  type: "BatchNorm"
  bottom: "map16_68_conv1"
  top: "map16_68_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_68_bn1"
  type: "BatchNorm"
  bottom: "map16_68_conv1"
  top: "map16_68_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_68_scale1"
  type: "Scale"
  bottom: "map16_68_conv1"
  top: "map16_68_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_68_relu1"
  type: "ReLU"
  bottom: "map16_68_conv1"
  top: "map16_68_conv1"
}
layer {
  name: "map16_68_conv2"
  type: "Convolution"
  bottom: "map16_68_conv1"
  top: "map16_68_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_68_bn2"
  type: "BatchNorm"
  bottom: "map16_68_conv2"
  top: "map16_68_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_68_bn2"
  type: "BatchNorm"
  bottom: "map16_68_conv2"
  top: "map16_68_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_68_scale2"
  type: "Scale"
  bottom: "map16_68_conv2"
  top: "map16_68_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_68_relu2"
  type: "ReLU"
  bottom: "map16_68_conv2"
  top: "map16_68_conv2"
}
layer {
  name: "map16_68_conv_end"
  type: "Convolution"
  bottom: "map16_68_conv2"
  top: "map16_68_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_68_eltsum"
  type: "Eltwise"
  bottom: "map16_67_eltsum"
  bottom: "map16_68_conv_end"
  top: "map16_68_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_69_bn_pre"
  type: "BatchNorm"
  bottom: "map16_68_eltsum"
  top: "map16_69_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_69_bn_pre"
  type: "BatchNorm"
  bottom: "map16_68_eltsum"
  top: "map16_69_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_69_pre_scale"
  type: "Scale"
  bottom: "map16_69_bn_pre"
  top: "map16_69_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_69_pre_relu"
  type: "ReLU"
  bottom: "map16_69_bn_pre"
  top: "map16_69_bn_pre"
}
layer {
  name: "map16_69_conv1"
  type: "Convolution"
  bottom: "map16_69_bn_pre"
  top: "map16_69_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_69_bn1"
  type: "BatchNorm"
  bottom: "map16_69_conv1"
  top: "map16_69_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_69_bn1"
  type: "BatchNorm"
  bottom: "map16_69_conv1"
  top: "map16_69_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_69_scale1"
  type: "Scale"
  bottom: "map16_69_conv1"
  top: "map16_69_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_69_relu1"
  type: "ReLU"
  bottom: "map16_69_conv1"
  top: "map16_69_conv1"
}
layer {
  name: "map16_69_conv2"
  type: "Convolution"
  bottom: "map16_69_conv1"
  top: "map16_69_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_69_bn2"
  type: "BatchNorm"
  bottom: "map16_69_conv2"
  top: "map16_69_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_69_bn2"
  type: "BatchNorm"
  bottom: "map16_69_conv2"
  top: "map16_69_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_69_scale2"
  type: "Scale"
  bottom: "map16_69_conv2"
  top: "map16_69_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_69_relu2"
  type: "ReLU"
  bottom: "map16_69_conv2"
  top: "map16_69_conv2"
}
layer {
  name: "map16_69_conv_end"
  type: "Convolution"
  bottom: "map16_69_conv2"
  top: "map16_69_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_69_eltsum"
  type: "Eltwise"
  bottom: "map16_68_eltsum"
  bottom: "map16_69_conv_end"
  top: "map16_69_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_70_bn_pre"
  type: "BatchNorm"
  bottom: "map16_69_eltsum"
  top: "map16_70_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_70_bn_pre"
  type: "BatchNorm"
  bottom: "map16_69_eltsum"
  top: "map16_70_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_70_pre_scale"
  type: "Scale"
  bottom: "map16_70_bn_pre"
  top: "map16_70_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_70_pre_relu"
  type: "ReLU"
  bottom: "map16_70_bn_pre"
  top: "map16_70_bn_pre"
}
layer {
  name: "map16_70_conv1"
  type: "Convolution"
  bottom: "map16_70_bn_pre"
  top: "map16_70_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_70_bn1"
  type: "BatchNorm"
  bottom: "map16_70_conv1"
  top: "map16_70_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_70_bn1"
  type: "BatchNorm"
  bottom: "map16_70_conv1"
  top: "map16_70_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_70_scale1"
  type: "Scale"
  bottom: "map16_70_conv1"
  top: "map16_70_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_70_relu1"
  type: "ReLU"
  bottom: "map16_70_conv1"
  top: "map16_70_conv1"
}
layer {
  name: "map16_70_conv2"
  type: "Convolution"
  bottom: "map16_70_conv1"
  top: "map16_70_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_70_bn2"
  type: "BatchNorm"
  bottom: "map16_70_conv2"
  top: "map16_70_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_70_bn2"
  type: "BatchNorm"
  bottom: "map16_70_conv2"
  top: "map16_70_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_70_scale2"
  type: "Scale"
  bottom: "map16_70_conv2"
  top: "map16_70_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_70_relu2"
  type: "ReLU"
  bottom: "map16_70_conv2"
  top: "map16_70_conv2"
}
layer {
  name: "map16_70_conv_end"
  type: "Convolution"
  bottom: "map16_70_conv2"
  top: "map16_70_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_70_eltsum"
  type: "Eltwise"
  bottom: "map16_69_eltsum"
  bottom: "map16_70_conv_end"
  top: "map16_70_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_71_bn_pre"
  type: "BatchNorm"
  bottom: "map16_70_eltsum"
  top: "map16_71_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_71_bn_pre"
  type: "BatchNorm"
  bottom: "map16_70_eltsum"
  top: "map16_71_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_71_pre_scale"
  type: "Scale"
  bottom: "map16_71_bn_pre"
  top: "map16_71_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_71_pre_relu"
  type: "ReLU"
  bottom: "map16_71_bn_pre"
  top: "map16_71_bn_pre"
}
layer {
  name: "map16_71_conv1"
  type: "Convolution"
  bottom: "map16_71_bn_pre"
  top: "map16_71_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_71_bn1"
  type: "BatchNorm"
  bottom: "map16_71_conv1"
  top: "map16_71_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_71_bn1"
  type: "BatchNorm"
  bottom: "map16_71_conv1"
  top: "map16_71_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_71_scale1"
  type: "Scale"
  bottom: "map16_71_conv1"
  top: "map16_71_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_71_relu1"
  type: "ReLU"
  bottom: "map16_71_conv1"
  top: "map16_71_conv1"
}
layer {
  name: "map16_71_conv2"
  type: "Convolution"
  bottom: "map16_71_conv1"
  top: "map16_71_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_71_bn2"
  type: "BatchNorm"
  bottom: "map16_71_conv2"
  top: "map16_71_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_71_bn2"
  type: "BatchNorm"
  bottom: "map16_71_conv2"
  top: "map16_71_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_71_scale2"
  type: "Scale"
  bottom: "map16_71_conv2"
  top: "map16_71_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_71_relu2"
  type: "ReLU"
  bottom: "map16_71_conv2"
  top: "map16_71_conv2"
}
layer {
  name: "map16_71_conv_end"
  type: "Convolution"
  bottom: "map16_71_conv2"
  top: "map16_71_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_71_eltsum"
  type: "Eltwise"
  bottom: "map16_70_eltsum"
  bottom: "map16_71_conv_end"
  top: "map16_71_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_72_bn_pre"
  type: "BatchNorm"
  bottom: "map16_71_eltsum"
  top: "map16_72_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_72_bn_pre"
  type: "BatchNorm"
  bottom: "map16_71_eltsum"
  top: "map16_72_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_72_pre_scale"
  type: "Scale"
  bottom: "map16_72_bn_pre"
  top: "map16_72_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_72_pre_relu"
  type: "ReLU"
  bottom: "map16_72_bn_pre"
  top: "map16_72_bn_pre"
}
layer {
  name: "map16_72_conv1"
  type: "Convolution"
  bottom: "map16_72_bn_pre"
  top: "map16_72_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_72_bn1"
  type: "BatchNorm"
  bottom: "map16_72_conv1"
  top: "map16_72_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_72_bn1"
  type: "BatchNorm"
  bottom: "map16_72_conv1"
  top: "map16_72_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_72_scale1"
  type: "Scale"
  bottom: "map16_72_conv1"
  top: "map16_72_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_72_relu1"
  type: "ReLU"
  bottom: "map16_72_conv1"
  top: "map16_72_conv1"
}
layer {
  name: "map16_72_conv2"
  type: "Convolution"
  bottom: "map16_72_conv1"
  top: "map16_72_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_72_bn2"
  type: "BatchNorm"
  bottom: "map16_72_conv2"
  top: "map16_72_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_72_bn2"
  type: "BatchNorm"
  bottom: "map16_72_conv2"
  top: "map16_72_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_72_scale2"
  type: "Scale"
  bottom: "map16_72_conv2"
  top: "map16_72_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_72_relu2"
  type: "ReLU"
  bottom: "map16_72_conv2"
  top: "map16_72_conv2"
}
layer {
  name: "map16_72_conv_end"
  type: "Convolution"
  bottom: "map16_72_conv2"
  top: "map16_72_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_72_eltsum"
  type: "Eltwise"
  bottom: "map16_71_eltsum"
  bottom: "map16_72_conv_end"
  top: "map16_72_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_73_bn_pre"
  type: "BatchNorm"
  bottom: "map16_72_eltsum"
  top: "map16_73_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_73_bn_pre"
  type: "BatchNorm"
  bottom: "map16_72_eltsum"
  top: "map16_73_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_73_pre_scale"
  type: "Scale"
  bottom: "map16_73_bn_pre"
  top: "map16_73_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_73_pre_relu"
  type: "ReLU"
  bottom: "map16_73_bn_pre"
  top: "map16_73_bn_pre"
}
layer {
  name: "map16_73_conv1"
  type: "Convolution"
  bottom: "map16_73_bn_pre"
  top: "map16_73_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_73_bn1"
  type: "BatchNorm"
  bottom: "map16_73_conv1"
  top: "map16_73_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_73_bn1"
  type: "BatchNorm"
  bottom: "map16_73_conv1"
  top: "map16_73_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_73_scale1"
  type: "Scale"
  bottom: "map16_73_conv1"
  top: "map16_73_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_73_relu1"
  type: "ReLU"
  bottom: "map16_73_conv1"
  top: "map16_73_conv1"
}
layer {
  name: "map16_73_conv2"
  type: "Convolution"
  bottom: "map16_73_conv1"
  top: "map16_73_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_73_bn2"
  type: "BatchNorm"
  bottom: "map16_73_conv2"
  top: "map16_73_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_73_bn2"
  type: "BatchNorm"
  bottom: "map16_73_conv2"
  top: "map16_73_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_73_scale2"
  type: "Scale"
  bottom: "map16_73_conv2"
  top: "map16_73_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_73_relu2"
  type: "ReLU"
  bottom: "map16_73_conv2"
  top: "map16_73_conv2"
}
layer {
  name: "map16_73_conv_end"
  type: "Convolution"
  bottom: "map16_73_conv2"
  top: "map16_73_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_73_eltsum"
  type: "Eltwise"
  bottom: "map16_72_eltsum"
  bottom: "map16_73_conv_end"
  top: "map16_73_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_74_bn_pre"
  type: "BatchNorm"
  bottom: "map16_73_eltsum"
  top: "map16_74_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_74_bn_pre"
  type: "BatchNorm"
  bottom: "map16_73_eltsum"
  top: "map16_74_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_74_pre_scale"
  type: "Scale"
  bottom: "map16_74_bn_pre"
  top: "map16_74_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_74_pre_relu"
  type: "ReLU"
  bottom: "map16_74_bn_pre"
  top: "map16_74_bn_pre"
}
layer {
  name: "map16_74_conv1"
  type: "Convolution"
  bottom: "map16_74_bn_pre"
  top: "map16_74_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_74_bn1"
  type: "BatchNorm"
  bottom: "map16_74_conv1"
  top: "map16_74_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_74_bn1"
  type: "BatchNorm"
  bottom: "map16_74_conv1"
  top: "map16_74_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_74_scale1"
  type: "Scale"
  bottom: "map16_74_conv1"
  top: "map16_74_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_74_relu1"
  type: "ReLU"
  bottom: "map16_74_conv1"
  top: "map16_74_conv1"
}
layer {
  name: "map16_74_conv2"
  type: "Convolution"
  bottom: "map16_74_conv1"
  top: "map16_74_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_74_bn2"
  type: "BatchNorm"
  bottom: "map16_74_conv2"
  top: "map16_74_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_74_bn2"
  type: "BatchNorm"
  bottom: "map16_74_conv2"
  top: "map16_74_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_74_scale2"
  type: "Scale"
  bottom: "map16_74_conv2"
  top: "map16_74_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_74_relu2"
  type: "ReLU"
  bottom: "map16_74_conv2"
  top: "map16_74_conv2"
}
layer {
  name: "map16_74_conv_end"
  type: "Convolution"
  bottom: "map16_74_conv2"
  top: "map16_74_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_74_eltsum"
  type: "Eltwise"
  bottom: "map16_73_eltsum"
  bottom: "map16_74_conv_end"
  top: "map16_74_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_75_bn_pre"
  type: "BatchNorm"
  bottom: "map16_74_eltsum"
  top: "map16_75_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_75_bn_pre"
  type: "BatchNorm"
  bottom: "map16_74_eltsum"
  top: "map16_75_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_75_pre_scale"
  type: "Scale"
  bottom: "map16_75_bn_pre"
  top: "map16_75_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_75_pre_relu"
  type: "ReLU"
  bottom: "map16_75_bn_pre"
  top: "map16_75_bn_pre"
}
layer {
  name: "map16_75_conv1"
  type: "Convolution"
  bottom: "map16_75_bn_pre"
  top: "map16_75_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_75_bn1"
  type: "BatchNorm"
  bottom: "map16_75_conv1"
  top: "map16_75_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_75_bn1"
  type: "BatchNorm"
  bottom: "map16_75_conv1"
  top: "map16_75_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_75_scale1"
  type: "Scale"
  bottom: "map16_75_conv1"
  top: "map16_75_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_75_relu1"
  type: "ReLU"
  bottom: "map16_75_conv1"
  top: "map16_75_conv1"
}
layer {
  name: "map16_75_conv2"
  type: "Convolution"
  bottom: "map16_75_conv1"
  top: "map16_75_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_75_bn2"
  type: "BatchNorm"
  bottom: "map16_75_conv2"
  top: "map16_75_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_75_bn2"
  type: "BatchNorm"
  bottom: "map16_75_conv2"
  top: "map16_75_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_75_scale2"
  type: "Scale"
  bottom: "map16_75_conv2"
  top: "map16_75_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_75_relu2"
  type: "ReLU"
  bottom: "map16_75_conv2"
  top: "map16_75_conv2"
}
layer {
  name: "map16_75_conv_end"
  type: "Convolution"
  bottom: "map16_75_conv2"
  top: "map16_75_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_75_eltsum"
  type: "Eltwise"
  bottom: "map16_74_eltsum"
  bottom: "map16_75_conv_end"
  top: "map16_75_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_76_bn_pre"
  type: "BatchNorm"
  bottom: "map16_75_eltsum"
  top: "map16_76_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_76_bn_pre"
  type: "BatchNorm"
  bottom: "map16_75_eltsum"
  top: "map16_76_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_76_pre_scale"
  type: "Scale"
  bottom: "map16_76_bn_pre"
  top: "map16_76_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_76_pre_relu"
  type: "ReLU"
  bottom: "map16_76_bn_pre"
  top: "map16_76_bn_pre"
}
layer {
  name: "map16_76_conv1"
  type: "Convolution"
  bottom: "map16_76_bn_pre"
  top: "map16_76_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_76_bn1"
  type: "BatchNorm"
  bottom: "map16_76_conv1"
  top: "map16_76_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_76_bn1"
  type: "BatchNorm"
  bottom: "map16_76_conv1"
  top: "map16_76_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_76_scale1"
  type: "Scale"
  bottom: "map16_76_conv1"
  top: "map16_76_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_76_relu1"
  type: "ReLU"
  bottom: "map16_76_conv1"
  top: "map16_76_conv1"
}
layer {
  name: "map16_76_conv2"
  type: "Convolution"
  bottom: "map16_76_conv1"
  top: "map16_76_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_76_bn2"
  type: "BatchNorm"
  bottom: "map16_76_conv2"
  top: "map16_76_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_76_bn2"
  type: "BatchNorm"
  bottom: "map16_76_conv2"
  top: "map16_76_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_76_scale2"
  type: "Scale"
  bottom: "map16_76_conv2"
  top: "map16_76_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_76_relu2"
  type: "ReLU"
  bottom: "map16_76_conv2"
  top: "map16_76_conv2"
}
layer {
  name: "map16_76_conv_end"
  type: "Convolution"
  bottom: "map16_76_conv2"
  top: "map16_76_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_76_eltsum"
  type: "Eltwise"
  bottom: "map16_75_eltsum"
  bottom: "map16_76_conv_end"
  top: "map16_76_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_77_bn_pre"
  type: "BatchNorm"
  bottom: "map16_76_eltsum"
  top: "map16_77_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_77_bn_pre"
  type: "BatchNorm"
  bottom: "map16_76_eltsum"
  top: "map16_77_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_77_pre_scale"
  type: "Scale"
  bottom: "map16_77_bn_pre"
  top: "map16_77_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_77_pre_relu"
  type: "ReLU"
  bottom: "map16_77_bn_pre"
  top: "map16_77_bn_pre"
}
layer {
  name: "map16_77_conv1"
  type: "Convolution"
  bottom: "map16_77_bn_pre"
  top: "map16_77_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_77_bn1"
  type: "BatchNorm"
  bottom: "map16_77_conv1"
  top: "map16_77_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_77_bn1"
  type: "BatchNorm"
  bottom: "map16_77_conv1"
  top: "map16_77_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_77_scale1"
  type: "Scale"
  bottom: "map16_77_conv1"
  top: "map16_77_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_77_relu1"
  type: "ReLU"
  bottom: "map16_77_conv1"
  top: "map16_77_conv1"
}
layer {
  name: "map16_77_conv2"
  type: "Convolution"
  bottom: "map16_77_conv1"
  top: "map16_77_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_77_bn2"
  type: "BatchNorm"
  bottom: "map16_77_conv2"
  top: "map16_77_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_77_bn2"
  type: "BatchNorm"
  bottom: "map16_77_conv2"
  top: "map16_77_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_77_scale2"
  type: "Scale"
  bottom: "map16_77_conv2"
  top: "map16_77_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_77_relu2"
  type: "ReLU"
  bottom: "map16_77_conv2"
  top: "map16_77_conv2"
}
layer {
  name: "map16_77_conv_end"
  type: "Convolution"
  bottom: "map16_77_conv2"
  top: "map16_77_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_77_eltsum"
  type: "Eltwise"
  bottom: "map16_76_eltsum"
  bottom: "map16_77_conv_end"
  top: "map16_77_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_78_bn_pre"
  type: "BatchNorm"
  bottom: "map16_77_eltsum"
  top: "map16_78_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_78_bn_pre"
  type: "BatchNorm"
  bottom: "map16_77_eltsum"
  top: "map16_78_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_78_pre_scale"
  type: "Scale"
  bottom: "map16_78_bn_pre"
  top: "map16_78_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_78_pre_relu"
  type: "ReLU"
  bottom: "map16_78_bn_pre"
  top: "map16_78_bn_pre"
}
layer {
  name: "map16_78_conv1"
  type: "Convolution"
  bottom: "map16_78_bn_pre"
  top: "map16_78_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_78_bn1"
  type: "BatchNorm"
  bottom: "map16_78_conv1"
  top: "map16_78_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_78_bn1"
  type: "BatchNorm"
  bottom: "map16_78_conv1"
  top: "map16_78_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_78_scale1"
  type: "Scale"
  bottom: "map16_78_conv1"
  top: "map16_78_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_78_relu1"
  type: "ReLU"
  bottom: "map16_78_conv1"
  top: "map16_78_conv1"
}
layer {
  name: "map16_78_conv2"
  type: "Convolution"
  bottom: "map16_78_conv1"
  top: "map16_78_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_78_bn2"
  type: "BatchNorm"
  bottom: "map16_78_conv2"
  top: "map16_78_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_78_bn2"
  type: "BatchNorm"
  bottom: "map16_78_conv2"
  top: "map16_78_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_78_scale2"
  type: "Scale"
  bottom: "map16_78_conv2"
  top: "map16_78_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_78_relu2"
  type: "ReLU"
  bottom: "map16_78_conv2"
  top: "map16_78_conv2"
}
layer {
  name: "map16_78_conv_end"
  type: "Convolution"
  bottom: "map16_78_conv2"
  top: "map16_78_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_78_eltsum"
  type: "Eltwise"
  bottom: "map16_77_eltsum"
  bottom: "map16_78_conv_end"
  top: "map16_78_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_79_bn_pre"
  type: "BatchNorm"
  bottom: "map16_78_eltsum"
  top: "map16_79_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_79_bn_pre"
  type: "BatchNorm"
  bottom: "map16_78_eltsum"
  top: "map16_79_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_79_pre_scale"
  type: "Scale"
  bottom: "map16_79_bn_pre"
  top: "map16_79_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_79_pre_relu"
  type: "ReLU"
  bottom: "map16_79_bn_pre"
  top: "map16_79_bn_pre"
}
layer {
  name: "map16_79_conv1"
  type: "Convolution"
  bottom: "map16_79_bn_pre"
  top: "map16_79_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_79_bn1"
  type: "BatchNorm"
  bottom: "map16_79_conv1"
  top: "map16_79_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_79_bn1"
  type: "BatchNorm"
  bottom: "map16_79_conv1"
  top: "map16_79_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_79_scale1"
  type: "Scale"
  bottom: "map16_79_conv1"
  top: "map16_79_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_79_relu1"
  type: "ReLU"
  bottom: "map16_79_conv1"
  top: "map16_79_conv1"
}
layer {
  name: "map16_79_conv2"
  type: "Convolution"
  bottom: "map16_79_conv1"
  top: "map16_79_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_79_bn2"
  type: "BatchNorm"
  bottom: "map16_79_conv2"
  top: "map16_79_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_79_bn2"
  type: "BatchNorm"
  bottom: "map16_79_conv2"
  top: "map16_79_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_79_scale2"
  type: "Scale"
  bottom: "map16_79_conv2"
  top: "map16_79_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_79_relu2"
  type: "ReLU"
  bottom: "map16_79_conv2"
  top: "map16_79_conv2"
}
layer {
  name: "map16_79_conv_end"
  type: "Convolution"
  bottom: "map16_79_conv2"
  top: "map16_79_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_79_eltsum"
  type: "Eltwise"
  bottom: "map16_78_eltsum"
  bottom: "map16_79_conv_end"
  top: "map16_79_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_80_bn_pre"
  type: "BatchNorm"
  bottom: "map16_79_eltsum"
  top: "map16_80_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_80_bn_pre"
  type: "BatchNorm"
  bottom: "map16_79_eltsum"
  top: "map16_80_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_80_pre_scale"
  type: "Scale"
  bottom: "map16_80_bn_pre"
  top: "map16_80_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_80_pre_relu"
  type: "ReLU"
  bottom: "map16_80_bn_pre"
  top: "map16_80_bn_pre"
}
layer {
  name: "map16_80_conv1"
  type: "Convolution"
  bottom: "map16_80_bn_pre"
  top: "map16_80_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_80_bn1"
  type: "BatchNorm"
  bottom: "map16_80_conv1"
  top: "map16_80_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_80_bn1"
  type: "BatchNorm"
  bottom: "map16_80_conv1"
  top: "map16_80_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_80_scale1"
  type: "Scale"
  bottom: "map16_80_conv1"
  top: "map16_80_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_80_relu1"
  type: "ReLU"
  bottom: "map16_80_conv1"
  top: "map16_80_conv1"
}
layer {
  name: "map16_80_conv2"
  type: "Convolution"
  bottom: "map16_80_conv1"
  top: "map16_80_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_80_bn2"
  type: "BatchNorm"
  bottom: "map16_80_conv2"
  top: "map16_80_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_80_bn2"
  type: "BatchNorm"
  bottom: "map16_80_conv2"
  top: "map16_80_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_80_scale2"
  type: "Scale"
  bottom: "map16_80_conv2"
  top: "map16_80_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_80_relu2"
  type: "ReLU"
  bottom: "map16_80_conv2"
  top: "map16_80_conv2"
}
layer {
  name: "map16_80_conv_end"
  type: "Convolution"
  bottom: "map16_80_conv2"
  top: "map16_80_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_80_eltsum"
  type: "Eltwise"
  bottom: "map16_79_eltsum"
  bottom: "map16_80_conv_end"
  top: "map16_80_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_81_bn_pre"
  type: "BatchNorm"
  bottom: "map16_80_eltsum"
  top: "map16_81_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_81_bn_pre"
  type: "BatchNorm"
  bottom: "map16_80_eltsum"
  top: "map16_81_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_81_pre_scale"
  type: "Scale"
  bottom: "map16_81_bn_pre"
  top: "map16_81_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_81_pre_relu"
  type: "ReLU"
  bottom: "map16_81_bn_pre"
  top: "map16_81_bn_pre"
}
layer {
  name: "map16_81_conv1"
  type: "Convolution"
  bottom: "map16_81_bn_pre"
  top: "map16_81_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_81_bn1"
  type: "BatchNorm"
  bottom: "map16_81_conv1"
  top: "map16_81_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_81_bn1"
  type: "BatchNorm"
  bottom: "map16_81_conv1"
  top: "map16_81_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_81_scale1"
  type: "Scale"
  bottom: "map16_81_conv1"
  top: "map16_81_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_81_relu1"
  type: "ReLU"
  bottom: "map16_81_conv1"
  top: "map16_81_conv1"
}
layer {
  name: "map16_81_conv2"
  type: "Convolution"
  bottom: "map16_81_conv1"
  top: "map16_81_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_81_bn2"
  type: "BatchNorm"
  bottom: "map16_81_conv2"
  top: "map16_81_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_81_bn2"
  type: "BatchNorm"
  bottom: "map16_81_conv2"
  top: "map16_81_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_81_scale2"
  type: "Scale"
  bottom: "map16_81_conv2"
  top: "map16_81_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_81_relu2"
  type: "ReLU"
  bottom: "map16_81_conv2"
  top: "map16_81_conv2"
}
layer {
  name: "map16_81_conv_end"
  type: "Convolution"
  bottom: "map16_81_conv2"
  top: "map16_81_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_81_eltsum"
  type: "Eltwise"
  bottom: "map16_80_eltsum"
  bottom: "map16_81_conv_end"
  top: "map16_81_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_82_bn_pre"
  type: "BatchNorm"
  bottom: "map16_81_eltsum"
  top: "map16_82_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_82_bn_pre"
  type: "BatchNorm"
  bottom: "map16_81_eltsum"
  top: "map16_82_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_82_pre_scale"
  type: "Scale"
  bottom: "map16_82_bn_pre"
  top: "map16_82_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_82_pre_relu"
  type: "ReLU"
  bottom: "map16_82_bn_pre"
  top: "map16_82_bn_pre"
}
layer {
  name: "map16_82_conv1"
  type: "Convolution"
  bottom: "map16_82_bn_pre"
  top: "map16_82_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_82_bn1"
  type: "BatchNorm"
  bottom: "map16_82_conv1"
  top: "map16_82_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_82_bn1"
  type: "BatchNorm"
  bottom: "map16_82_conv1"
  top: "map16_82_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_82_scale1"
  type: "Scale"
  bottom: "map16_82_conv1"
  top: "map16_82_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_82_relu1"
  type: "ReLU"
  bottom: "map16_82_conv1"
  top: "map16_82_conv1"
}
layer {
  name: "map16_82_conv2"
  type: "Convolution"
  bottom: "map16_82_conv1"
  top: "map16_82_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_82_bn2"
  type: "BatchNorm"
  bottom: "map16_82_conv2"
  top: "map16_82_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_82_bn2"
  type: "BatchNorm"
  bottom: "map16_82_conv2"
  top: "map16_82_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_82_scale2"
  type: "Scale"
  bottom: "map16_82_conv2"
  top: "map16_82_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_82_relu2"
  type: "ReLU"
  bottom: "map16_82_conv2"
  top: "map16_82_conv2"
}
layer {
  name: "map16_82_conv_end"
  type: "Convolution"
  bottom: "map16_82_conv2"
  top: "map16_82_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_82_eltsum"
  type: "Eltwise"
  bottom: "map16_81_eltsum"
  bottom: "map16_82_conv_end"
  top: "map16_82_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_83_bn_pre"
  type: "BatchNorm"
  bottom: "map16_82_eltsum"
  top: "map16_83_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_83_bn_pre"
  type: "BatchNorm"
  bottom: "map16_82_eltsum"
  top: "map16_83_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_83_pre_scale"
  type: "Scale"
  bottom: "map16_83_bn_pre"
  top: "map16_83_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_83_pre_relu"
  type: "ReLU"
  bottom: "map16_83_bn_pre"
  top: "map16_83_bn_pre"
}
layer {
  name: "map16_83_conv1"
  type: "Convolution"
  bottom: "map16_83_bn_pre"
  top: "map16_83_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_83_bn1"
  type: "BatchNorm"
  bottom: "map16_83_conv1"
  top: "map16_83_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_83_bn1"
  type: "BatchNorm"
  bottom: "map16_83_conv1"
  top: "map16_83_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_83_scale1"
  type: "Scale"
  bottom: "map16_83_conv1"
  top: "map16_83_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_83_relu1"
  type: "ReLU"
  bottom: "map16_83_conv1"
  top: "map16_83_conv1"
}
layer {
  name: "map16_83_conv2"
  type: "Convolution"
  bottom: "map16_83_conv1"
  top: "map16_83_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_83_bn2"
  type: "BatchNorm"
  bottom: "map16_83_conv2"
  top: "map16_83_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_83_bn2"
  type: "BatchNorm"
  bottom: "map16_83_conv2"
  top: "map16_83_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_83_scale2"
  type: "Scale"
  bottom: "map16_83_conv2"
  top: "map16_83_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_83_relu2"
  type: "ReLU"
  bottom: "map16_83_conv2"
  top: "map16_83_conv2"
}
layer {
  name: "map16_83_conv_end"
  type: "Convolution"
  bottom: "map16_83_conv2"
  top: "map16_83_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_83_eltsum"
  type: "Eltwise"
  bottom: "map16_82_eltsum"
  bottom: "map16_83_conv_end"
  top: "map16_83_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_84_bn_pre"
  type: "BatchNorm"
  bottom: "map16_83_eltsum"
  top: "map16_84_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_84_bn_pre"
  type: "BatchNorm"
  bottom: "map16_83_eltsum"
  top: "map16_84_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_84_pre_scale"
  type: "Scale"
  bottom: "map16_84_bn_pre"
  top: "map16_84_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_84_pre_relu"
  type: "ReLU"
  bottom: "map16_84_bn_pre"
  top: "map16_84_bn_pre"
}
layer {
  name: "map16_84_conv1"
  type: "Convolution"
  bottom: "map16_84_bn_pre"
  top: "map16_84_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_84_bn1"
  type: "BatchNorm"
  bottom: "map16_84_conv1"
  top: "map16_84_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_84_bn1"
  type: "BatchNorm"
  bottom: "map16_84_conv1"
  top: "map16_84_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_84_scale1"
  type: "Scale"
  bottom: "map16_84_conv1"
  top: "map16_84_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_84_relu1"
  type: "ReLU"
  bottom: "map16_84_conv1"
  top: "map16_84_conv1"
}
layer {
  name: "map16_84_conv2"
  type: "Convolution"
  bottom: "map16_84_conv1"
  top: "map16_84_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_84_bn2"
  type: "BatchNorm"
  bottom: "map16_84_conv2"
  top: "map16_84_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_84_bn2"
  type: "BatchNorm"
  bottom: "map16_84_conv2"
  top: "map16_84_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_84_scale2"
  type: "Scale"
  bottom: "map16_84_conv2"
  top: "map16_84_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_84_relu2"
  type: "ReLU"
  bottom: "map16_84_conv2"
  top: "map16_84_conv2"
}
layer {
  name: "map16_84_conv_end"
  type: "Convolution"
  bottom: "map16_84_conv2"
  top: "map16_84_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_84_eltsum"
  type: "Eltwise"
  bottom: "map16_83_eltsum"
  bottom: "map16_84_conv_end"
  top: "map16_84_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_85_bn_pre"
  type: "BatchNorm"
  bottom: "map16_84_eltsum"
  top: "map16_85_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_85_bn_pre"
  type: "BatchNorm"
  bottom: "map16_84_eltsum"
  top: "map16_85_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_85_pre_scale"
  type: "Scale"
  bottom: "map16_85_bn_pre"
  top: "map16_85_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_85_pre_relu"
  type: "ReLU"
  bottom: "map16_85_bn_pre"
  top: "map16_85_bn_pre"
}
layer {
  name: "map16_85_conv1"
  type: "Convolution"
  bottom: "map16_85_bn_pre"
  top: "map16_85_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_85_bn1"
  type: "BatchNorm"
  bottom: "map16_85_conv1"
  top: "map16_85_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_85_bn1"
  type: "BatchNorm"
  bottom: "map16_85_conv1"
  top: "map16_85_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_85_scale1"
  type: "Scale"
  bottom: "map16_85_conv1"
  top: "map16_85_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_85_relu1"
  type: "ReLU"
  bottom: "map16_85_conv1"
  top: "map16_85_conv1"
}
layer {
  name: "map16_85_conv2"
  type: "Convolution"
  bottom: "map16_85_conv1"
  top: "map16_85_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_85_bn2"
  type: "BatchNorm"
  bottom: "map16_85_conv2"
  top: "map16_85_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_85_bn2"
  type: "BatchNorm"
  bottom: "map16_85_conv2"
  top: "map16_85_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_85_scale2"
  type: "Scale"
  bottom: "map16_85_conv2"
  top: "map16_85_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_85_relu2"
  type: "ReLU"
  bottom: "map16_85_conv2"
  top: "map16_85_conv2"
}
layer {
  name: "map16_85_conv_end"
  type: "Convolution"
  bottom: "map16_85_conv2"
  top: "map16_85_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_85_eltsum"
  type: "Eltwise"
  bottom: "map16_84_eltsum"
  bottom: "map16_85_conv_end"
  top: "map16_85_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_86_bn_pre"
  type: "BatchNorm"
  bottom: "map16_85_eltsum"
  top: "map16_86_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_86_bn_pre"
  type: "BatchNorm"
  bottom: "map16_85_eltsum"
  top: "map16_86_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_86_pre_scale"
  type: "Scale"
  bottom: "map16_86_bn_pre"
  top: "map16_86_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_86_pre_relu"
  type: "ReLU"
  bottom: "map16_86_bn_pre"
  top: "map16_86_bn_pre"
}
layer {
  name: "map16_86_conv1"
  type: "Convolution"
  bottom: "map16_86_bn_pre"
  top: "map16_86_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_86_bn1"
  type: "BatchNorm"
  bottom: "map16_86_conv1"
  top: "map16_86_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_86_bn1"
  type: "BatchNorm"
  bottom: "map16_86_conv1"
  top: "map16_86_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_86_scale1"
  type: "Scale"
  bottom: "map16_86_conv1"
  top: "map16_86_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_86_relu1"
  type: "ReLU"
  bottom: "map16_86_conv1"
  top: "map16_86_conv1"
}
layer {
  name: "map16_86_conv2"
  type: "Convolution"
  bottom: "map16_86_conv1"
  top: "map16_86_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_86_bn2"
  type: "BatchNorm"
  bottom: "map16_86_conv2"
  top: "map16_86_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_86_bn2"
  type: "BatchNorm"
  bottom: "map16_86_conv2"
  top: "map16_86_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_86_scale2"
  type: "Scale"
  bottom: "map16_86_conv2"
  top: "map16_86_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_86_relu2"
  type: "ReLU"
  bottom: "map16_86_conv2"
  top: "map16_86_conv2"
}
layer {
  name: "map16_86_conv_end"
  type: "Convolution"
  bottom: "map16_86_conv2"
  top: "map16_86_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_86_eltsum"
  type: "Eltwise"
  bottom: "map16_85_eltsum"
  bottom: "map16_86_conv_end"
  top: "map16_86_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_87_bn_pre"
  type: "BatchNorm"
  bottom: "map16_86_eltsum"
  top: "map16_87_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_87_bn_pre"
  type: "BatchNorm"
  bottom: "map16_86_eltsum"
  top: "map16_87_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_87_pre_scale"
  type: "Scale"
  bottom: "map16_87_bn_pre"
  top: "map16_87_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_87_pre_relu"
  type: "ReLU"
  bottom: "map16_87_bn_pre"
  top: "map16_87_bn_pre"
}
layer {
  name: "map16_87_conv1"
  type: "Convolution"
  bottom: "map16_87_bn_pre"
  top: "map16_87_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_87_bn1"
  type: "BatchNorm"
  bottom: "map16_87_conv1"
  top: "map16_87_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_87_bn1"
  type: "BatchNorm"
  bottom: "map16_87_conv1"
  top: "map16_87_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_87_scale1"
  type: "Scale"
  bottom: "map16_87_conv1"
  top: "map16_87_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_87_relu1"
  type: "ReLU"
  bottom: "map16_87_conv1"
  top: "map16_87_conv1"
}
layer {
  name: "map16_87_conv2"
  type: "Convolution"
  bottom: "map16_87_conv1"
  top: "map16_87_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_87_bn2"
  type: "BatchNorm"
  bottom: "map16_87_conv2"
  top: "map16_87_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_87_bn2"
  type: "BatchNorm"
  bottom: "map16_87_conv2"
  top: "map16_87_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_87_scale2"
  type: "Scale"
  bottom: "map16_87_conv2"
  top: "map16_87_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_87_relu2"
  type: "ReLU"
  bottom: "map16_87_conv2"
  top: "map16_87_conv2"
}
layer {
  name: "map16_87_conv_end"
  type: "Convolution"
  bottom: "map16_87_conv2"
  top: "map16_87_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_87_eltsum"
  type: "Eltwise"
  bottom: "map16_86_eltsum"
  bottom: "map16_87_conv_end"
  top: "map16_87_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_88_bn_pre"
  type: "BatchNorm"
  bottom: "map16_87_eltsum"
  top: "map16_88_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_88_bn_pre"
  type: "BatchNorm"
  bottom: "map16_87_eltsum"
  top: "map16_88_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_88_pre_scale"
  type: "Scale"
  bottom: "map16_88_bn_pre"
  top: "map16_88_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_88_pre_relu"
  type: "ReLU"
  bottom: "map16_88_bn_pre"
  top: "map16_88_bn_pre"
}
layer {
  name: "map16_88_conv1"
  type: "Convolution"
  bottom: "map16_88_bn_pre"
  top: "map16_88_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_88_bn1"
  type: "BatchNorm"
  bottom: "map16_88_conv1"
  top: "map16_88_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_88_bn1"
  type: "BatchNorm"
  bottom: "map16_88_conv1"
  top: "map16_88_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_88_scale1"
  type: "Scale"
  bottom: "map16_88_conv1"
  top: "map16_88_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_88_relu1"
  type: "ReLU"
  bottom: "map16_88_conv1"
  top: "map16_88_conv1"
}
layer {
  name: "map16_88_conv2"
  type: "Convolution"
  bottom: "map16_88_conv1"
  top: "map16_88_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_88_bn2"
  type: "BatchNorm"
  bottom: "map16_88_conv2"
  top: "map16_88_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_88_bn2"
  type: "BatchNorm"
  bottom: "map16_88_conv2"
  top: "map16_88_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_88_scale2"
  type: "Scale"
  bottom: "map16_88_conv2"
  top: "map16_88_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_88_relu2"
  type: "ReLU"
  bottom: "map16_88_conv2"
  top: "map16_88_conv2"
}
layer {
  name: "map16_88_conv_end"
  type: "Convolution"
  bottom: "map16_88_conv2"
  top: "map16_88_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_88_eltsum"
  type: "Eltwise"
  bottom: "map16_87_eltsum"
  bottom: "map16_88_conv_end"
  top: "map16_88_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_89_bn_pre"
  type: "BatchNorm"
  bottom: "map16_88_eltsum"
  top: "map16_89_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_89_bn_pre"
  type: "BatchNorm"
  bottom: "map16_88_eltsum"
  top: "map16_89_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_89_pre_scale"
  type: "Scale"
  bottom: "map16_89_bn_pre"
  top: "map16_89_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_89_pre_relu"
  type: "ReLU"
  bottom: "map16_89_bn_pre"
  top: "map16_89_bn_pre"
}
layer {
  name: "map16_89_conv1"
  type: "Convolution"
  bottom: "map16_89_bn_pre"
  top: "map16_89_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_89_bn1"
  type: "BatchNorm"
  bottom: "map16_89_conv1"
  top: "map16_89_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_89_bn1"
  type: "BatchNorm"
  bottom: "map16_89_conv1"
  top: "map16_89_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_89_scale1"
  type: "Scale"
  bottom: "map16_89_conv1"
  top: "map16_89_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_89_relu1"
  type: "ReLU"
  bottom: "map16_89_conv1"
  top: "map16_89_conv1"
}
layer {
  name: "map16_89_conv2"
  type: "Convolution"
  bottom: "map16_89_conv1"
  top: "map16_89_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_89_bn2"
  type: "BatchNorm"
  bottom: "map16_89_conv2"
  top: "map16_89_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_89_bn2"
  type: "BatchNorm"
  bottom: "map16_89_conv2"
  top: "map16_89_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_89_scale2"
  type: "Scale"
  bottom: "map16_89_conv2"
  top: "map16_89_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_89_relu2"
  type: "ReLU"
  bottom: "map16_89_conv2"
  top: "map16_89_conv2"
}
layer {
  name: "map16_89_conv_end"
  type: "Convolution"
  bottom: "map16_89_conv2"
  top: "map16_89_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_89_eltsum"
  type: "Eltwise"
  bottom: "map16_88_eltsum"
  bottom: "map16_89_conv_end"
  top: "map16_89_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_90_bn_pre"
  type: "BatchNorm"
  bottom: "map16_89_eltsum"
  top: "map16_90_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_90_bn_pre"
  type: "BatchNorm"
  bottom: "map16_89_eltsum"
  top: "map16_90_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_90_pre_scale"
  type: "Scale"
  bottom: "map16_90_bn_pre"
  top: "map16_90_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_90_pre_relu"
  type: "ReLU"
  bottom: "map16_90_bn_pre"
  top: "map16_90_bn_pre"
}
layer {
  name: "map16_90_conv1"
  type: "Convolution"
  bottom: "map16_90_bn_pre"
  top: "map16_90_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_90_bn1"
  type: "BatchNorm"
  bottom: "map16_90_conv1"
  top: "map16_90_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_90_bn1"
  type: "BatchNorm"
  bottom: "map16_90_conv1"
  top: "map16_90_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_90_scale1"
  type: "Scale"
  bottom: "map16_90_conv1"
  top: "map16_90_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_90_relu1"
  type: "ReLU"
  bottom: "map16_90_conv1"
  top: "map16_90_conv1"
}
layer {
  name: "map16_90_conv2"
  type: "Convolution"
  bottom: "map16_90_conv1"
  top: "map16_90_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_90_bn2"
  type: "BatchNorm"
  bottom: "map16_90_conv2"
  top: "map16_90_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_90_bn2"
  type: "BatchNorm"
  bottom: "map16_90_conv2"
  top: "map16_90_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_90_scale2"
  type: "Scale"
  bottom: "map16_90_conv2"
  top: "map16_90_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_90_relu2"
  type: "ReLU"
  bottom: "map16_90_conv2"
  top: "map16_90_conv2"
}
layer {
  name: "map16_90_conv_end"
  type: "Convolution"
  bottom: "map16_90_conv2"
  top: "map16_90_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_90_eltsum"
  type: "Eltwise"
  bottom: "map16_89_eltsum"
  bottom: "map16_90_conv_end"
  top: "map16_90_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_91_bn_pre"
  type: "BatchNorm"
  bottom: "map16_90_eltsum"
  top: "map16_91_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_91_bn_pre"
  type: "BatchNorm"
  bottom: "map16_90_eltsum"
  top: "map16_91_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_91_pre_scale"
  type: "Scale"
  bottom: "map16_91_bn_pre"
  top: "map16_91_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_91_pre_relu"
  type: "ReLU"
  bottom: "map16_91_bn_pre"
  top: "map16_91_bn_pre"
}
layer {
  name: "map16_91_conv1"
  type: "Convolution"
  bottom: "map16_91_bn_pre"
  top: "map16_91_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_91_bn1"
  type: "BatchNorm"
  bottom: "map16_91_conv1"
  top: "map16_91_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_91_bn1"
  type: "BatchNorm"
  bottom: "map16_91_conv1"
  top: "map16_91_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_91_scale1"
  type: "Scale"
  bottom: "map16_91_conv1"
  top: "map16_91_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_91_relu1"
  type: "ReLU"
  bottom: "map16_91_conv1"
  top: "map16_91_conv1"
}
layer {
  name: "map16_91_conv2"
  type: "Convolution"
  bottom: "map16_91_conv1"
  top: "map16_91_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_91_bn2"
  type: "BatchNorm"
  bottom: "map16_91_conv2"
  top: "map16_91_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_91_bn2"
  type: "BatchNorm"
  bottom: "map16_91_conv2"
  top: "map16_91_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_91_scale2"
  type: "Scale"
  bottom: "map16_91_conv2"
  top: "map16_91_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_91_relu2"
  type: "ReLU"
  bottom: "map16_91_conv2"
  top: "map16_91_conv2"
}
layer {
  name: "map16_91_conv_end"
  type: "Convolution"
  bottom: "map16_91_conv2"
  top: "map16_91_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_91_eltsum"
  type: "Eltwise"
  bottom: "map16_90_eltsum"
  bottom: "map16_91_conv_end"
  top: "map16_91_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_92_bn_pre"
  type: "BatchNorm"
  bottom: "map16_91_eltsum"
  top: "map16_92_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_92_bn_pre"
  type: "BatchNorm"
  bottom: "map16_91_eltsum"
  top: "map16_92_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_92_pre_scale"
  type: "Scale"
  bottom: "map16_92_bn_pre"
  top: "map16_92_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_92_pre_relu"
  type: "ReLU"
  bottom: "map16_92_bn_pre"
  top: "map16_92_bn_pre"
}
layer {
  name: "map16_92_conv1"
  type: "Convolution"
  bottom: "map16_92_bn_pre"
  top: "map16_92_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_92_bn1"
  type: "BatchNorm"
  bottom: "map16_92_conv1"
  top: "map16_92_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_92_bn1"
  type: "BatchNorm"
  bottom: "map16_92_conv1"
  top: "map16_92_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_92_scale1"
  type: "Scale"
  bottom: "map16_92_conv1"
  top: "map16_92_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_92_relu1"
  type: "ReLU"
  bottom: "map16_92_conv1"
  top: "map16_92_conv1"
}
layer {
  name: "map16_92_conv2"
  type: "Convolution"
  bottom: "map16_92_conv1"
  top: "map16_92_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_92_bn2"
  type: "BatchNorm"
  bottom: "map16_92_conv2"
  top: "map16_92_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_92_bn2"
  type: "BatchNorm"
  bottom: "map16_92_conv2"
  top: "map16_92_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_92_scale2"
  type: "Scale"
  bottom: "map16_92_conv2"
  top: "map16_92_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_92_relu2"
  type: "ReLU"
  bottom: "map16_92_conv2"
  top: "map16_92_conv2"
}
layer {
  name: "map16_92_conv_end"
  type: "Convolution"
  bottom: "map16_92_conv2"
  top: "map16_92_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_92_eltsum"
  type: "Eltwise"
  bottom: "map16_91_eltsum"
  bottom: "map16_92_conv_end"
  top: "map16_92_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_93_bn_pre"
  type: "BatchNorm"
  bottom: "map16_92_eltsum"
  top: "map16_93_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_93_bn_pre"
  type: "BatchNorm"
  bottom: "map16_92_eltsum"
  top: "map16_93_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_93_pre_scale"
  type: "Scale"
  bottom: "map16_93_bn_pre"
  top: "map16_93_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_93_pre_relu"
  type: "ReLU"
  bottom: "map16_93_bn_pre"
  top: "map16_93_bn_pre"
}
layer {
  name: "map16_93_conv1"
  type: "Convolution"
  bottom: "map16_93_bn_pre"
  top: "map16_93_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_93_bn1"
  type: "BatchNorm"
  bottom: "map16_93_conv1"
  top: "map16_93_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_93_bn1"
  type: "BatchNorm"
  bottom: "map16_93_conv1"
  top: "map16_93_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_93_scale1"
  type: "Scale"
  bottom: "map16_93_conv1"
  top: "map16_93_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_93_relu1"
  type: "ReLU"
  bottom: "map16_93_conv1"
  top: "map16_93_conv1"
}
layer {
  name: "map16_93_conv2"
  type: "Convolution"
  bottom: "map16_93_conv1"
  top: "map16_93_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_93_bn2"
  type: "BatchNorm"
  bottom: "map16_93_conv2"
  top: "map16_93_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_93_bn2"
  type: "BatchNorm"
  bottom: "map16_93_conv2"
  top: "map16_93_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_93_scale2"
  type: "Scale"
  bottom: "map16_93_conv2"
  top: "map16_93_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_93_relu2"
  type: "ReLU"
  bottom: "map16_93_conv2"
  top: "map16_93_conv2"
}
layer {
  name: "map16_93_conv_end"
  type: "Convolution"
  bottom: "map16_93_conv2"
  top: "map16_93_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_93_eltsum"
  type: "Eltwise"
  bottom: "map16_92_eltsum"
  bottom: "map16_93_conv_end"
  top: "map16_93_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_94_bn_pre"
  type: "BatchNorm"
  bottom: "map16_93_eltsum"
  top: "map16_94_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_94_bn_pre"
  type: "BatchNorm"
  bottom: "map16_93_eltsum"
  top: "map16_94_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_94_pre_scale"
  type: "Scale"
  bottom: "map16_94_bn_pre"
  top: "map16_94_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_94_pre_relu"
  type: "ReLU"
  bottom: "map16_94_bn_pre"
  top: "map16_94_bn_pre"
}
layer {
  name: "map16_94_conv1"
  type: "Convolution"
  bottom: "map16_94_bn_pre"
  top: "map16_94_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_94_bn1"
  type: "BatchNorm"
  bottom: "map16_94_conv1"
  top: "map16_94_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_94_bn1"
  type: "BatchNorm"
  bottom: "map16_94_conv1"
  top: "map16_94_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_94_scale1"
  type: "Scale"
  bottom: "map16_94_conv1"
  top: "map16_94_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_94_relu1"
  type: "ReLU"
  bottom: "map16_94_conv1"
  top: "map16_94_conv1"
}
layer {
  name: "map16_94_conv2"
  type: "Convolution"
  bottom: "map16_94_conv1"
  top: "map16_94_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_94_bn2"
  type: "BatchNorm"
  bottom: "map16_94_conv2"
  top: "map16_94_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_94_bn2"
  type: "BatchNorm"
  bottom: "map16_94_conv2"
  top: "map16_94_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_94_scale2"
  type: "Scale"
  bottom: "map16_94_conv2"
  top: "map16_94_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_94_relu2"
  type: "ReLU"
  bottom: "map16_94_conv2"
  top: "map16_94_conv2"
}
layer {
  name: "map16_94_conv_end"
  type: "Convolution"
  bottom: "map16_94_conv2"
  top: "map16_94_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_94_eltsum"
  type: "Eltwise"
  bottom: "map16_93_eltsum"
  bottom: "map16_94_conv_end"
  top: "map16_94_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_95_bn_pre"
  type: "BatchNorm"
  bottom: "map16_94_eltsum"
  top: "map16_95_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_95_bn_pre"
  type: "BatchNorm"
  bottom: "map16_94_eltsum"
  top: "map16_95_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_95_pre_scale"
  type: "Scale"
  bottom: "map16_95_bn_pre"
  top: "map16_95_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_95_pre_relu"
  type: "ReLU"
  bottom: "map16_95_bn_pre"
  top: "map16_95_bn_pre"
}
layer {
  name: "map16_95_conv1"
  type: "Convolution"
  bottom: "map16_95_bn_pre"
  top: "map16_95_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_95_bn1"
  type: "BatchNorm"
  bottom: "map16_95_conv1"
  top: "map16_95_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_95_bn1"
  type: "BatchNorm"
  bottom: "map16_95_conv1"
  top: "map16_95_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_95_scale1"
  type: "Scale"
  bottom: "map16_95_conv1"
  top: "map16_95_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_95_relu1"
  type: "ReLU"
  bottom: "map16_95_conv1"
  top: "map16_95_conv1"
}
layer {
  name: "map16_95_conv2"
  type: "Convolution"
  bottom: "map16_95_conv1"
  top: "map16_95_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_95_bn2"
  type: "BatchNorm"
  bottom: "map16_95_conv2"
  top: "map16_95_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_95_bn2"
  type: "BatchNorm"
  bottom: "map16_95_conv2"
  top: "map16_95_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_95_scale2"
  type: "Scale"
  bottom: "map16_95_conv2"
  top: "map16_95_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_95_relu2"
  type: "ReLU"
  bottom: "map16_95_conv2"
  top: "map16_95_conv2"
}
layer {
  name: "map16_95_conv_end"
  type: "Convolution"
  bottom: "map16_95_conv2"
  top: "map16_95_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_95_eltsum"
  type: "Eltwise"
  bottom: "map16_94_eltsum"
  bottom: "map16_95_conv_end"
  top: "map16_95_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_96_bn_pre"
  type: "BatchNorm"
  bottom: "map16_95_eltsum"
  top: "map16_96_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_96_bn_pre"
  type: "BatchNorm"
  bottom: "map16_95_eltsum"
  top: "map16_96_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_96_pre_scale"
  type: "Scale"
  bottom: "map16_96_bn_pre"
  top: "map16_96_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_96_pre_relu"
  type: "ReLU"
  bottom: "map16_96_bn_pre"
  top: "map16_96_bn_pre"
}
layer {
  name: "map16_96_conv1"
  type: "Convolution"
  bottom: "map16_96_bn_pre"
  top: "map16_96_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_96_bn1"
  type: "BatchNorm"
  bottom: "map16_96_conv1"
  top: "map16_96_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_96_bn1"
  type: "BatchNorm"
  bottom: "map16_96_conv1"
  top: "map16_96_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_96_scale1"
  type: "Scale"
  bottom: "map16_96_conv1"
  top: "map16_96_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_96_relu1"
  type: "ReLU"
  bottom: "map16_96_conv1"
  top: "map16_96_conv1"
}
layer {
  name: "map16_96_conv2"
  type: "Convolution"
  bottom: "map16_96_conv1"
  top: "map16_96_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_96_bn2"
  type: "BatchNorm"
  bottom: "map16_96_conv2"
  top: "map16_96_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_96_bn2"
  type: "BatchNorm"
  bottom: "map16_96_conv2"
  top: "map16_96_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_96_scale2"
  type: "Scale"
  bottom: "map16_96_conv2"
  top: "map16_96_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_96_relu2"
  type: "ReLU"
  bottom: "map16_96_conv2"
  top: "map16_96_conv2"
}
layer {
  name: "map16_96_conv_end"
  type: "Convolution"
  bottom: "map16_96_conv2"
  top: "map16_96_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_96_eltsum"
  type: "Eltwise"
  bottom: "map16_95_eltsum"
  bottom: "map16_96_conv_end"
  top: "map16_96_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_97_bn_pre"
  type: "BatchNorm"
  bottom: "map16_96_eltsum"
  top: "map16_97_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_97_bn_pre"
  type: "BatchNorm"
  bottom: "map16_96_eltsum"
  top: "map16_97_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_97_pre_scale"
  type: "Scale"
  bottom: "map16_97_bn_pre"
  top: "map16_97_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_97_pre_relu"
  type: "ReLU"
  bottom: "map16_97_bn_pre"
  top: "map16_97_bn_pre"
}
layer {
  name: "map16_97_conv1"
  type: "Convolution"
  bottom: "map16_97_bn_pre"
  top: "map16_97_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_97_bn1"
  type: "BatchNorm"
  bottom: "map16_97_conv1"
  top: "map16_97_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_97_bn1"
  type: "BatchNorm"
  bottom: "map16_97_conv1"
  top: "map16_97_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_97_scale1"
  type: "Scale"
  bottom: "map16_97_conv1"
  top: "map16_97_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_97_relu1"
  type: "ReLU"
  bottom: "map16_97_conv1"
  top: "map16_97_conv1"
}
layer {
  name: "map16_97_conv2"
  type: "Convolution"
  bottom: "map16_97_conv1"
  top: "map16_97_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_97_bn2"
  type: "BatchNorm"
  bottom: "map16_97_conv2"
  top: "map16_97_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_97_bn2"
  type: "BatchNorm"
  bottom: "map16_97_conv2"
  top: "map16_97_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_97_scale2"
  type: "Scale"
  bottom: "map16_97_conv2"
  top: "map16_97_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_97_relu2"
  type: "ReLU"
  bottom: "map16_97_conv2"
  top: "map16_97_conv2"
}
layer {
  name: "map16_97_conv_end"
  type: "Convolution"
  bottom: "map16_97_conv2"
  top: "map16_97_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_97_eltsum"
  type: "Eltwise"
  bottom: "map16_96_eltsum"
  bottom: "map16_97_conv_end"
  top: "map16_97_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_98_bn_pre"
  type: "BatchNorm"
  bottom: "map16_97_eltsum"
  top: "map16_98_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_98_bn_pre"
  type: "BatchNorm"
  bottom: "map16_97_eltsum"
  top: "map16_98_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_98_pre_scale"
  type: "Scale"
  bottom: "map16_98_bn_pre"
  top: "map16_98_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_98_pre_relu"
  type: "ReLU"
  bottom: "map16_98_bn_pre"
  top: "map16_98_bn_pre"
}
layer {
  name: "map16_98_conv1"
  type: "Convolution"
  bottom: "map16_98_bn_pre"
  top: "map16_98_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_98_bn1"
  type: "BatchNorm"
  bottom: "map16_98_conv1"
  top: "map16_98_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_98_bn1"
  type: "BatchNorm"
  bottom: "map16_98_conv1"
  top: "map16_98_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_98_scale1"
  type: "Scale"
  bottom: "map16_98_conv1"
  top: "map16_98_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_98_relu1"
  type: "ReLU"
  bottom: "map16_98_conv1"
  top: "map16_98_conv1"
}
layer {
  name: "map16_98_conv2"
  type: "Convolution"
  bottom: "map16_98_conv1"
  top: "map16_98_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_98_bn2"
  type: "BatchNorm"
  bottom: "map16_98_conv2"
  top: "map16_98_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_98_bn2"
  type: "BatchNorm"
  bottom: "map16_98_conv2"
  top: "map16_98_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_98_scale2"
  type: "Scale"
  bottom: "map16_98_conv2"
  top: "map16_98_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_98_relu2"
  type: "ReLU"
  bottom: "map16_98_conv2"
  top: "map16_98_conv2"
}
layer {
  name: "map16_98_conv_end"
  type: "Convolution"
  bottom: "map16_98_conv2"
  top: "map16_98_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_98_eltsum"
  type: "Eltwise"
  bottom: "map16_97_eltsum"
  bottom: "map16_98_conv_end"
  top: "map16_98_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_99_bn_pre"
  type: "BatchNorm"
  bottom: "map16_98_eltsum"
  top: "map16_99_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_99_bn_pre"
  type: "BatchNorm"
  bottom: "map16_98_eltsum"
  top: "map16_99_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_99_pre_scale"
  type: "Scale"
  bottom: "map16_99_bn_pre"
  top: "map16_99_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_99_pre_relu"
  type: "ReLU"
  bottom: "map16_99_bn_pre"
  top: "map16_99_bn_pre"
}
layer {
  name: "map16_99_conv1"
  type: "Convolution"
  bottom: "map16_99_bn_pre"
  top: "map16_99_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_99_bn1"
  type: "BatchNorm"
  bottom: "map16_99_conv1"
  top: "map16_99_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_99_bn1"
  type: "BatchNorm"
  bottom: "map16_99_conv1"
  top: "map16_99_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_99_scale1"
  type: "Scale"
  bottom: "map16_99_conv1"
  top: "map16_99_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_99_relu1"
  type: "ReLU"
  bottom: "map16_99_conv1"
  top: "map16_99_conv1"
}
layer {
  name: "map16_99_conv2"
  type: "Convolution"
  bottom: "map16_99_conv1"
  top: "map16_99_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_99_bn2"
  type: "BatchNorm"
  bottom: "map16_99_conv2"
  top: "map16_99_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_99_bn2"
  type: "BatchNorm"
  bottom: "map16_99_conv2"
  top: "map16_99_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_99_scale2"
  type: "Scale"
  bottom: "map16_99_conv2"
  top: "map16_99_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_99_relu2"
  type: "ReLU"
  bottom: "map16_99_conv2"
  top: "map16_99_conv2"
}
layer {
  name: "map16_99_conv_end"
  type: "Convolution"
  bottom: "map16_99_conv2"
  top: "map16_99_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_99_eltsum"
  type: "Eltwise"
  bottom: "map16_98_eltsum"
  bottom: "map16_99_conv_end"
  top: "map16_99_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_100_bn_pre"
  type: "BatchNorm"
  bottom: "map16_99_eltsum"
  top: "map16_100_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_100_bn_pre"
  type: "BatchNorm"
  bottom: "map16_99_eltsum"
  top: "map16_100_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_100_pre_scale"
  type: "Scale"
  bottom: "map16_100_bn_pre"
  top: "map16_100_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_100_pre_relu"
  type: "ReLU"
  bottom: "map16_100_bn_pre"
  top: "map16_100_bn_pre"
}
layer {
  name: "map16_100_conv1"
  type: "Convolution"
  bottom: "map16_100_bn_pre"
  top: "map16_100_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_100_bn1"
  type: "BatchNorm"
  bottom: "map16_100_conv1"
  top: "map16_100_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_100_bn1"
  type: "BatchNorm"
  bottom: "map16_100_conv1"
  top: "map16_100_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_100_scale1"
  type: "Scale"
  bottom: "map16_100_conv1"
  top: "map16_100_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_100_relu1"
  type: "ReLU"
  bottom: "map16_100_conv1"
  top: "map16_100_conv1"
}
layer {
  name: "map16_100_conv2"
  type: "Convolution"
  bottom: "map16_100_conv1"
  top: "map16_100_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_100_bn2"
  type: "BatchNorm"
  bottom: "map16_100_conv2"
  top: "map16_100_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_100_bn2"
  type: "BatchNorm"
  bottom: "map16_100_conv2"
  top: "map16_100_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_100_scale2"
  type: "Scale"
  bottom: "map16_100_conv2"
  top: "map16_100_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_100_relu2"
  type: "ReLU"
  bottom: "map16_100_conv2"
  top: "map16_100_conv2"
}
layer {
  name: "map16_100_conv_end"
  type: "Convolution"
  bottom: "map16_100_conv2"
  top: "map16_100_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_100_eltsum"
  type: "Eltwise"
  bottom: "map16_99_eltsum"
  bottom: "map16_100_conv_end"
  top: "map16_100_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_101_bn_pre"
  type: "BatchNorm"
  bottom: "map16_100_eltsum"
  top: "map16_101_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_101_bn_pre"
  type: "BatchNorm"
  bottom: "map16_100_eltsum"
  top: "map16_101_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_101_pre_scale"
  type: "Scale"
  bottom: "map16_101_bn_pre"
  top: "map16_101_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_101_pre_relu"
  type: "ReLU"
  bottom: "map16_101_bn_pre"
  top: "map16_101_bn_pre"
}
layer {
  name: "map16_101_conv1"
  type: "Convolution"
  bottom: "map16_101_bn_pre"
  top: "map16_101_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_101_bn1"
  type: "BatchNorm"
  bottom: "map16_101_conv1"
  top: "map16_101_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_101_bn1"
  type: "BatchNorm"
  bottom: "map16_101_conv1"
  top: "map16_101_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_101_scale1"
  type: "Scale"
  bottom: "map16_101_conv1"
  top: "map16_101_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_101_relu1"
  type: "ReLU"
  bottom: "map16_101_conv1"
  top: "map16_101_conv1"
}
layer {
  name: "map16_101_conv2"
  type: "Convolution"
  bottom: "map16_101_conv1"
  top: "map16_101_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_101_bn2"
  type: "BatchNorm"
  bottom: "map16_101_conv2"
  top: "map16_101_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_101_bn2"
  type: "BatchNorm"
  bottom: "map16_101_conv2"
  top: "map16_101_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_101_scale2"
  type: "Scale"
  bottom: "map16_101_conv2"
  top: "map16_101_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_101_relu2"
  type: "ReLU"
  bottom: "map16_101_conv2"
  top: "map16_101_conv2"
}
layer {
  name: "map16_101_conv_end"
  type: "Convolution"
  bottom: "map16_101_conv2"
  top: "map16_101_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_101_eltsum"
  type: "Eltwise"
  bottom: "map16_100_eltsum"
  bottom: "map16_101_conv_end"
  top: "map16_101_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_102_bn_pre"
  type: "BatchNorm"
  bottom: "map16_101_eltsum"
  top: "map16_102_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_102_bn_pre"
  type: "BatchNorm"
  bottom: "map16_101_eltsum"
  top: "map16_102_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_102_pre_scale"
  type: "Scale"
  bottom: "map16_102_bn_pre"
  top: "map16_102_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_102_pre_relu"
  type: "ReLU"
  bottom: "map16_102_bn_pre"
  top: "map16_102_bn_pre"
}
layer {
  name: "map16_102_conv1"
  type: "Convolution"
  bottom: "map16_102_bn_pre"
  top: "map16_102_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_102_bn1"
  type: "BatchNorm"
  bottom: "map16_102_conv1"
  top: "map16_102_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_102_bn1"
  type: "BatchNorm"
  bottom: "map16_102_conv1"
  top: "map16_102_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_102_scale1"
  type: "Scale"
  bottom: "map16_102_conv1"
  top: "map16_102_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_102_relu1"
  type: "ReLU"
  bottom: "map16_102_conv1"
  top: "map16_102_conv1"
}
layer {
  name: "map16_102_conv2"
  type: "Convolution"
  bottom: "map16_102_conv1"
  top: "map16_102_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_102_bn2"
  type: "BatchNorm"
  bottom: "map16_102_conv2"
  top: "map16_102_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_102_bn2"
  type: "BatchNorm"
  bottom: "map16_102_conv2"
  top: "map16_102_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_102_scale2"
  type: "Scale"
  bottom: "map16_102_conv2"
  top: "map16_102_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_102_relu2"
  type: "ReLU"
  bottom: "map16_102_conv2"
  top: "map16_102_conv2"
}
layer {
  name: "map16_102_conv_end"
  type: "Convolution"
  bottom: "map16_102_conv2"
  top: "map16_102_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_102_eltsum"
  type: "Eltwise"
  bottom: "map16_101_eltsum"
  bottom: "map16_102_conv_end"
  top: "map16_102_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_103_bn_pre"
  type: "BatchNorm"
  bottom: "map16_102_eltsum"
  top: "map16_103_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_103_bn_pre"
  type: "BatchNorm"
  bottom: "map16_102_eltsum"
  top: "map16_103_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_103_pre_scale"
  type: "Scale"
  bottom: "map16_103_bn_pre"
  top: "map16_103_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_103_pre_relu"
  type: "ReLU"
  bottom: "map16_103_bn_pre"
  top: "map16_103_bn_pre"
}
layer {
  name: "map16_103_conv1"
  type: "Convolution"
  bottom: "map16_103_bn_pre"
  top: "map16_103_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_103_bn1"
  type: "BatchNorm"
  bottom: "map16_103_conv1"
  top: "map16_103_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_103_bn1"
  type: "BatchNorm"
  bottom: "map16_103_conv1"
  top: "map16_103_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_103_scale1"
  type: "Scale"
  bottom: "map16_103_conv1"
  top: "map16_103_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_103_relu1"
  type: "ReLU"
  bottom: "map16_103_conv1"
  top: "map16_103_conv1"
}
layer {
  name: "map16_103_conv2"
  type: "Convolution"
  bottom: "map16_103_conv1"
  top: "map16_103_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_103_bn2"
  type: "BatchNorm"
  bottom: "map16_103_conv2"
  top: "map16_103_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_103_bn2"
  type: "BatchNorm"
  bottom: "map16_103_conv2"
  top: "map16_103_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_103_scale2"
  type: "Scale"
  bottom: "map16_103_conv2"
  top: "map16_103_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_103_relu2"
  type: "ReLU"
  bottom: "map16_103_conv2"
  top: "map16_103_conv2"
}
layer {
  name: "map16_103_conv_end"
  type: "Convolution"
  bottom: "map16_103_conv2"
  top: "map16_103_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_103_eltsum"
  type: "Eltwise"
  bottom: "map16_102_eltsum"
  bottom: "map16_103_conv_end"
  top: "map16_103_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_104_bn_pre"
  type: "BatchNorm"
  bottom: "map16_103_eltsum"
  top: "map16_104_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_104_bn_pre"
  type: "BatchNorm"
  bottom: "map16_103_eltsum"
  top: "map16_104_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_104_pre_scale"
  type: "Scale"
  bottom: "map16_104_bn_pre"
  top: "map16_104_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_104_pre_relu"
  type: "ReLU"
  bottom: "map16_104_bn_pre"
  top: "map16_104_bn_pre"
}
layer {
  name: "map16_104_conv1"
  type: "Convolution"
  bottom: "map16_104_bn_pre"
  top: "map16_104_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_104_bn1"
  type: "BatchNorm"
  bottom: "map16_104_conv1"
  top: "map16_104_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_104_bn1"
  type: "BatchNorm"
  bottom: "map16_104_conv1"
  top: "map16_104_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_104_scale1"
  type: "Scale"
  bottom: "map16_104_conv1"
  top: "map16_104_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_104_relu1"
  type: "ReLU"
  bottom: "map16_104_conv1"
  top: "map16_104_conv1"
}
layer {
  name: "map16_104_conv2"
  type: "Convolution"
  bottom: "map16_104_conv1"
  top: "map16_104_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_104_bn2"
  type: "BatchNorm"
  bottom: "map16_104_conv2"
  top: "map16_104_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_104_bn2"
  type: "BatchNorm"
  bottom: "map16_104_conv2"
  top: "map16_104_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_104_scale2"
  type: "Scale"
  bottom: "map16_104_conv2"
  top: "map16_104_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_104_relu2"
  type: "ReLU"
  bottom: "map16_104_conv2"
  top: "map16_104_conv2"
}
layer {
  name: "map16_104_conv_end"
  type: "Convolution"
  bottom: "map16_104_conv2"
  top: "map16_104_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_104_eltsum"
  type: "Eltwise"
  bottom: "map16_103_eltsum"
  bottom: "map16_104_conv_end"
  top: "map16_104_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_105_bn_pre"
  type: "BatchNorm"
  bottom: "map16_104_eltsum"
  top: "map16_105_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_105_bn_pre"
  type: "BatchNorm"
  bottom: "map16_104_eltsum"
  top: "map16_105_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_105_pre_scale"
  type: "Scale"
  bottom: "map16_105_bn_pre"
  top: "map16_105_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_105_pre_relu"
  type: "ReLU"
  bottom: "map16_105_bn_pre"
  top: "map16_105_bn_pre"
}
layer {
  name: "map16_105_conv1"
  type: "Convolution"
  bottom: "map16_105_bn_pre"
  top: "map16_105_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_105_bn1"
  type: "BatchNorm"
  bottom: "map16_105_conv1"
  top: "map16_105_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_105_bn1"
  type: "BatchNorm"
  bottom: "map16_105_conv1"
  top: "map16_105_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_105_scale1"
  type: "Scale"
  bottom: "map16_105_conv1"
  top: "map16_105_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_105_relu1"
  type: "ReLU"
  bottom: "map16_105_conv1"
  top: "map16_105_conv1"
}
layer {
  name: "map16_105_conv2"
  type: "Convolution"
  bottom: "map16_105_conv1"
  top: "map16_105_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_105_bn2"
  type: "BatchNorm"
  bottom: "map16_105_conv2"
  top: "map16_105_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_105_bn2"
  type: "BatchNorm"
  bottom: "map16_105_conv2"
  top: "map16_105_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_105_scale2"
  type: "Scale"
  bottom: "map16_105_conv2"
  top: "map16_105_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_105_relu2"
  type: "ReLU"
  bottom: "map16_105_conv2"
  top: "map16_105_conv2"
}
layer {
  name: "map16_105_conv_end"
  type: "Convolution"
  bottom: "map16_105_conv2"
  top: "map16_105_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_105_eltsum"
  type: "Eltwise"
  bottom: "map16_104_eltsum"
  bottom: "map16_105_conv_end"
  top: "map16_105_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_106_bn_pre"
  type: "BatchNorm"
  bottom: "map16_105_eltsum"
  top: "map16_106_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_106_bn_pre"
  type: "BatchNorm"
  bottom: "map16_105_eltsum"
  top: "map16_106_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_106_pre_scale"
  type: "Scale"
  bottom: "map16_106_bn_pre"
  top: "map16_106_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_106_pre_relu"
  type: "ReLU"
  bottom: "map16_106_bn_pre"
  top: "map16_106_bn_pre"
}
layer {
  name: "map16_106_conv1"
  type: "Convolution"
  bottom: "map16_106_bn_pre"
  top: "map16_106_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_106_bn1"
  type: "BatchNorm"
  bottom: "map16_106_conv1"
  top: "map16_106_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_106_bn1"
  type: "BatchNorm"
  bottom: "map16_106_conv1"
  top: "map16_106_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_106_scale1"
  type: "Scale"
  bottom: "map16_106_conv1"
  top: "map16_106_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_106_relu1"
  type: "ReLU"
  bottom: "map16_106_conv1"
  top: "map16_106_conv1"
}
layer {
  name: "map16_106_conv2"
  type: "Convolution"
  bottom: "map16_106_conv1"
  top: "map16_106_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_106_bn2"
  type: "BatchNorm"
  bottom: "map16_106_conv2"
  top: "map16_106_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_106_bn2"
  type: "BatchNorm"
  bottom: "map16_106_conv2"
  top: "map16_106_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_106_scale2"
  type: "Scale"
  bottom: "map16_106_conv2"
  top: "map16_106_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_106_relu2"
  type: "ReLU"
  bottom: "map16_106_conv2"
  top: "map16_106_conv2"
}
layer {
  name: "map16_106_conv_end"
  type: "Convolution"
  bottom: "map16_106_conv2"
  top: "map16_106_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_106_eltsum"
  type: "Eltwise"
  bottom: "map16_105_eltsum"
  bottom: "map16_106_conv_end"
  top: "map16_106_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_107_bn_pre"
  type: "BatchNorm"
  bottom: "map16_106_eltsum"
  top: "map16_107_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_107_bn_pre"
  type: "BatchNorm"
  bottom: "map16_106_eltsum"
  top: "map16_107_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_107_pre_scale"
  type: "Scale"
  bottom: "map16_107_bn_pre"
  top: "map16_107_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_107_pre_relu"
  type: "ReLU"
  bottom: "map16_107_bn_pre"
  top: "map16_107_bn_pre"
}
layer {
  name: "map16_107_conv1"
  type: "Convolution"
  bottom: "map16_107_bn_pre"
  top: "map16_107_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_107_bn1"
  type: "BatchNorm"
  bottom: "map16_107_conv1"
  top: "map16_107_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_107_bn1"
  type: "BatchNorm"
  bottom: "map16_107_conv1"
  top: "map16_107_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_107_scale1"
  type: "Scale"
  bottom: "map16_107_conv1"
  top: "map16_107_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_107_relu1"
  type: "ReLU"
  bottom: "map16_107_conv1"
  top: "map16_107_conv1"
}
layer {
  name: "map16_107_conv2"
  type: "Convolution"
  bottom: "map16_107_conv1"
  top: "map16_107_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_107_bn2"
  type: "BatchNorm"
  bottom: "map16_107_conv2"
  top: "map16_107_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_107_bn2"
  type: "BatchNorm"
  bottom: "map16_107_conv2"
  top: "map16_107_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_107_scale2"
  type: "Scale"
  bottom: "map16_107_conv2"
  top: "map16_107_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_107_relu2"
  type: "ReLU"
  bottom: "map16_107_conv2"
  top: "map16_107_conv2"
}
layer {
  name: "map16_107_conv_end"
  type: "Convolution"
  bottom: "map16_107_conv2"
  top: "map16_107_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_107_eltsum"
  type: "Eltwise"
  bottom: "map16_106_eltsum"
  bottom: "map16_107_conv_end"
  top: "map16_107_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_108_bn_pre"
  type: "BatchNorm"
  bottom: "map16_107_eltsum"
  top: "map16_108_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_108_bn_pre"
  type: "BatchNorm"
  bottom: "map16_107_eltsum"
  top: "map16_108_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_108_pre_scale"
  type: "Scale"
  bottom: "map16_108_bn_pre"
  top: "map16_108_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_108_pre_relu"
  type: "ReLU"
  bottom: "map16_108_bn_pre"
  top: "map16_108_bn_pre"
}
layer {
  name: "map16_108_conv1"
  type: "Convolution"
  bottom: "map16_108_bn_pre"
  top: "map16_108_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_108_bn1"
  type: "BatchNorm"
  bottom: "map16_108_conv1"
  top: "map16_108_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_108_bn1"
  type: "BatchNorm"
  bottom: "map16_108_conv1"
  top: "map16_108_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_108_scale1"
  type: "Scale"
  bottom: "map16_108_conv1"
  top: "map16_108_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_108_relu1"
  type: "ReLU"
  bottom: "map16_108_conv1"
  top: "map16_108_conv1"
}
layer {
  name: "map16_108_conv2"
  type: "Convolution"
  bottom: "map16_108_conv1"
  top: "map16_108_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_108_bn2"
  type: "BatchNorm"
  bottom: "map16_108_conv2"
  top: "map16_108_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_108_bn2"
  type: "BatchNorm"
  bottom: "map16_108_conv2"
  top: "map16_108_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_108_scale2"
  type: "Scale"
  bottom: "map16_108_conv2"
  top: "map16_108_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_108_relu2"
  type: "ReLU"
  bottom: "map16_108_conv2"
  top: "map16_108_conv2"
}
layer {
  name: "map16_108_conv_end"
  type: "Convolution"
  bottom: "map16_108_conv2"
  top: "map16_108_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_108_eltsum"
  type: "Eltwise"
  bottom: "map16_107_eltsum"
  bottom: "map16_108_conv_end"
  top: "map16_108_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_109_bn_pre"
  type: "BatchNorm"
  bottom: "map16_108_eltsum"
  top: "map16_109_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_109_bn_pre"
  type: "BatchNorm"
  bottom: "map16_108_eltsum"
  top: "map16_109_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_109_pre_scale"
  type: "Scale"
  bottom: "map16_109_bn_pre"
  top: "map16_109_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_109_pre_relu"
  type: "ReLU"
  bottom: "map16_109_bn_pre"
  top: "map16_109_bn_pre"
}
layer {
  name: "map16_109_conv1"
  type: "Convolution"
  bottom: "map16_109_bn_pre"
  top: "map16_109_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_109_bn1"
  type: "BatchNorm"
  bottom: "map16_109_conv1"
  top: "map16_109_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_109_bn1"
  type: "BatchNorm"
  bottom: "map16_109_conv1"
  top: "map16_109_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_109_scale1"
  type: "Scale"
  bottom: "map16_109_conv1"
  top: "map16_109_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_109_relu1"
  type: "ReLU"
  bottom: "map16_109_conv1"
  top: "map16_109_conv1"
}
layer {
  name: "map16_109_conv2"
  type: "Convolution"
  bottom: "map16_109_conv1"
  top: "map16_109_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_109_bn2"
  type: "BatchNorm"
  bottom: "map16_109_conv2"
  top: "map16_109_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_109_bn2"
  type: "BatchNorm"
  bottom: "map16_109_conv2"
  top: "map16_109_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_109_scale2"
  type: "Scale"
  bottom: "map16_109_conv2"
  top: "map16_109_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_109_relu2"
  type: "ReLU"
  bottom: "map16_109_conv2"
  top: "map16_109_conv2"
}
layer {
  name: "map16_109_conv_end"
  type: "Convolution"
  bottom: "map16_109_conv2"
  top: "map16_109_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_109_eltsum"
  type: "Eltwise"
  bottom: "map16_108_eltsum"
  bottom: "map16_109_conv_end"
  top: "map16_109_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_110_bn_pre"
  type: "BatchNorm"
  bottom: "map16_109_eltsum"
  top: "map16_110_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_110_bn_pre"
  type: "BatchNorm"
  bottom: "map16_109_eltsum"
  top: "map16_110_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_110_pre_scale"
  type: "Scale"
  bottom: "map16_110_bn_pre"
  top: "map16_110_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_110_pre_relu"
  type: "ReLU"
  bottom: "map16_110_bn_pre"
  top: "map16_110_bn_pre"
}
layer {
  name: "map16_110_conv1"
  type: "Convolution"
  bottom: "map16_110_bn_pre"
  top: "map16_110_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_110_bn1"
  type: "BatchNorm"
  bottom: "map16_110_conv1"
  top: "map16_110_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_110_bn1"
  type: "BatchNorm"
  bottom: "map16_110_conv1"
  top: "map16_110_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_110_scale1"
  type: "Scale"
  bottom: "map16_110_conv1"
  top: "map16_110_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_110_relu1"
  type: "ReLU"
  bottom: "map16_110_conv1"
  top: "map16_110_conv1"
}
layer {
  name: "map16_110_conv2"
  type: "Convolution"
  bottom: "map16_110_conv1"
  top: "map16_110_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_110_bn2"
  type: "BatchNorm"
  bottom: "map16_110_conv2"
  top: "map16_110_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_110_bn2"
  type: "BatchNorm"
  bottom: "map16_110_conv2"
  top: "map16_110_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_110_scale2"
  type: "Scale"
  bottom: "map16_110_conv2"
  top: "map16_110_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_110_relu2"
  type: "ReLU"
  bottom: "map16_110_conv2"
  top: "map16_110_conv2"
}
layer {
  name: "map16_110_conv_end"
  type: "Convolution"
  bottom: "map16_110_conv2"
  top: "map16_110_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_110_eltsum"
  type: "Eltwise"
  bottom: "map16_109_eltsum"
  bottom: "map16_110_conv_end"
  top: "map16_110_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map16_111_bn_pre"
  type: "BatchNorm"
  bottom: "map16_110_eltsum"
  top: "map16_111_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_111_bn_pre"
  type: "BatchNorm"
  bottom: "map16_110_eltsum"
  top: "map16_111_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_111_pre_scale"
  type: "Scale"
  bottom: "map16_111_bn_pre"
  top: "map16_111_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_111_pre_relu"
  type: "ReLU"
  bottom: "map16_111_bn_pre"
  top: "map16_111_bn_pre"
}
layer {
  name: "map16_111_conv1"
  type: "Convolution"
  bottom: "map16_111_bn_pre"
  top: "map16_111_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_111_bn1"
  type: "BatchNorm"
  bottom: "map16_111_conv1"
  top: "map16_111_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_111_bn1"
  type: "BatchNorm"
  bottom: "map16_111_conv1"
  top: "map16_111_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_111_scale1"
  type: "Scale"
  bottom: "map16_111_conv1"
  top: "map16_111_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_111_relu1"
  type: "ReLU"
  bottom: "map16_111_conv1"
  top: "map16_111_conv1"
}
layer {
  name: "map16_111_conv2"
  type: "Convolution"
  bottom: "map16_111_conv1"
  top: "map16_111_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_111_bn2"
  type: "BatchNorm"
  bottom: "map16_111_conv2"
  top: "map16_111_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map16_111_bn2"
  type: "BatchNorm"
  bottom: "map16_111_conv2"
  top: "map16_111_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map16_111_scale2"
  type: "Scale"
  bottom: "map16_111_conv2"
  top: "map16_111_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map16_111_relu2"
  type: "ReLU"
  bottom: "map16_111_conv2"
  top: "map16_111_conv2"
}
layer {
  name: "map16_111_conv_end"
  type: "Convolution"
  bottom: "map16_111_conv2"
  top: "map16_111_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map16_111_eltsum"
  type: "Eltwise"
  bottom: "map16_110_eltsum"
  bottom: "map16_111_conv_end"
  top: "map16_111_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_1_bn_pre"
  type: "BatchNorm"
  bottom: "map16_111_eltsum"
  top: "map32_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn_pre"
  type: "BatchNorm"
  bottom: "map16_111_eltsum"
  top: "map32_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_pre_scale"
  type: "Scale"
  bottom: "map32_1_bn_pre"
  top: "map32_1_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_pre_relu"
  type: "ReLU"
  bottom: "map32_1_bn_pre"
  top: "map32_1_bn_pre"
}
layer {
  name: "map32_1_conv1"
  type: "Convolution"
  bottom: "map32_1_bn_pre"
  top: "map32_1_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn1"
  type: "BatchNorm"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn1"
  type: "BatchNorm"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale1"
  type: "Scale"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_relu1"
  type: "ReLU"
  bottom: "map32_1_conv1"
  top: "map32_1_conv1"
}
layer {
  name: "map32_1_conv2"
  type: "Convolution"
  bottom: "map32_1_conv1"
  top: "map32_1_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_bn2"
  type: "BatchNorm"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_1_bn2"
  type: "BatchNorm"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_1_scale2"
  type: "Scale"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_1_relu2"
  type: "ReLU"
  bottom: "map32_1_conv2"
  top: "map32_1_conv2"
}
layer {
  name: "map32_1_conv_end"
  type: "Convolution"
  bottom: "map32_1_conv2"
  top: "map32_1_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "map32_1_bn_pre"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_1_eltsum"
  type: "Eltwise"
  bottom: "Convolution2"
  bottom: "map32_1_conv_end"
  top: "map32_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_2_bn_pre"
  type: "BatchNorm"
  bottom: "map32_1_eltsum"
  top: "map32_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn_pre"
  type: "BatchNorm"
  bottom: "map32_1_eltsum"
  top: "map32_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_pre_scale"
  type: "Scale"
  bottom: "map32_2_bn_pre"
  top: "map32_2_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_pre_relu"
  type: "ReLU"
  bottom: "map32_2_bn_pre"
  top: "map32_2_bn_pre"
}
layer {
  name: "map32_2_conv1"
  type: "Convolution"
  bottom: "map32_2_bn_pre"
  top: "map32_2_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_bn1"
  type: "BatchNorm"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn1"
  type: "BatchNorm"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_scale1"
  type: "Scale"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_relu1"
  type: "ReLU"
  bottom: "map32_2_conv1"
  top: "map32_2_conv1"
}
layer {
  name: "map32_2_conv2"
  type: "Convolution"
  bottom: "map32_2_conv1"
  top: "map32_2_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_bn2"
  type: "BatchNorm"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_2_bn2"
  type: "BatchNorm"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_2_scale2"
  type: "Scale"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_2_relu2"
  type: "ReLU"
  bottom: "map32_2_conv2"
  top: "map32_2_conv2"
}
layer {
  name: "map32_2_conv_end"
  type: "Convolution"
  bottom: "map32_2_conv2"
  top: "map32_2_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_2_eltsum"
  type: "Eltwise"
  bottom: "map32_1_eltsum"
  bottom: "map32_2_conv_end"
  top: "map32_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_3_bn_pre"
  type: "BatchNorm"
  bottom: "map32_2_eltsum"
  top: "map32_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn_pre"
  type: "BatchNorm"
  bottom: "map32_2_eltsum"
  top: "map32_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_pre_scale"
  type: "Scale"
  bottom: "map32_3_bn_pre"
  top: "map32_3_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_pre_relu"
  type: "ReLU"
  bottom: "map32_3_bn_pre"
  top: "map32_3_bn_pre"
}
layer {
  name: "map32_3_conv1"
  type: "Convolution"
  bottom: "map32_3_bn_pre"
  top: "map32_3_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_bn1"
  type: "BatchNorm"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn1"
  type: "BatchNorm"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_scale1"
  type: "Scale"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_relu1"
  type: "ReLU"
  bottom: "map32_3_conv1"
  top: "map32_3_conv1"
}
layer {
  name: "map32_3_conv2"
  type: "Convolution"
  bottom: "map32_3_conv1"
  top: "map32_3_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_bn2"
  type: "BatchNorm"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_3_bn2"
  type: "BatchNorm"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_3_scale2"
  type: "Scale"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_3_relu2"
  type: "ReLU"
  bottom: "map32_3_conv2"
  top: "map32_3_conv2"
}
layer {
  name: "map32_3_conv_end"
  type: "Convolution"
  bottom: "map32_3_conv2"
  top: "map32_3_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_3_eltsum"
  type: "Eltwise"
  bottom: "map32_2_eltsum"
  bottom: "map32_3_conv_end"
  top: "map32_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_4_bn_pre"
  type: "BatchNorm"
  bottom: "map32_3_eltsum"
  top: "map32_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_4_bn_pre"
  type: "BatchNorm"
  bottom: "map32_3_eltsum"
  top: "map32_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_4_pre_scale"
  type: "Scale"
  bottom: "map32_4_bn_pre"
  top: "map32_4_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_4_pre_relu"
  type: "ReLU"
  bottom: "map32_4_bn_pre"
  top: "map32_4_bn_pre"
}
layer {
  name: "map32_4_conv1"
  type: "Convolution"
  bottom: "map32_4_bn_pre"
  top: "map32_4_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_4_bn1"
  type: "BatchNorm"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_4_bn1"
  type: "BatchNorm"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_4_scale1"
  type: "Scale"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_4_relu1"
  type: "ReLU"
  bottom: "map32_4_conv1"
  top: "map32_4_conv1"
}
layer {
  name: "map32_4_conv2"
  type: "Convolution"
  bottom: "map32_4_conv1"
  top: "map32_4_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_4_bn2"
  type: "BatchNorm"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_4_bn2"
  type: "BatchNorm"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_4_scale2"
  type: "Scale"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_4_relu2"
  type: "ReLU"
  bottom: "map32_4_conv2"
  top: "map32_4_conv2"
}
layer {
  name: "map32_4_conv_end"
  type: "Convolution"
  bottom: "map32_4_conv2"
  top: "map32_4_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_4_eltsum"
  type: "Eltwise"
  bottom: "map32_3_eltsum"
  bottom: "map32_4_conv_end"
  top: "map32_4_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_5_bn_pre"
  type: "BatchNorm"
  bottom: "map32_4_eltsum"
  top: "map32_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_5_bn_pre"
  type: "BatchNorm"
  bottom: "map32_4_eltsum"
  top: "map32_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_5_pre_scale"
  type: "Scale"
  bottom: "map32_5_bn_pre"
  top: "map32_5_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_5_pre_relu"
  type: "ReLU"
  bottom: "map32_5_bn_pre"
  top: "map32_5_bn_pre"
}
layer {
  name: "map32_5_conv1"
  type: "Convolution"
  bottom: "map32_5_bn_pre"
  top: "map32_5_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_5_bn1"
  type: "BatchNorm"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_5_bn1"
  type: "BatchNorm"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_5_scale1"
  type: "Scale"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_5_relu1"
  type: "ReLU"
  bottom: "map32_5_conv1"
  top: "map32_5_conv1"
}
layer {
  name: "map32_5_conv2"
  type: "Convolution"
  bottom: "map32_5_conv1"
  top: "map32_5_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_5_bn2"
  type: "BatchNorm"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_5_bn2"
  type: "BatchNorm"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_5_scale2"
  type: "Scale"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_5_relu2"
  type: "ReLU"
  bottom: "map32_5_conv2"
  top: "map32_5_conv2"
}
layer {
  name: "map32_5_conv_end"
  type: "Convolution"
  bottom: "map32_5_conv2"
  top: "map32_5_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_5_eltsum"
  type: "Eltwise"
  bottom: "map32_4_eltsum"
  bottom: "map32_5_conv_end"
  top: "map32_5_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_6_bn_pre"
  type: "BatchNorm"
  bottom: "map32_5_eltsum"
  top: "map32_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_6_bn_pre"
  type: "BatchNorm"
  bottom: "map32_5_eltsum"
  top: "map32_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_6_pre_scale"
  type: "Scale"
  bottom: "map32_6_bn_pre"
  top: "map32_6_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_6_pre_relu"
  type: "ReLU"
  bottom: "map32_6_bn_pre"
  top: "map32_6_bn_pre"
}
layer {
  name: "map32_6_conv1"
  type: "Convolution"
  bottom: "map32_6_bn_pre"
  top: "map32_6_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_6_bn1"
  type: "BatchNorm"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_6_bn1"
  type: "BatchNorm"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_6_scale1"
  type: "Scale"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_6_relu1"
  type: "ReLU"
  bottom: "map32_6_conv1"
  top: "map32_6_conv1"
}
layer {
  name: "map32_6_conv2"
  type: "Convolution"
  bottom: "map32_6_conv1"
  top: "map32_6_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_6_bn2"
  type: "BatchNorm"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_6_bn2"
  type: "BatchNorm"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_6_scale2"
  type: "Scale"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_6_relu2"
  type: "ReLU"
  bottom: "map32_6_conv2"
  top: "map32_6_conv2"
}
layer {
  name: "map32_6_conv_end"
  type: "Convolution"
  bottom: "map32_6_conv2"
  top: "map32_6_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_6_eltsum"
  type: "Eltwise"
  bottom: "map32_5_eltsum"
  bottom: "map32_6_conv_end"
  top: "map32_6_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_7_bn_pre"
  type: "BatchNorm"
  bottom: "map32_6_eltsum"
  top: "map32_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_7_bn_pre"
  type: "BatchNorm"
  bottom: "map32_6_eltsum"
  top: "map32_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_7_pre_scale"
  type: "Scale"
  bottom: "map32_7_bn_pre"
  top: "map32_7_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_7_pre_relu"
  type: "ReLU"
  bottom: "map32_7_bn_pre"
  top: "map32_7_bn_pre"
}
layer {
  name: "map32_7_conv1"
  type: "Convolution"
  bottom: "map32_7_bn_pre"
  top: "map32_7_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_7_bn1"
  type: "BatchNorm"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_7_bn1"
  type: "BatchNorm"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_7_scale1"
  type: "Scale"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_7_relu1"
  type: "ReLU"
  bottom: "map32_7_conv1"
  top: "map32_7_conv1"
}
layer {
  name: "map32_7_conv2"
  type: "Convolution"
  bottom: "map32_7_conv1"
  top: "map32_7_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_7_bn2"
  type: "BatchNorm"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_7_bn2"
  type: "BatchNorm"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_7_scale2"
  type: "Scale"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_7_relu2"
  type: "ReLU"
  bottom: "map32_7_conv2"
  top: "map32_7_conv2"
}
layer {
  name: "map32_7_conv_end"
  type: "Convolution"
  bottom: "map32_7_conv2"
  top: "map32_7_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_7_eltsum"
  type: "Eltwise"
  bottom: "map32_6_eltsum"
  bottom: "map32_7_conv_end"
  top: "map32_7_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_8_bn_pre"
  type: "BatchNorm"
  bottom: "map32_7_eltsum"
  top: "map32_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_8_bn_pre"
  type: "BatchNorm"
  bottom: "map32_7_eltsum"
  top: "map32_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_8_pre_scale"
  type: "Scale"
  bottom: "map32_8_bn_pre"
  top: "map32_8_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_8_pre_relu"
  type: "ReLU"
  bottom: "map32_8_bn_pre"
  top: "map32_8_bn_pre"
}
layer {
  name: "map32_8_conv1"
  type: "Convolution"
  bottom: "map32_8_bn_pre"
  top: "map32_8_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_8_bn1"
  type: "BatchNorm"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_8_bn1"
  type: "BatchNorm"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_8_scale1"
  type: "Scale"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_8_relu1"
  type: "ReLU"
  bottom: "map32_8_conv1"
  top: "map32_8_conv1"
}
layer {
  name: "map32_8_conv2"
  type: "Convolution"
  bottom: "map32_8_conv1"
  top: "map32_8_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_8_bn2"
  type: "BatchNorm"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_8_bn2"
  type: "BatchNorm"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_8_scale2"
  type: "Scale"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_8_relu2"
  type: "ReLU"
  bottom: "map32_8_conv2"
  top: "map32_8_conv2"
}
layer {
  name: "map32_8_conv_end"
  type: "Convolution"
  bottom: "map32_8_conv2"
  top: "map32_8_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_8_eltsum"
  type: "Eltwise"
  bottom: "map32_7_eltsum"
  bottom: "map32_8_conv_end"
  top: "map32_8_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_9_bn_pre"
  type: "BatchNorm"
  bottom: "map32_8_eltsum"
  top: "map32_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_9_bn_pre"
  type: "BatchNorm"
  bottom: "map32_8_eltsum"
  top: "map32_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_9_pre_scale"
  type: "Scale"
  bottom: "map32_9_bn_pre"
  top: "map32_9_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_9_pre_relu"
  type: "ReLU"
  bottom: "map32_9_bn_pre"
  top: "map32_9_bn_pre"
}
layer {
  name: "map32_9_conv1"
  type: "Convolution"
  bottom: "map32_9_bn_pre"
  top: "map32_9_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_9_bn1"
  type: "BatchNorm"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_9_bn1"
  type: "BatchNorm"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_9_scale1"
  type: "Scale"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_9_relu1"
  type: "ReLU"
  bottom: "map32_9_conv1"
  top: "map32_9_conv1"
}
layer {
  name: "map32_9_conv2"
  type: "Convolution"
  bottom: "map32_9_conv1"
  top: "map32_9_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_9_bn2"
  type: "BatchNorm"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_9_bn2"
  type: "BatchNorm"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_9_scale2"
  type: "Scale"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_9_relu2"
  type: "ReLU"
  bottom: "map32_9_conv2"
  top: "map32_9_conv2"
}
layer {
  name: "map32_9_conv_end"
  type: "Convolution"
  bottom: "map32_9_conv2"
  top: "map32_9_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_9_eltsum"
  type: "Eltwise"
  bottom: "map32_8_eltsum"
  bottom: "map32_9_conv_end"
  top: "map32_9_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_10_bn_pre"
  type: "BatchNorm"
  bottom: "map32_9_eltsum"
  top: "map32_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_10_bn_pre"
  type: "BatchNorm"
  bottom: "map32_9_eltsum"
  top: "map32_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_10_pre_scale"
  type: "Scale"
  bottom: "map32_10_bn_pre"
  top: "map32_10_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_10_pre_relu"
  type: "ReLU"
  bottom: "map32_10_bn_pre"
  top: "map32_10_bn_pre"
}
layer {
  name: "map32_10_conv1"
  type: "Convolution"
  bottom: "map32_10_bn_pre"
  top: "map32_10_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_10_bn1"
  type: "BatchNorm"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_10_bn1"
  type: "BatchNorm"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_10_scale1"
  type: "Scale"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_10_relu1"
  type: "ReLU"
  bottom: "map32_10_conv1"
  top: "map32_10_conv1"
}
layer {
  name: "map32_10_conv2"
  type: "Convolution"
  bottom: "map32_10_conv1"
  top: "map32_10_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_10_bn2"
  type: "BatchNorm"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_10_bn2"
  type: "BatchNorm"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_10_scale2"
  type: "Scale"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_10_relu2"
  type: "ReLU"
  bottom: "map32_10_conv2"
  top: "map32_10_conv2"
}
layer {
  name: "map32_10_conv_end"
  type: "Convolution"
  bottom: "map32_10_conv2"
  top: "map32_10_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_10_eltsum"
  type: "Eltwise"
  bottom: "map32_9_eltsum"
  bottom: "map32_10_conv_end"
  top: "map32_10_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_11_bn_pre"
  type: "BatchNorm"
  bottom: "map32_10_eltsum"
  top: "map32_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_11_bn_pre"
  type: "BatchNorm"
  bottom: "map32_10_eltsum"
  top: "map32_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_11_pre_scale"
  type: "Scale"
  bottom: "map32_11_bn_pre"
  top: "map32_11_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_11_pre_relu"
  type: "ReLU"
  bottom: "map32_11_bn_pre"
  top: "map32_11_bn_pre"
}
layer {
  name: "map32_11_conv1"
  type: "Convolution"
  bottom: "map32_11_bn_pre"
  top: "map32_11_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_11_bn1"
  type: "BatchNorm"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_11_bn1"
  type: "BatchNorm"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_11_scale1"
  type: "Scale"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_11_relu1"
  type: "ReLU"
  bottom: "map32_11_conv1"
  top: "map32_11_conv1"
}
layer {
  name: "map32_11_conv2"
  type: "Convolution"
  bottom: "map32_11_conv1"
  top: "map32_11_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_11_bn2"
  type: "BatchNorm"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_11_bn2"
  type: "BatchNorm"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_11_scale2"
  type: "Scale"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_11_relu2"
  type: "ReLU"
  bottom: "map32_11_conv2"
  top: "map32_11_conv2"
}
layer {
  name: "map32_11_conv_end"
  type: "Convolution"
  bottom: "map32_11_conv2"
  top: "map32_11_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_11_eltsum"
  type: "Eltwise"
  bottom: "map32_10_eltsum"
  bottom: "map32_11_conv_end"
  top: "map32_11_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_12_bn_pre"
  type: "BatchNorm"
  bottom: "map32_11_eltsum"
  top: "map32_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_12_bn_pre"
  type: "BatchNorm"
  bottom: "map32_11_eltsum"
  top: "map32_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_12_pre_scale"
  type: "Scale"
  bottom: "map32_12_bn_pre"
  top: "map32_12_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_12_pre_relu"
  type: "ReLU"
  bottom: "map32_12_bn_pre"
  top: "map32_12_bn_pre"
}
layer {
  name: "map32_12_conv1"
  type: "Convolution"
  bottom: "map32_12_bn_pre"
  top: "map32_12_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_12_bn1"
  type: "BatchNorm"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_12_bn1"
  type: "BatchNorm"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_12_scale1"
  type: "Scale"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_12_relu1"
  type: "ReLU"
  bottom: "map32_12_conv1"
  top: "map32_12_conv1"
}
layer {
  name: "map32_12_conv2"
  type: "Convolution"
  bottom: "map32_12_conv1"
  top: "map32_12_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_12_bn2"
  type: "BatchNorm"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_12_bn2"
  type: "BatchNorm"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_12_scale2"
  type: "Scale"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_12_relu2"
  type: "ReLU"
  bottom: "map32_12_conv2"
  top: "map32_12_conv2"
}
layer {
  name: "map32_12_conv_end"
  type: "Convolution"
  bottom: "map32_12_conv2"
  top: "map32_12_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_12_eltsum"
  type: "Eltwise"
  bottom: "map32_11_eltsum"
  bottom: "map32_12_conv_end"
  top: "map32_12_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_13_bn_pre"
  type: "BatchNorm"
  bottom: "map32_12_eltsum"
  top: "map32_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_13_bn_pre"
  type: "BatchNorm"
  bottom: "map32_12_eltsum"
  top: "map32_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_13_pre_scale"
  type: "Scale"
  bottom: "map32_13_bn_pre"
  top: "map32_13_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_13_pre_relu"
  type: "ReLU"
  bottom: "map32_13_bn_pre"
  top: "map32_13_bn_pre"
}
layer {
  name: "map32_13_conv1"
  type: "Convolution"
  bottom: "map32_13_bn_pre"
  top: "map32_13_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_13_bn1"
  type: "BatchNorm"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_13_bn1"
  type: "BatchNorm"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_13_scale1"
  type: "Scale"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_13_relu1"
  type: "ReLU"
  bottom: "map32_13_conv1"
  top: "map32_13_conv1"
}
layer {
  name: "map32_13_conv2"
  type: "Convolution"
  bottom: "map32_13_conv1"
  top: "map32_13_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_13_bn2"
  type: "BatchNorm"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_13_bn2"
  type: "BatchNorm"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_13_scale2"
  type: "Scale"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_13_relu2"
  type: "ReLU"
  bottom: "map32_13_conv2"
  top: "map32_13_conv2"
}
layer {
  name: "map32_13_conv_end"
  type: "Convolution"
  bottom: "map32_13_conv2"
  top: "map32_13_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_13_eltsum"
  type: "Eltwise"
  bottom: "map32_12_eltsum"
  bottom: "map32_13_conv_end"
  top: "map32_13_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_14_bn_pre"
  type: "BatchNorm"
  bottom: "map32_13_eltsum"
  top: "map32_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_14_bn_pre"
  type: "BatchNorm"
  bottom: "map32_13_eltsum"
  top: "map32_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_14_pre_scale"
  type: "Scale"
  bottom: "map32_14_bn_pre"
  top: "map32_14_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_14_pre_relu"
  type: "ReLU"
  bottom: "map32_14_bn_pre"
  top: "map32_14_bn_pre"
}
layer {
  name: "map32_14_conv1"
  type: "Convolution"
  bottom: "map32_14_bn_pre"
  top: "map32_14_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_14_bn1"
  type: "BatchNorm"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_14_bn1"
  type: "BatchNorm"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_14_scale1"
  type: "Scale"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_14_relu1"
  type: "ReLU"
  bottom: "map32_14_conv1"
  top: "map32_14_conv1"
}
layer {
  name: "map32_14_conv2"
  type: "Convolution"
  bottom: "map32_14_conv1"
  top: "map32_14_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_14_bn2"
  type: "BatchNorm"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_14_bn2"
  type: "BatchNorm"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_14_scale2"
  type: "Scale"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_14_relu2"
  type: "ReLU"
  bottom: "map32_14_conv2"
  top: "map32_14_conv2"
}
layer {
  name: "map32_14_conv_end"
  type: "Convolution"
  bottom: "map32_14_conv2"
  top: "map32_14_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_14_eltsum"
  type: "Eltwise"
  bottom: "map32_13_eltsum"
  bottom: "map32_14_conv_end"
  top: "map32_14_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_15_bn_pre"
  type: "BatchNorm"
  bottom: "map32_14_eltsum"
  top: "map32_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_15_bn_pre"
  type: "BatchNorm"
  bottom: "map32_14_eltsum"
  top: "map32_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_15_pre_scale"
  type: "Scale"
  bottom: "map32_15_bn_pre"
  top: "map32_15_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_15_pre_relu"
  type: "ReLU"
  bottom: "map32_15_bn_pre"
  top: "map32_15_bn_pre"
}
layer {
  name: "map32_15_conv1"
  type: "Convolution"
  bottom: "map32_15_bn_pre"
  top: "map32_15_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_15_bn1"
  type: "BatchNorm"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_15_bn1"
  type: "BatchNorm"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_15_scale1"
  type: "Scale"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_15_relu1"
  type: "ReLU"
  bottom: "map32_15_conv1"
  top: "map32_15_conv1"
}
layer {
  name: "map32_15_conv2"
  type: "Convolution"
  bottom: "map32_15_conv1"
  top: "map32_15_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_15_bn2"
  type: "BatchNorm"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_15_bn2"
  type: "BatchNorm"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_15_scale2"
  type: "Scale"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_15_relu2"
  type: "ReLU"
  bottom: "map32_15_conv2"
  top: "map32_15_conv2"
}
layer {
  name: "map32_15_conv_end"
  type: "Convolution"
  bottom: "map32_15_conv2"
  top: "map32_15_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_15_eltsum"
  type: "Eltwise"
  bottom: "map32_14_eltsum"
  bottom: "map32_15_conv_end"
  top: "map32_15_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_16_bn_pre"
  type: "BatchNorm"
  bottom: "map32_15_eltsum"
  top: "map32_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_16_bn_pre"
  type: "BatchNorm"
  bottom: "map32_15_eltsum"
  top: "map32_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_16_pre_scale"
  type: "Scale"
  bottom: "map32_16_bn_pre"
  top: "map32_16_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_16_pre_relu"
  type: "ReLU"
  bottom: "map32_16_bn_pre"
  top: "map32_16_bn_pre"
}
layer {
  name: "map32_16_conv1"
  type: "Convolution"
  bottom: "map32_16_bn_pre"
  top: "map32_16_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_16_bn1"
  type: "BatchNorm"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_16_bn1"
  type: "BatchNorm"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_16_scale1"
  type: "Scale"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_16_relu1"
  type: "ReLU"
  bottom: "map32_16_conv1"
  top: "map32_16_conv1"
}
layer {
  name: "map32_16_conv2"
  type: "Convolution"
  bottom: "map32_16_conv1"
  top: "map32_16_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_16_bn2"
  type: "BatchNorm"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_16_bn2"
  type: "BatchNorm"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_16_scale2"
  type: "Scale"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_16_relu2"
  type: "ReLU"
  bottom: "map32_16_conv2"
  top: "map32_16_conv2"
}
layer {
  name: "map32_16_conv_end"
  type: "Convolution"
  bottom: "map32_16_conv2"
  top: "map32_16_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_16_eltsum"
  type: "Eltwise"
  bottom: "map32_15_eltsum"
  bottom: "map32_16_conv_end"
  top: "map32_16_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_17_bn_pre"
  type: "BatchNorm"
  bottom: "map32_16_eltsum"
  top: "map32_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_17_bn_pre"
  type: "BatchNorm"
  bottom: "map32_16_eltsum"
  top: "map32_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_17_pre_scale"
  type: "Scale"
  bottom: "map32_17_bn_pre"
  top: "map32_17_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_17_pre_relu"
  type: "ReLU"
  bottom: "map32_17_bn_pre"
  top: "map32_17_bn_pre"
}
layer {
  name: "map32_17_conv1"
  type: "Convolution"
  bottom: "map32_17_bn_pre"
  top: "map32_17_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_17_bn1"
  type: "BatchNorm"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_17_bn1"
  type: "BatchNorm"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_17_scale1"
  type: "Scale"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_17_relu1"
  type: "ReLU"
  bottom: "map32_17_conv1"
  top: "map32_17_conv1"
}
layer {
  name: "map32_17_conv2"
  type: "Convolution"
  bottom: "map32_17_conv1"
  top: "map32_17_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_17_bn2"
  type: "BatchNorm"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_17_bn2"
  type: "BatchNorm"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_17_scale2"
  type: "Scale"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_17_relu2"
  type: "ReLU"
  bottom: "map32_17_conv2"
  top: "map32_17_conv2"
}
layer {
  name: "map32_17_conv_end"
  type: "Convolution"
  bottom: "map32_17_conv2"
  top: "map32_17_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_17_eltsum"
  type: "Eltwise"
  bottom: "map32_16_eltsum"
  bottom: "map32_17_conv_end"
  top: "map32_17_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_18_bn_pre"
  type: "BatchNorm"
  bottom: "map32_17_eltsum"
  top: "map32_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_18_bn_pre"
  type: "BatchNorm"
  bottom: "map32_17_eltsum"
  top: "map32_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_18_pre_scale"
  type: "Scale"
  bottom: "map32_18_bn_pre"
  top: "map32_18_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_18_pre_relu"
  type: "ReLU"
  bottom: "map32_18_bn_pre"
  top: "map32_18_bn_pre"
}
layer {
  name: "map32_18_conv1"
  type: "Convolution"
  bottom: "map32_18_bn_pre"
  top: "map32_18_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_18_bn1"
  type: "BatchNorm"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_18_bn1"
  type: "BatchNorm"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_18_scale1"
  type: "Scale"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_18_relu1"
  type: "ReLU"
  bottom: "map32_18_conv1"
  top: "map32_18_conv1"
}
layer {
  name: "map32_18_conv2"
  type: "Convolution"
  bottom: "map32_18_conv1"
  top: "map32_18_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_18_bn2"
  type: "BatchNorm"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_18_bn2"
  type: "BatchNorm"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_18_scale2"
  type: "Scale"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_18_relu2"
  type: "ReLU"
  bottom: "map32_18_conv2"
  top: "map32_18_conv2"
}
layer {
  name: "map32_18_conv_end"
  type: "Convolution"
  bottom: "map32_18_conv2"
  top: "map32_18_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_18_eltsum"
  type: "Eltwise"
  bottom: "map32_17_eltsum"
  bottom: "map32_18_conv_end"
  top: "map32_18_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_19_bn_pre"
  type: "BatchNorm"
  bottom: "map32_18_eltsum"
  top: "map32_19_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_19_bn_pre"
  type: "BatchNorm"
  bottom: "map32_18_eltsum"
  top: "map32_19_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_19_pre_scale"
  type: "Scale"
  bottom: "map32_19_bn_pre"
  top: "map32_19_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_19_pre_relu"
  type: "ReLU"
  bottom: "map32_19_bn_pre"
  top: "map32_19_bn_pre"
}
layer {
  name: "map32_19_conv1"
  type: "Convolution"
  bottom: "map32_19_bn_pre"
  top: "map32_19_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_19_bn1"
  type: "BatchNorm"
  bottom: "map32_19_conv1"
  top: "map32_19_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_19_bn1"
  type: "BatchNorm"
  bottom: "map32_19_conv1"
  top: "map32_19_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_19_scale1"
  type: "Scale"
  bottom: "map32_19_conv1"
  top: "map32_19_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_19_relu1"
  type: "ReLU"
  bottom: "map32_19_conv1"
  top: "map32_19_conv1"
}
layer {
  name: "map32_19_conv2"
  type: "Convolution"
  bottom: "map32_19_conv1"
  top: "map32_19_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_19_bn2"
  type: "BatchNorm"
  bottom: "map32_19_conv2"
  top: "map32_19_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_19_bn2"
  type: "BatchNorm"
  bottom: "map32_19_conv2"
  top: "map32_19_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_19_scale2"
  type: "Scale"
  bottom: "map32_19_conv2"
  top: "map32_19_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_19_relu2"
  type: "ReLU"
  bottom: "map32_19_conv2"
  top: "map32_19_conv2"
}
layer {
  name: "map32_19_conv_end"
  type: "Convolution"
  bottom: "map32_19_conv2"
  top: "map32_19_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_19_eltsum"
  type: "Eltwise"
  bottom: "map32_18_eltsum"
  bottom: "map32_19_conv_end"
  top: "map32_19_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_20_bn_pre"
  type: "BatchNorm"
  bottom: "map32_19_eltsum"
  top: "map32_20_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_20_bn_pre"
  type: "BatchNorm"
  bottom: "map32_19_eltsum"
  top: "map32_20_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_20_pre_scale"
  type: "Scale"
  bottom: "map32_20_bn_pre"
  top: "map32_20_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_20_pre_relu"
  type: "ReLU"
  bottom: "map32_20_bn_pre"
  top: "map32_20_bn_pre"
}
layer {
  name: "map32_20_conv1"
  type: "Convolution"
  bottom: "map32_20_bn_pre"
  top: "map32_20_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_20_bn1"
  type: "BatchNorm"
  bottom: "map32_20_conv1"
  top: "map32_20_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_20_bn1"
  type: "BatchNorm"
  bottom: "map32_20_conv1"
  top: "map32_20_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_20_scale1"
  type: "Scale"
  bottom: "map32_20_conv1"
  top: "map32_20_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_20_relu1"
  type: "ReLU"
  bottom: "map32_20_conv1"
  top: "map32_20_conv1"
}
layer {
  name: "map32_20_conv2"
  type: "Convolution"
  bottom: "map32_20_conv1"
  top: "map32_20_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_20_bn2"
  type: "BatchNorm"
  bottom: "map32_20_conv2"
  top: "map32_20_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_20_bn2"
  type: "BatchNorm"
  bottom: "map32_20_conv2"
  top: "map32_20_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_20_scale2"
  type: "Scale"
  bottom: "map32_20_conv2"
  top: "map32_20_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_20_relu2"
  type: "ReLU"
  bottom: "map32_20_conv2"
  top: "map32_20_conv2"
}
layer {
  name: "map32_20_conv_end"
  type: "Convolution"
  bottom: "map32_20_conv2"
  top: "map32_20_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_20_eltsum"
  type: "Eltwise"
  bottom: "map32_19_eltsum"
  bottom: "map32_20_conv_end"
  top: "map32_20_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_21_bn_pre"
  type: "BatchNorm"
  bottom: "map32_20_eltsum"
  top: "map32_21_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_21_bn_pre"
  type: "BatchNorm"
  bottom: "map32_20_eltsum"
  top: "map32_21_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_21_pre_scale"
  type: "Scale"
  bottom: "map32_21_bn_pre"
  top: "map32_21_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_21_pre_relu"
  type: "ReLU"
  bottom: "map32_21_bn_pre"
  top: "map32_21_bn_pre"
}
layer {
  name: "map32_21_conv1"
  type: "Convolution"
  bottom: "map32_21_bn_pre"
  top: "map32_21_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_21_bn1"
  type: "BatchNorm"
  bottom: "map32_21_conv1"
  top: "map32_21_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_21_bn1"
  type: "BatchNorm"
  bottom: "map32_21_conv1"
  top: "map32_21_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_21_scale1"
  type: "Scale"
  bottom: "map32_21_conv1"
  top: "map32_21_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_21_relu1"
  type: "ReLU"
  bottom: "map32_21_conv1"
  top: "map32_21_conv1"
}
layer {
  name: "map32_21_conv2"
  type: "Convolution"
  bottom: "map32_21_conv1"
  top: "map32_21_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_21_bn2"
  type: "BatchNorm"
  bottom: "map32_21_conv2"
  top: "map32_21_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_21_bn2"
  type: "BatchNorm"
  bottom: "map32_21_conv2"
  top: "map32_21_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_21_scale2"
  type: "Scale"
  bottom: "map32_21_conv2"
  top: "map32_21_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_21_relu2"
  type: "ReLU"
  bottom: "map32_21_conv2"
  top: "map32_21_conv2"
}
layer {
  name: "map32_21_conv_end"
  type: "Convolution"
  bottom: "map32_21_conv2"
  top: "map32_21_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_21_eltsum"
  type: "Eltwise"
  bottom: "map32_20_eltsum"
  bottom: "map32_21_conv_end"
  top: "map32_21_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_22_bn_pre"
  type: "BatchNorm"
  bottom: "map32_21_eltsum"
  top: "map32_22_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_22_bn_pre"
  type: "BatchNorm"
  bottom: "map32_21_eltsum"
  top: "map32_22_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_22_pre_scale"
  type: "Scale"
  bottom: "map32_22_bn_pre"
  top: "map32_22_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_22_pre_relu"
  type: "ReLU"
  bottom: "map32_22_bn_pre"
  top: "map32_22_bn_pre"
}
layer {
  name: "map32_22_conv1"
  type: "Convolution"
  bottom: "map32_22_bn_pre"
  top: "map32_22_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_22_bn1"
  type: "BatchNorm"
  bottom: "map32_22_conv1"
  top: "map32_22_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_22_bn1"
  type: "BatchNorm"
  bottom: "map32_22_conv1"
  top: "map32_22_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_22_scale1"
  type: "Scale"
  bottom: "map32_22_conv1"
  top: "map32_22_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_22_relu1"
  type: "ReLU"
  bottom: "map32_22_conv1"
  top: "map32_22_conv1"
}
layer {
  name: "map32_22_conv2"
  type: "Convolution"
  bottom: "map32_22_conv1"
  top: "map32_22_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_22_bn2"
  type: "BatchNorm"
  bottom: "map32_22_conv2"
  top: "map32_22_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_22_bn2"
  type: "BatchNorm"
  bottom: "map32_22_conv2"
  top: "map32_22_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_22_scale2"
  type: "Scale"
  bottom: "map32_22_conv2"
  top: "map32_22_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_22_relu2"
  type: "ReLU"
  bottom: "map32_22_conv2"
  top: "map32_22_conv2"
}
layer {
  name: "map32_22_conv_end"
  type: "Convolution"
  bottom: "map32_22_conv2"
  top: "map32_22_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_22_eltsum"
  type: "Eltwise"
  bottom: "map32_21_eltsum"
  bottom: "map32_22_conv_end"
  top: "map32_22_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_23_bn_pre"
  type: "BatchNorm"
  bottom: "map32_22_eltsum"
  top: "map32_23_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_23_bn_pre"
  type: "BatchNorm"
  bottom: "map32_22_eltsum"
  top: "map32_23_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_23_pre_scale"
  type: "Scale"
  bottom: "map32_23_bn_pre"
  top: "map32_23_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_23_pre_relu"
  type: "ReLU"
  bottom: "map32_23_bn_pre"
  top: "map32_23_bn_pre"
}
layer {
  name: "map32_23_conv1"
  type: "Convolution"
  bottom: "map32_23_bn_pre"
  top: "map32_23_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_23_bn1"
  type: "BatchNorm"
  bottom: "map32_23_conv1"
  top: "map32_23_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_23_bn1"
  type: "BatchNorm"
  bottom: "map32_23_conv1"
  top: "map32_23_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_23_scale1"
  type: "Scale"
  bottom: "map32_23_conv1"
  top: "map32_23_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_23_relu1"
  type: "ReLU"
  bottom: "map32_23_conv1"
  top: "map32_23_conv1"
}
layer {
  name: "map32_23_conv2"
  type: "Convolution"
  bottom: "map32_23_conv1"
  top: "map32_23_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_23_bn2"
  type: "BatchNorm"
  bottom: "map32_23_conv2"
  top: "map32_23_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_23_bn2"
  type: "BatchNorm"
  bottom: "map32_23_conv2"
  top: "map32_23_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_23_scale2"
  type: "Scale"
  bottom: "map32_23_conv2"
  top: "map32_23_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_23_relu2"
  type: "ReLU"
  bottom: "map32_23_conv2"
  top: "map32_23_conv2"
}
layer {
  name: "map32_23_conv_end"
  type: "Convolution"
  bottom: "map32_23_conv2"
  top: "map32_23_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_23_eltsum"
  type: "Eltwise"
  bottom: "map32_22_eltsum"
  bottom: "map32_23_conv_end"
  top: "map32_23_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_24_bn_pre"
  type: "BatchNorm"
  bottom: "map32_23_eltsum"
  top: "map32_24_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_24_bn_pre"
  type: "BatchNorm"
  bottom: "map32_23_eltsum"
  top: "map32_24_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_24_pre_scale"
  type: "Scale"
  bottom: "map32_24_bn_pre"
  top: "map32_24_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_24_pre_relu"
  type: "ReLU"
  bottom: "map32_24_bn_pre"
  top: "map32_24_bn_pre"
}
layer {
  name: "map32_24_conv1"
  type: "Convolution"
  bottom: "map32_24_bn_pre"
  top: "map32_24_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_24_bn1"
  type: "BatchNorm"
  bottom: "map32_24_conv1"
  top: "map32_24_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_24_bn1"
  type: "BatchNorm"
  bottom: "map32_24_conv1"
  top: "map32_24_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_24_scale1"
  type: "Scale"
  bottom: "map32_24_conv1"
  top: "map32_24_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_24_relu1"
  type: "ReLU"
  bottom: "map32_24_conv1"
  top: "map32_24_conv1"
}
layer {
  name: "map32_24_conv2"
  type: "Convolution"
  bottom: "map32_24_conv1"
  top: "map32_24_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_24_bn2"
  type: "BatchNorm"
  bottom: "map32_24_conv2"
  top: "map32_24_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_24_bn2"
  type: "BatchNorm"
  bottom: "map32_24_conv2"
  top: "map32_24_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_24_scale2"
  type: "Scale"
  bottom: "map32_24_conv2"
  top: "map32_24_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_24_relu2"
  type: "ReLU"
  bottom: "map32_24_conv2"
  top: "map32_24_conv2"
}
layer {
  name: "map32_24_conv_end"
  type: "Convolution"
  bottom: "map32_24_conv2"
  top: "map32_24_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_24_eltsum"
  type: "Eltwise"
  bottom: "map32_23_eltsum"
  bottom: "map32_24_conv_end"
  top: "map32_24_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_25_bn_pre"
  type: "BatchNorm"
  bottom: "map32_24_eltsum"
  top: "map32_25_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_25_bn_pre"
  type: "BatchNorm"
  bottom: "map32_24_eltsum"
  top: "map32_25_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_25_pre_scale"
  type: "Scale"
  bottom: "map32_25_bn_pre"
  top: "map32_25_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_25_pre_relu"
  type: "ReLU"
  bottom: "map32_25_bn_pre"
  top: "map32_25_bn_pre"
}
layer {
  name: "map32_25_conv1"
  type: "Convolution"
  bottom: "map32_25_bn_pre"
  top: "map32_25_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_25_bn1"
  type: "BatchNorm"
  bottom: "map32_25_conv1"
  top: "map32_25_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_25_bn1"
  type: "BatchNorm"
  bottom: "map32_25_conv1"
  top: "map32_25_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_25_scale1"
  type: "Scale"
  bottom: "map32_25_conv1"
  top: "map32_25_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_25_relu1"
  type: "ReLU"
  bottom: "map32_25_conv1"
  top: "map32_25_conv1"
}
layer {
  name: "map32_25_conv2"
  type: "Convolution"
  bottom: "map32_25_conv1"
  top: "map32_25_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_25_bn2"
  type: "BatchNorm"
  bottom: "map32_25_conv2"
  top: "map32_25_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_25_bn2"
  type: "BatchNorm"
  bottom: "map32_25_conv2"
  top: "map32_25_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_25_scale2"
  type: "Scale"
  bottom: "map32_25_conv2"
  top: "map32_25_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_25_relu2"
  type: "ReLU"
  bottom: "map32_25_conv2"
  top: "map32_25_conv2"
}
layer {
  name: "map32_25_conv_end"
  type: "Convolution"
  bottom: "map32_25_conv2"
  top: "map32_25_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_25_eltsum"
  type: "Eltwise"
  bottom: "map32_24_eltsum"
  bottom: "map32_25_conv_end"
  top: "map32_25_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_26_bn_pre"
  type: "BatchNorm"
  bottom: "map32_25_eltsum"
  top: "map32_26_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_26_bn_pre"
  type: "BatchNorm"
  bottom: "map32_25_eltsum"
  top: "map32_26_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_26_pre_scale"
  type: "Scale"
  bottom: "map32_26_bn_pre"
  top: "map32_26_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_26_pre_relu"
  type: "ReLU"
  bottom: "map32_26_bn_pre"
  top: "map32_26_bn_pre"
}
layer {
  name: "map32_26_conv1"
  type: "Convolution"
  bottom: "map32_26_bn_pre"
  top: "map32_26_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_26_bn1"
  type: "BatchNorm"
  bottom: "map32_26_conv1"
  top: "map32_26_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_26_bn1"
  type: "BatchNorm"
  bottom: "map32_26_conv1"
  top: "map32_26_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_26_scale1"
  type: "Scale"
  bottom: "map32_26_conv1"
  top: "map32_26_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_26_relu1"
  type: "ReLU"
  bottom: "map32_26_conv1"
  top: "map32_26_conv1"
}
layer {
  name: "map32_26_conv2"
  type: "Convolution"
  bottom: "map32_26_conv1"
  top: "map32_26_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_26_bn2"
  type: "BatchNorm"
  bottom: "map32_26_conv2"
  top: "map32_26_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_26_bn2"
  type: "BatchNorm"
  bottom: "map32_26_conv2"
  top: "map32_26_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_26_scale2"
  type: "Scale"
  bottom: "map32_26_conv2"
  top: "map32_26_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_26_relu2"
  type: "ReLU"
  bottom: "map32_26_conv2"
  top: "map32_26_conv2"
}
layer {
  name: "map32_26_conv_end"
  type: "Convolution"
  bottom: "map32_26_conv2"
  top: "map32_26_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_26_eltsum"
  type: "Eltwise"
  bottom: "map32_25_eltsum"
  bottom: "map32_26_conv_end"
  top: "map32_26_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_27_bn_pre"
  type: "BatchNorm"
  bottom: "map32_26_eltsum"
  top: "map32_27_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_27_bn_pre"
  type: "BatchNorm"
  bottom: "map32_26_eltsum"
  top: "map32_27_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_27_pre_scale"
  type: "Scale"
  bottom: "map32_27_bn_pre"
  top: "map32_27_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_27_pre_relu"
  type: "ReLU"
  bottom: "map32_27_bn_pre"
  top: "map32_27_bn_pre"
}
layer {
  name: "map32_27_conv1"
  type: "Convolution"
  bottom: "map32_27_bn_pre"
  top: "map32_27_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_27_bn1"
  type: "BatchNorm"
  bottom: "map32_27_conv1"
  top: "map32_27_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_27_bn1"
  type: "BatchNorm"
  bottom: "map32_27_conv1"
  top: "map32_27_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_27_scale1"
  type: "Scale"
  bottom: "map32_27_conv1"
  top: "map32_27_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_27_relu1"
  type: "ReLU"
  bottom: "map32_27_conv1"
  top: "map32_27_conv1"
}
layer {
  name: "map32_27_conv2"
  type: "Convolution"
  bottom: "map32_27_conv1"
  top: "map32_27_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_27_bn2"
  type: "BatchNorm"
  bottom: "map32_27_conv2"
  top: "map32_27_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_27_bn2"
  type: "BatchNorm"
  bottom: "map32_27_conv2"
  top: "map32_27_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_27_scale2"
  type: "Scale"
  bottom: "map32_27_conv2"
  top: "map32_27_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_27_relu2"
  type: "ReLU"
  bottom: "map32_27_conv2"
  top: "map32_27_conv2"
}
layer {
  name: "map32_27_conv_end"
  type: "Convolution"
  bottom: "map32_27_conv2"
  top: "map32_27_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_27_eltsum"
  type: "Eltwise"
  bottom: "map32_26_eltsum"
  bottom: "map32_27_conv_end"
  top: "map32_27_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_28_bn_pre"
  type: "BatchNorm"
  bottom: "map32_27_eltsum"
  top: "map32_28_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_28_bn_pre"
  type: "BatchNorm"
  bottom: "map32_27_eltsum"
  top: "map32_28_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_28_pre_scale"
  type: "Scale"
  bottom: "map32_28_bn_pre"
  top: "map32_28_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_28_pre_relu"
  type: "ReLU"
  bottom: "map32_28_bn_pre"
  top: "map32_28_bn_pre"
}
layer {
  name: "map32_28_conv1"
  type: "Convolution"
  bottom: "map32_28_bn_pre"
  top: "map32_28_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_28_bn1"
  type: "BatchNorm"
  bottom: "map32_28_conv1"
  top: "map32_28_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_28_bn1"
  type: "BatchNorm"
  bottom: "map32_28_conv1"
  top: "map32_28_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_28_scale1"
  type: "Scale"
  bottom: "map32_28_conv1"
  top: "map32_28_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_28_relu1"
  type: "ReLU"
  bottom: "map32_28_conv1"
  top: "map32_28_conv1"
}
layer {
  name: "map32_28_conv2"
  type: "Convolution"
  bottom: "map32_28_conv1"
  top: "map32_28_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_28_bn2"
  type: "BatchNorm"
  bottom: "map32_28_conv2"
  top: "map32_28_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_28_bn2"
  type: "BatchNorm"
  bottom: "map32_28_conv2"
  top: "map32_28_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_28_scale2"
  type: "Scale"
  bottom: "map32_28_conv2"
  top: "map32_28_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_28_relu2"
  type: "ReLU"
  bottom: "map32_28_conv2"
  top: "map32_28_conv2"
}
layer {
  name: "map32_28_conv_end"
  type: "Convolution"
  bottom: "map32_28_conv2"
  top: "map32_28_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_28_eltsum"
  type: "Eltwise"
  bottom: "map32_27_eltsum"
  bottom: "map32_28_conv_end"
  top: "map32_28_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_29_bn_pre"
  type: "BatchNorm"
  bottom: "map32_28_eltsum"
  top: "map32_29_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_29_bn_pre"
  type: "BatchNorm"
  bottom: "map32_28_eltsum"
  top: "map32_29_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_29_pre_scale"
  type: "Scale"
  bottom: "map32_29_bn_pre"
  top: "map32_29_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_29_pre_relu"
  type: "ReLU"
  bottom: "map32_29_bn_pre"
  top: "map32_29_bn_pre"
}
layer {
  name: "map32_29_conv1"
  type: "Convolution"
  bottom: "map32_29_bn_pre"
  top: "map32_29_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_29_bn1"
  type: "BatchNorm"
  bottom: "map32_29_conv1"
  top: "map32_29_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_29_bn1"
  type: "BatchNorm"
  bottom: "map32_29_conv1"
  top: "map32_29_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_29_scale1"
  type: "Scale"
  bottom: "map32_29_conv1"
  top: "map32_29_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_29_relu1"
  type: "ReLU"
  bottom: "map32_29_conv1"
  top: "map32_29_conv1"
}
layer {
  name: "map32_29_conv2"
  type: "Convolution"
  bottom: "map32_29_conv1"
  top: "map32_29_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_29_bn2"
  type: "BatchNorm"
  bottom: "map32_29_conv2"
  top: "map32_29_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_29_bn2"
  type: "BatchNorm"
  bottom: "map32_29_conv2"
  top: "map32_29_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_29_scale2"
  type: "Scale"
  bottom: "map32_29_conv2"
  top: "map32_29_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_29_relu2"
  type: "ReLU"
  bottom: "map32_29_conv2"
  top: "map32_29_conv2"
}
layer {
  name: "map32_29_conv_end"
  type: "Convolution"
  bottom: "map32_29_conv2"
  top: "map32_29_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_29_eltsum"
  type: "Eltwise"
  bottom: "map32_28_eltsum"
  bottom: "map32_29_conv_end"
  top: "map32_29_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_30_bn_pre"
  type: "BatchNorm"
  bottom: "map32_29_eltsum"
  top: "map32_30_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_30_bn_pre"
  type: "BatchNorm"
  bottom: "map32_29_eltsum"
  top: "map32_30_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_30_pre_scale"
  type: "Scale"
  bottom: "map32_30_bn_pre"
  top: "map32_30_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_30_pre_relu"
  type: "ReLU"
  bottom: "map32_30_bn_pre"
  top: "map32_30_bn_pre"
}
layer {
  name: "map32_30_conv1"
  type: "Convolution"
  bottom: "map32_30_bn_pre"
  top: "map32_30_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_30_bn1"
  type: "BatchNorm"
  bottom: "map32_30_conv1"
  top: "map32_30_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_30_bn1"
  type: "BatchNorm"
  bottom: "map32_30_conv1"
  top: "map32_30_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_30_scale1"
  type: "Scale"
  bottom: "map32_30_conv1"
  top: "map32_30_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_30_relu1"
  type: "ReLU"
  bottom: "map32_30_conv1"
  top: "map32_30_conv1"
}
layer {
  name: "map32_30_conv2"
  type: "Convolution"
  bottom: "map32_30_conv1"
  top: "map32_30_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_30_bn2"
  type: "BatchNorm"
  bottom: "map32_30_conv2"
  top: "map32_30_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_30_bn2"
  type: "BatchNorm"
  bottom: "map32_30_conv2"
  top: "map32_30_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_30_scale2"
  type: "Scale"
  bottom: "map32_30_conv2"
  top: "map32_30_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_30_relu2"
  type: "ReLU"
  bottom: "map32_30_conv2"
  top: "map32_30_conv2"
}
layer {
  name: "map32_30_conv_end"
  type: "Convolution"
  bottom: "map32_30_conv2"
  top: "map32_30_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_30_eltsum"
  type: "Eltwise"
  bottom: "map32_29_eltsum"
  bottom: "map32_30_conv_end"
  top: "map32_30_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_31_bn_pre"
  type: "BatchNorm"
  bottom: "map32_30_eltsum"
  top: "map32_31_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_31_bn_pre"
  type: "BatchNorm"
  bottom: "map32_30_eltsum"
  top: "map32_31_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_31_pre_scale"
  type: "Scale"
  bottom: "map32_31_bn_pre"
  top: "map32_31_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_31_pre_relu"
  type: "ReLU"
  bottom: "map32_31_bn_pre"
  top: "map32_31_bn_pre"
}
layer {
  name: "map32_31_conv1"
  type: "Convolution"
  bottom: "map32_31_bn_pre"
  top: "map32_31_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_31_bn1"
  type: "BatchNorm"
  bottom: "map32_31_conv1"
  top: "map32_31_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_31_bn1"
  type: "BatchNorm"
  bottom: "map32_31_conv1"
  top: "map32_31_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_31_scale1"
  type: "Scale"
  bottom: "map32_31_conv1"
  top: "map32_31_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_31_relu1"
  type: "ReLU"
  bottom: "map32_31_conv1"
  top: "map32_31_conv1"
}
layer {
  name: "map32_31_conv2"
  type: "Convolution"
  bottom: "map32_31_conv1"
  top: "map32_31_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_31_bn2"
  type: "BatchNorm"
  bottom: "map32_31_conv2"
  top: "map32_31_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_31_bn2"
  type: "BatchNorm"
  bottom: "map32_31_conv2"
  top: "map32_31_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_31_scale2"
  type: "Scale"
  bottom: "map32_31_conv2"
  top: "map32_31_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_31_relu2"
  type: "ReLU"
  bottom: "map32_31_conv2"
  top: "map32_31_conv2"
}
layer {
  name: "map32_31_conv_end"
  type: "Convolution"
  bottom: "map32_31_conv2"
  top: "map32_31_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_31_eltsum"
  type: "Eltwise"
  bottom: "map32_30_eltsum"
  bottom: "map32_31_conv_end"
  top: "map32_31_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_32_bn_pre"
  type: "BatchNorm"
  bottom: "map32_31_eltsum"
  top: "map32_32_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_32_bn_pre"
  type: "BatchNorm"
  bottom: "map32_31_eltsum"
  top: "map32_32_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_32_pre_scale"
  type: "Scale"
  bottom: "map32_32_bn_pre"
  top: "map32_32_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_32_pre_relu"
  type: "ReLU"
  bottom: "map32_32_bn_pre"
  top: "map32_32_bn_pre"
}
layer {
  name: "map32_32_conv1"
  type: "Convolution"
  bottom: "map32_32_bn_pre"
  top: "map32_32_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_32_bn1"
  type: "BatchNorm"
  bottom: "map32_32_conv1"
  top: "map32_32_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_32_bn1"
  type: "BatchNorm"
  bottom: "map32_32_conv1"
  top: "map32_32_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_32_scale1"
  type: "Scale"
  bottom: "map32_32_conv1"
  top: "map32_32_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_32_relu1"
  type: "ReLU"
  bottom: "map32_32_conv1"
  top: "map32_32_conv1"
}
layer {
  name: "map32_32_conv2"
  type: "Convolution"
  bottom: "map32_32_conv1"
  top: "map32_32_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_32_bn2"
  type: "BatchNorm"
  bottom: "map32_32_conv2"
  top: "map32_32_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_32_bn2"
  type: "BatchNorm"
  bottom: "map32_32_conv2"
  top: "map32_32_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_32_scale2"
  type: "Scale"
  bottom: "map32_32_conv2"
  top: "map32_32_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_32_relu2"
  type: "ReLU"
  bottom: "map32_32_conv2"
  top: "map32_32_conv2"
}
layer {
  name: "map32_32_conv_end"
  type: "Convolution"
  bottom: "map32_32_conv2"
  top: "map32_32_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_32_eltsum"
  type: "Eltwise"
  bottom: "map32_31_eltsum"
  bottom: "map32_32_conv_end"
  top: "map32_32_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_33_bn_pre"
  type: "BatchNorm"
  bottom: "map32_32_eltsum"
  top: "map32_33_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_33_bn_pre"
  type: "BatchNorm"
  bottom: "map32_32_eltsum"
  top: "map32_33_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_33_pre_scale"
  type: "Scale"
  bottom: "map32_33_bn_pre"
  top: "map32_33_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_33_pre_relu"
  type: "ReLU"
  bottom: "map32_33_bn_pre"
  top: "map32_33_bn_pre"
}
layer {
  name: "map32_33_conv1"
  type: "Convolution"
  bottom: "map32_33_bn_pre"
  top: "map32_33_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_33_bn1"
  type: "BatchNorm"
  bottom: "map32_33_conv1"
  top: "map32_33_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_33_bn1"
  type: "BatchNorm"
  bottom: "map32_33_conv1"
  top: "map32_33_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_33_scale1"
  type: "Scale"
  bottom: "map32_33_conv1"
  top: "map32_33_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_33_relu1"
  type: "ReLU"
  bottom: "map32_33_conv1"
  top: "map32_33_conv1"
}
layer {
  name: "map32_33_conv2"
  type: "Convolution"
  bottom: "map32_33_conv1"
  top: "map32_33_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_33_bn2"
  type: "BatchNorm"
  bottom: "map32_33_conv2"
  top: "map32_33_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_33_bn2"
  type: "BatchNorm"
  bottom: "map32_33_conv2"
  top: "map32_33_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_33_scale2"
  type: "Scale"
  bottom: "map32_33_conv2"
  top: "map32_33_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_33_relu2"
  type: "ReLU"
  bottom: "map32_33_conv2"
  top: "map32_33_conv2"
}
layer {
  name: "map32_33_conv_end"
  type: "Convolution"
  bottom: "map32_33_conv2"
  top: "map32_33_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_33_eltsum"
  type: "Eltwise"
  bottom: "map32_32_eltsum"
  bottom: "map32_33_conv_end"
  top: "map32_33_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_34_bn_pre"
  type: "BatchNorm"
  bottom: "map32_33_eltsum"
  top: "map32_34_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_34_bn_pre"
  type: "BatchNorm"
  bottom: "map32_33_eltsum"
  top: "map32_34_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_34_pre_scale"
  type: "Scale"
  bottom: "map32_34_bn_pre"
  top: "map32_34_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_34_pre_relu"
  type: "ReLU"
  bottom: "map32_34_bn_pre"
  top: "map32_34_bn_pre"
}
layer {
  name: "map32_34_conv1"
  type: "Convolution"
  bottom: "map32_34_bn_pre"
  top: "map32_34_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_34_bn1"
  type: "BatchNorm"
  bottom: "map32_34_conv1"
  top: "map32_34_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_34_bn1"
  type: "BatchNorm"
  bottom: "map32_34_conv1"
  top: "map32_34_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_34_scale1"
  type: "Scale"
  bottom: "map32_34_conv1"
  top: "map32_34_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_34_relu1"
  type: "ReLU"
  bottom: "map32_34_conv1"
  top: "map32_34_conv1"
}
layer {
  name: "map32_34_conv2"
  type: "Convolution"
  bottom: "map32_34_conv1"
  top: "map32_34_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_34_bn2"
  type: "BatchNorm"
  bottom: "map32_34_conv2"
  top: "map32_34_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_34_bn2"
  type: "BatchNorm"
  bottom: "map32_34_conv2"
  top: "map32_34_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_34_scale2"
  type: "Scale"
  bottom: "map32_34_conv2"
  top: "map32_34_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_34_relu2"
  type: "ReLU"
  bottom: "map32_34_conv2"
  top: "map32_34_conv2"
}
layer {
  name: "map32_34_conv_end"
  type: "Convolution"
  bottom: "map32_34_conv2"
  top: "map32_34_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_34_eltsum"
  type: "Eltwise"
  bottom: "map32_33_eltsum"
  bottom: "map32_34_conv_end"
  top: "map32_34_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_35_bn_pre"
  type: "BatchNorm"
  bottom: "map32_34_eltsum"
  top: "map32_35_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_35_bn_pre"
  type: "BatchNorm"
  bottom: "map32_34_eltsum"
  top: "map32_35_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_35_pre_scale"
  type: "Scale"
  bottom: "map32_35_bn_pre"
  top: "map32_35_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_35_pre_relu"
  type: "ReLU"
  bottom: "map32_35_bn_pre"
  top: "map32_35_bn_pre"
}
layer {
  name: "map32_35_conv1"
  type: "Convolution"
  bottom: "map32_35_bn_pre"
  top: "map32_35_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_35_bn1"
  type: "BatchNorm"
  bottom: "map32_35_conv1"
  top: "map32_35_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_35_bn1"
  type: "BatchNorm"
  bottom: "map32_35_conv1"
  top: "map32_35_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_35_scale1"
  type: "Scale"
  bottom: "map32_35_conv1"
  top: "map32_35_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_35_relu1"
  type: "ReLU"
  bottom: "map32_35_conv1"
  top: "map32_35_conv1"
}
layer {
  name: "map32_35_conv2"
  type: "Convolution"
  bottom: "map32_35_conv1"
  top: "map32_35_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_35_bn2"
  type: "BatchNorm"
  bottom: "map32_35_conv2"
  top: "map32_35_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_35_bn2"
  type: "BatchNorm"
  bottom: "map32_35_conv2"
  top: "map32_35_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_35_scale2"
  type: "Scale"
  bottom: "map32_35_conv2"
  top: "map32_35_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_35_relu2"
  type: "ReLU"
  bottom: "map32_35_conv2"
  top: "map32_35_conv2"
}
layer {
  name: "map32_35_conv_end"
  type: "Convolution"
  bottom: "map32_35_conv2"
  top: "map32_35_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_35_eltsum"
  type: "Eltwise"
  bottom: "map32_34_eltsum"
  bottom: "map32_35_conv_end"
  top: "map32_35_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_36_bn_pre"
  type: "BatchNorm"
  bottom: "map32_35_eltsum"
  top: "map32_36_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_36_bn_pre"
  type: "BatchNorm"
  bottom: "map32_35_eltsum"
  top: "map32_36_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_36_pre_scale"
  type: "Scale"
  bottom: "map32_36_bn_pre"
  top: "map32_36_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_36_pre_relu"
  type: "ReLU"
  bottom: "map32_36_bn_pre"
  top: "map32_36_bn_pre"
}
layer {
  name: "map32_36_conv1"
  type: "Convolution"
  bottom: "map32_36_bn_pre"
  top: "map32_36_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_36_bn1"
  type: "BatchNorm"
  bottom: "map32_36_conv1"
  top: "map32_36_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_36_bn1"
  type: "BatchNorm"
  bottom: "map32_36_conv1"
  top: "map32_36_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_36_scale1"
  type: "Scale"
  bottom: "map32_36_conv1"
  top: "map32_36_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_36_relu1"
  type: "ReLU"
  bottom: "map32_36_conv1"
  top: "map32_36_conv1"
}
layer {
  name: "map32_36_conv2"
  type: "Convolution"
  bottom: "map32_36_conv1"
  top: "map32_36_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_36_bn2"
  type: "BatchNorm"
  bottom: "map32_36_conv2"
  top: "map32_36_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_36_bn2"
  type: "BatchNorm"
  bottom: "map32_36_conv2"
  top: "map32_36_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_36_scale2"
  type: "Scale"
  bottom: "map32_36_conv2"
  top: "map32_36_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_36_relu2"
  type: "ReLU"
  bottom: "map32_36_conv2"
  top: "map32_36_conv2"
}
layer {
  name: "map32_36_conv_end"
  type: "Convolution"
  bottom: "map32_36_conv2"
  top: "map32_36_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_36_eltsum"
  type: "Eltwise"
  bottom: "map32_35_eltsum"
  bottom: "map32_36_conv_end"
  top: "map32_36_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_37_bn_pre"
  type: "BatchNorm"
  bottom: "map32_36_eltsum"
  top: "map32_37_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_37_bn_pre"
  type: "BatchNorm"
  bottom: "map32_36_eltsum"
  top: "map32_37_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_37_pre_scale"
  type: "Scale"
  bottom: "map32_37_bn_pre"
  top: "map32_37_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_37_pre_relu"
  type: "ReLU"
  bottom: "map32_37_bn_pre"
  top: "map32_37_bn_pre"
}
layer {
  name: "map32_37_conv1"
  type: "Convolution"
  bottom: "map32_37_bn_pre"
  top: "map32_37_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_37_bn1"
  type: "BatchNorm"
  bottom: "map32_37_conv1"
  top: "map32_37_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_37_bn1"
  type: "BatchNorm"
  bottom: "map32_37_conv1"
  top: "map32_37_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_37_scale1"
  type: "Scale"
  bottom: "map32_37_conv1"
  top: "map32_37_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_37_relu1"
  type: "ReLU"
  bottom: "map32_37_conv1"
  top: "map32_37_conv1"
}
layer {
  name: "map32_37_conv2"
  type: "Convolution"
  bottom: "map32_37_conv1"
  top: "map32_37_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_37_bn2"
  type: "BatchNorm"
  bottom: "map32_37_conv2"
  top: "map32_37_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_37_bn2"
  type: "BatchNorm"
  bottom: "map32_37_conv2"
  top: "map32_37_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_37_scale2"
  type: "Scale"
  bottom: "map32_37_conv2"
  top: "map32_37_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_37_relu2"
  type: "ReLU"
  bottom: "map32_37_conv2"
  top: "map32_37_conv2"
}
layer {
  name: "map32_37_conv_end"
  type: "Convolution"
  bottom: "map32_37_conv2"
  top: "map32_37_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_37_eltsum"
  type: "Eltwise"
  bottom: "map32_36_eltsum"
  bottom: "map32_37_conv_end"
  top: "map32_37_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_38_bn_pre"
  type: "BatchNorm"
  bottom: "map32_37_eltsum"
  top: "map32_38_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_38_bn_pre"
  type: "BatchNorm"
  bottom: "map32_37_eltsum"
  top: "map32_38_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_38_pre_scale"
  type: "Scale"
  bottom: "map32_38_bn_pre"
  top: "map32_38_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_38_pre_relu"
  type: "ReLU"
  bottom: "map32_38_bn_pre"
  top: "map32_38_bn_pre"
}
layer {
  name: "map32_38_conv1"
  type: "Convolution"
  bottom: "map32_38_bn_pre"
  top: "map32_38_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_38_bn1"
  type: "BatchNorm"
  bottom: "map32_38_conv1"
  top: "map32_38_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_38_bn1"
  type: "BatchNorm"
  bottom: "map32_38_conv1"
  top: "map32_38_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_38_scale1"
  type: "Scale"
  bottom: "map32_38_conv1"
  top: "map32_38_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_38_relu1"
  type: "ReLU"
  bottom: "map32_38_conv1"
  top: "map32_38_conv1"
}
layer {
  name: "map32_38_conv2"
  type: "Convolution"
  bottom: "map32_38_conv1"
  top: "map32_38_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_38_bn2"
  type: "BatchNorm"
  bottom: "map32_38_conv2"
  top: "map32_38_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_38_bn2"
  type: "BatchNorm"
  bottom: "map32_38_conv2"
  top: "map32_38_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_38_scale2"
  type: "Scale"
  bottom: "map32_38_conv2"
  top: "map32_38_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_38_relu2"
  type: "ReLU"
  bottom: "map32_38_conv2"
  top: "map32_38_conv2"
}
layer {
  name: "map32_38_conv_end"
  type: "Convolution"
  bottom: "map32_38_conv2"
  top: "map32_38_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_38_eltsum"
  type: "Eltwise"
  bottom: "map32_37_eltsum"
  bottom: "map32_38_conv_end"
  top: "map32_38_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_39_bn_pre"
  type: "BatchNorm"
  bottom: "map32_38_eltsum"
  top: "map32_39_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_39_bn_pre"
  type: "BatchNorm"
  bottom: "map32_38_eltsum"
  top: "map32_39_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_39_pre_scale"
  type: "Scale"
  bottom: "map32_39_bn_pre"
  top: "map32_39_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_39_pre_relu"
  type: "ReLU"
  bottom: "map32_39_bn_pre"
  top: "map32_39_bn_pre"
}
layer {
  name: "map32_39_conv1"
  type: "Convolution"
  bottom: "map32_39_bn_pre"
  top: "map32_39_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_39_bn1"
  type: "BatchNorm"
  bottom: "map32_39_conv1"
  top: "map32_39_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_39_bn1"
  type: "BatchNorm"
  bottom: "map32_39_conv1"
  top: "map32_39_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_39_scale1"
  type: "Scale"
  bottom: "map32_39_conv1"
  top: "map32_39_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_39_relu1"
  type: "ReLU"
  bottom: "map32_39_conv1"
  top: "map32_39_conv1"
}
layer {
  name: "map32_39_conv2"
  type: "Convolution"
  bottom: "map32_39_conv1"
  top: "map32_39_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_39_bn2"
  type: "BatchNorm"
  bottom: "map32_39_conv2"
  top: "map32_39_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_39_bn2"
  type: "BatchNorm"
  bottom: "map32_39_conv2"
  top: "map32_39_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_39_scale2"
  type: "Scale"
  bottom: "map32_39_conv2"
  top: "map32_39_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_39_relu2"
  type: "ReLU"
  bottom: "map32_39_conv2"
  top: "map32_39_conv2"
}
layer {
  name: "map32_39_conv_end"
  type: "Convolution"
  bottom: "map32_39_conv2"
  top: "map32_39_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_39_eltsum"
  type: "Eltwise"
  bottom: "map32_38_eltsum"
  bottom: "map32_39_conv_end"
  top: "map32_39_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_40_bn_pre"
  type: "BatchNorm"
  bottom: "map32_39_eltsum"
  top: "map32_40_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_40_bn_pre"
  type: "BatchNorm"
  bottom: "map32_39_eltsum"
  top: "map32_40_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_40_pre_scale"
  type: "Scale"
  bottom: "map32_40_bn_pre"
  top: "map32_40_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_40_pre_relu"
  type: "ReLU"
  bottom: "map32_40_bn_pre"
  top: "map32_40_bn_pre"
}
layer {
  name: "map32_40_conv1"
  type: "Convolution"
  bottom: "map32_40_bn_pre"
  top: "map32_40_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_40_bn1"
  type: "BatchNorm"
  bottom: "map32_40_conv1"
  top: "map32_40_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_40_bn1"
  type: "BatchNorm"
  bottom: "map32_40_conv1"
  top: "map32_40_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_40_scale1"
  type: "Scale"
  bottom: "map32_40_conv1"
  top: "map32_40_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_40_relu1"
  type: "ReLU"
  bottom: "map32_40_conv1"
  top: "map32_40_conv1"
}
layer {
  name: "map32_40_conv2"
  type: "Convolution"
  bottom: "map32_40_conv1"
  top: "map32_40_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_40_bn2"
  type: "BatchNorm"
  bottom: "map32_40_conv2"
  top: "map32_40_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_40_bn2"
  type: "BatchNorm"
  bottom: "map32_40_conv2"
  top: "map32_40_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_40_scale2"
  type: "Scale"
  bottom: "map32_40_conv2"
  top: "map32_40_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_40_relu2"
  type: "ReLU"
  bottom: "map32_40_conv2"
  top: "map32_40_conv2"
}
layer {
  name: "map32_40_conv_end"
  type: "Convolution"
  bottom: "map32_40_conv2"
  top: "map32_40_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_40_eltsum"
  type: "Eltwise"
  bottom: "map32_39_eltsum"
  bottom: "map32_40_conv_end"
  top: "map32_40_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_41_bn_pre"
  type: "BatchNorm"
  bottom: "map32_40_eltsum"
  top: "map32_41_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_41_bn_pre"
  type: "BatchNorm"
  bottom: "map32_40_eltsum"
  top: "map32_41_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_41_pre_scale"
  type: "Scale"
  bottom: "map32_41_bn_pre"
  top: "map32_41_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_41_pre_relu"
  type: "ReLU"
  bottom: "map32_41_bn_pre"
  top: "map32_41_bn_pre"
}
layer {
  name: "map32_41_conv1"
  type: "Convolution"
  bottom: "map32_41_bn_pre"
  top: "map32_41_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_41_bn1"
  type: "BatchNorm"
  bottom: "map32_41_conv1"
  top: "map32_41_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_41_bn1"
  type: "BatchNorm"
  bottom: "map32_41_conv1"
  top: "map32_41_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_41_scale1"
  type: "Scale"
  bottom: "map32_41_conv1"
  top: "map32_41_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_41_relu1"
  type: "ReLU"
  bottom: "map32_41_conv1"
  top: "map32_41_conv1"
}
layer {
  name: "map32_41_conv2"
  type: "Convolution"
  bottom: "map32_41_conv1"
  top: "map32_41_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_41_bn2"
  type: "BatchNorm"
  bottom: "map32_41_conv2"
  top: "map32_41_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_41_bn2"
  type: "BatchNorm"
  bottom: "map32_41_conv2"
  top: "map32_41_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_41_scale2"
  type: "Scale"
  bottom: "map32_41_conv2"
  top: "map32_41_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_41_relu2"
  type: "ReLU"
  bottom: "map32_41_conv2"
  top: "map32_41_conv2"
}
layer {
  name: "map32_41_conv_end"
  type: "Convolution"
  bottom: "map32_41_conv2"
  top: "map32_41_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_41_eltsum"
  type: "Eltwise"
  bottom: "map32_40_eltsum"
  bottom: "map32_41_conv_end"
  top: "map32_41_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_42_bn_pre"
  type: "BatchNorm"
  bottom: "map32_41_eltsum"
  top: "map32_42_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_42_bn_pre"
  type: "BatchNorm"
  bottom: "map32_41_eltsum"
  top: "map32_42_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_42_pre_scale"
  type: "Scale"
  bottom: "map32_42_bn_pre"
  top: "map32_42_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_42_pre_relu"
  type: "ReLU"
  bottom: "map32_42_bn_pre"
  top: "map32_42_bn_pre"
}
layer {
  name: "map32_42_conv1"
  type: "Convolution"
  bottom: "map32_42_bn_pre"
  top: "map32_42_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_42_bn1"
  type: "BatchNorm"
  bottom: "map32_42_conv1"
  top: "map32_42_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_42_bn1"
  type: "BatchNorm"
  bottom: "map32_42_conv1"
  top: "map32_42_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_42_scale1"
  type: "Scale"
  bottom: "map32_42_conv1"
  top: "map32_42_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_42_relu1"
  type: "ReLU"
  bottom: "map32_42_conv1"
  top: "map32_42_conv1"
}
layer {
  name: "map32_42_conv2"
  type: "Convolution"
  bottom: "map32_42_conv1"
  top: "map32_42_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_42_bn2"
  type: "BatchNorm"
  bottom: "map32_42_conv2"
  top: "map32_42_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_42_bn2"
  type: "BatchNorm"
  bottom: "map32_42_conv2"
  top: "map32_42_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_42_scale2"
  type: "Scale"
  bottom: "map32_42_conv2"
  top: "map32_42_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_42_relu2"
  type: "ReLU"
  bottom: "map32_42_conv2"
  top: "map32_42_conv2"
}
layer {
  name: "map32_42_conv_end"
  type: "Convolution"
  bottom: "map32_42_conv2"
  top: "map32_42_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_42_eltsum"
  type: "Eltwise"
  bottom: "map32_41_eltsum"
  bottom: "map32_42_conv_end"
  top: "map32_42_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_43_bn_pre"
  type: "BatchNorm"
  bottom: "map32_42_eltsum"
  top: "map32_43_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_43_bn_pre"
  type: "BatchNorm"
  bottom: "map32_42_eltsum"
  top: "map32_43_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_43_pre_scale"
  type: "Scale"
  bottom: "map32_43_bn_pre"
  top: "map32_43_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_43_pre_relu"
  type: "ReLU"
  bottom: "map32_43_bn_pre"
  top: "map32_43_bn_pre"
}
layer {
  name: "map32_43_conv1"
  type: "Convolution"
  bottom: "map32_43_bn_pre"
  top: "map32_43_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_43_bn1"
  type: "BatchNorm"
  bottom: "map32_43_conv1"
  top: "map32_43_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_43_bn1"
  type: "BatchNorm"
  bottom: "map32_43_conv1"
  top: "map32_43_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_43_scale1"
  type: "Scale"
  bottom: "map32_43_conv1"
  top: "map32_43_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_43_relu1"
  type: "ReLU"
  bottom: "map32_43_conv1"
  top: "map32_43_conv1"
}
layer {
  name: "map32_43_conv2"
  type: "Convolution"
  bottom: "map32_43_conv1"
  top: "map32_43_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_43_bn2"
  type: "BatchNorm"
  bottom: "map32_43_conv2"
  top: "map32_43_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_43_bn2"
  type: "BatchNorm"
  bottom: "map32_43_conv2"
  top: "map32_43_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_43_scale2"
  type: "Scale"
  bottom: "map32_43_conv2"
  top: "map32_43_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_43_relu2"
  type: "ReLU"
  bottom: "map32_43_conv2"
  top: "map32_43_conv2"
}
layer {
  name: "map32_43_conv_end"
  type: "Convolution"
  bottom: "map32_43_conv2"
  top: "map32_43_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_43_eltsum"
  type: "Eltwise"
  bottom: "map32_42_eltsum"
  bottom: "map32_43_conv_end"
  top: "map32_43_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_44_bn_pre"
  type: "BatchNorm"
  bottom: "map32_43_eltsum"
  top: "map32_44_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_44_bn_pre"
  type: "BatchNorm"
  bottom: "map32_43_eltsum"
  top: "map32_44_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_44_pre_scale"
  type: "Scale"
  bottom: "map32_44_bn_pre"
  top: "map32_44_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_44_pre_relu"
  type: "ReLU"
  bottom: "map32_44_bn_pre"
  top: "map32_44_bn_pre"
}
layer {
  name: "map32_44_conv1"
  type: "Convolution"
  bottom: "map32_44_bn_pre"
  top: "map32_44_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_44_bn1"
  type: "BatchNorm"
  bottom: "map32_44_conv1"
  top: "map32_44_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_44_bn1"
  type: "BatchNorm"
  bottom: "map32_44_conv1"
  top: "map32_44_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_44_scale1"
  type: "Scale"
  bottom: "map32_44_conv1"
  top: "map32_44_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_44_relu1"
  type: "ReLU"
  bottom: "map32_44_conv1"
  top: "map32_44_conv1"
}
layer {
  name: "map32_44_conv2"
  type: "Convolution"
  bottom: "map32_44_conv1"
  top: "map32_44_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_44_bn2"
  type: "BatchNorm"
  bottom: "map32_44_conv2"
  top: "map32_44_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_44_bn2"
  type: "BatchNorm"
  bottom: "map32_44_conv2"
  top: "map32_44_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_44_scale2"
  type: "Scale"
  bottom: "map32_44_conv2"
  top: "map32_44_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_44_relu2"
  type: "ReLU"
  bottom: "map32_44_conv2"
  top: "map32_44_conv2"
}
layer {
  name: "map32_44_conv_end"
  type: "Convolution"
  bottom: "map32_44_conv2"
  top: "map32_44_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_44_eltsum"
  type: "Eltwise"
  bottom: "map32_43_eltsum"
  bottom: "map32_44_conv_end"
  top: "map32_44_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_45_bn_pre"
  type: "BatchNorm"
  bottom: "map32_44_eltsum"
  top: "map32_45_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_45_bn_pre"
  type: "BatchNorm"
  bottom: "map32_44_eltsum"
  top: "map32_45_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_45_pre_scale"
  type: "Scale"
  bottom: "map32_45_bn_pre"
  top: "map32_45_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_45_pre_relu"
  type: "ReLU"
  bottom: "map32_45_bn_pre"
  top: "map32_45_bn_pre"
}
layer {
  name: "map32_45_conv1"
  type: "Convolution"
  bottom: "map32_45_bn_pre"
  top: "map32_45_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_45_bn1"
  type: "BatchNorm"
  bottom: "map32_45_conv1"
  top: "map32_45_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_45_bn1"
  type: "BatchNorm"
  bottom: "map32_45_conv1"
  top: "map32_45_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_45_scale1"
  type: "Scale"
  bottom: "map32_45_conv1"
  top: "map32_45_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_45_relu1"
  type: "ReLU"
  bottom: "map32_45_conv1"
  top: "map32_45_conv1"
}
layer {
  name: "map32_45_conv2"
  type: "Convolution"
  bottom: "map32_45_conv1"
  top: "map32_45_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_45_bn2"
  type: "BatchNorm"
  bottom: "map32_45_conv2"
  top: "map32_45_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_45_bn2"
  type: "BatchNorm"
  bottom: "map32_45_conv2"
  top: "map32_45_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_45_scale2"
  type: "Scale"
  bottom: "map32_45_conv2"
  top: "map32_45_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_45_relu2"
  type: "ReLU"
  bottom: "map32_45_conv2"
  top: "map32_45_conv2"
}
layer {
  name: "map32_45_conv_end"
  type: "Convolution"
  bottom: "map32_45_conv2"
  top: "map32_45_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_45_eltsum"
  type: "Eltwise"
  bottom: "map32_44_eltsum"
  bottom: "map32_45_conv_end"
  top: "map32_45_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_46_bn_pre"
  type: "BatchNorm"
  bottom: "map32_45_eltsum"
  top: "map32_46_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_46_bn_pre"
  type: "BatchNorm"
  bottom: "map32_45_eltsum"
  top: "map32_46_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_46_pre_scale"
  type: "Scale"
  bottom: "map32_46_bn_pre"
  top: "map32_46_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_46_pre_relu"
  type: "ReLU"
  bottom: "map32_46_bn_pre"
  top: "map32_46_bn_pre"
}
layer {
  name: "map32_46_conv1"
  type: "Convolution"
  bottom: "map32_46_bn_pre"
  top: "map32_46_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_46_bn1"
  type: "BatchNorm"
  bottom: "map32_46_conv1"
  top: "map32_46_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_46_bn1"
  type: "BatchNorm"
  bottom: "map32_46_conv1"
  top: "map32_46_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_46_scale1"
  type: "Scale"
  bottom: "map32_46_conv1"
  top: "map32_46_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_46_relu1"
  type: "ReLU"
  bottom: "map32_46_conv1"
  top: "map32_46_conv1"
}
layer {
  name: "map32_46_conv2"
  type: "Convolution"
  bottom: "map32_46_conv1"
  top: "map32_46_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_46_bn2"
  type: "BatchNorm"
  bottom: "map32_46_conv2"
  top: "map32_46_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_46_bn2"
  type: "BatchNorm"
  bottom: "map32_46_conv2"
  top: "map32_46_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_46_scale2"
  type: "Scale"
  bottom: "map32_46_conv2"
  top: "map32_46_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_46_relu2"
  type: "ReLU"
  bottom: "map32_46_conv2"
  top: "map32_46_conv2"
}
layer {
  name: "map32_46_conv_end"
  type: "Convolution"
  bottom: "map32_46_conv2"
  top: "map32_46_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_46_eltsum"
  type: "Eltwise"
  bottom: "map32_45_eltsum"
  bottom: "map32_46_conv_end"
  top: "map32_46_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_47_bn_pre"
  type: "BatchNorm"
  bottom: "map32_46_eltsum"
  top: "map32_47_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_47_bn_pre"
  type: "BatchNorm"
  bottom: "map32_46_eltsum"
  top: "map32_47_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_47_pre_scale"
  type: "Scale"
  bottom: "map32_47_bn_pre"
  top: "map32_47_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_47_pre_relu"
  type: "ReLU"
  bottom: "map32_47_bn_pre"
  top: "map32_47_bn_pre"
}
layer {
  name: "map32_47_conv1"
  type: "Convolution"
  bottom: "map32_47_bn_pre"
  top: "map32_47_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_47_bn1"
  type: "BatchNorm"
  bottom: "map32_47_conv1"
  top: "map32_47_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_47_bn1"
  type: "BatchNorm"
  bottom: "map32_47_conv1"
  top: "map32_47_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_47_scale1"
  type: "Scale"
  bottom: "map32_47_conv1"
  top: "map32_47_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_47_relu1"
  type: "ReLU"
  bottom: "map32_47_conv1"
  top: "map32_47_conv1"
}
layer {
  name: "map32_47_conv2"
  type: "Convolution"
  bottom: "map32_47_conv1"
  top: "map32_47_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_47_bn2"
  type: "BatchNorm"
  bottom: "map32_47_conv2"
  top: "map32_47_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_47_bn2"
  type: "BatchNorm"
  bottom: "map32_47_conv2"
  top: "map32_47_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_47_scale2"
  type: "Scale"
  bottom: "map32_47_conv2"
  top: "map32_47_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_47_relu2"
  type: "ReLU"
  bottom: "map32_47_conv2"
  top: "map32_47_conv2"
}
layer {
  name: "map32_47_conv_end"
  type: "Convolution"
  bottom: "map32_47_conv2"
  top: "map32_47_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_47_eltsum"
  type: "Eltwise"
  bottom: "map32_46_eltsum"
  bottom: "map32_47_conv_end"
  top: "map32_47_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_48_bn_pre"
  type: "BatchNorm"
  bottom: "map32_47_eltsum"
  top: "map32_48_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_48_bn_pre"
  type: "BatchNorm"
  bottom: "map32_47_eltsum"
  top: "map32_48_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_48_pre_scale"
  type: "Scale"
  bottom: "map32_48_bn_pre"
  top: "map32_48_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_48_pre_relu"
  type: "ReLU"
  bottom: "map32_48_bn_pre"
  top: "map32_48_bn_pre"
}
layer {
  name: "map32_48_conv1"
  type: "Convolution"
  bottom: "map32_48_bn_pre"
  top: "map32_48_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_48_bn1"
  type: "BatchNorm"
  bottom: "map32_48_conv1"
  top: "map32_48_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_48_bn1"
  type: "BatchNorm"
  bottom: "map32_48_conv1"
  top: "map32_48_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_48_scale1"
  type: "Scale"
  bottom: "map32_48_conv1"
  top: "map32_48_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_48_relu1"
  type: "ReLU"
  bottom: "map32_48_conv1"
  top: "map32_48_conv1"
}
layer {
  name: "map32_48_conv2"
  type: "Convolution"
  bottom: "map32_48_conv1"
  top: "map32_48_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_48_bn2"
  type: "BatchNorm"
  bottom: "map32_48_conv2"
  top: "map32_48_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_48_bn2"
  type: "BatchNorm"
  bottom: "map32_48_conv2"
  top: "map32_48_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_48_scale2"
  type: "Scale"
  bottom: "map32_48_conv2"
  top: "map32_48_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_48_relu2"
  type: "ReLU"
  bottom: "map32_48_conv2"
  top: "map32_48_conv2"
}
layer {
  name: "map32_48_conv_end"
  type: "Convolution"
  bottom: "map32_48_conv2"
  top: "map32_48_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_48_eltsum"
  type: "Eltwise"
  bottom: "map32_47_eltsum"
  bottom: "map32_48_conv_end"
  top: "map32_48_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_49_bn_pre"
  type: "BatchNorm"
  bottom: "map32_48_eltsum"
  top: "map32_49_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_49_bn_pre"
  type: "BatchNorm"
  bottom: "map32_48_eltsum"
  top: "map32_49_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_49_pre_scale"
  type: "Scale"
  bottom: "map32_49_bn_pre"
  top: "map32_49_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_49_pre_relu"
  type: "ReLU"
  bottom: "map32_49_bn_pre"
  top: "map32_49_bn_pre"
}
layer {
  name: "map32_49_conv1"
  type: "Convolution"
  bottom: "map32_49_bn_pre"
  top: "map32_49_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_49_bn1"
  type: "BatchNorm"
  bottom: "map32_49_conv1"
  top: "map32_49_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_49_bn1"
  type: "BatchNorm"
  bottom: "map32_49_conv1"
  top: "map32_49_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_49_scale1"
  type: "Scale"
  bottom: "map32_49_conv1"
  top: "map32_49_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_49_relu1"
  type: "ReLU"
  bottom: "map32_49_conv1"
  top: "map32_49_conv1"
}
layer {
  name: "map32_49_conv2"
  type: "Convolution"
  bottom: "map32_49_conv1"
  top: "map32_49_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_49_bn2"
  type: "BatchNorm"
  bottom: "map32_49_conv2"
  top: "map32_49_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_49_bn2"
  type: "BatchNorm"
  bottom: "map32_49_conv2"
  top: "map32_49_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_49_scale2"
  type: "Scale"
  bottom: "map32_49_conv2"
  top: "map32_49_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_49_relu2"
  type: "ReLU"
  bottom: "map32_49_conv2"
  top: "map32_49_conv2"
}
layer {
  name: "map32_49_conv_end"
  type: "Convolution"
  bottom: "map32_49_conv2"
  top: "map32_49_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_49_eltsum"
  type: "Eltwise"
  bottom: "map32_48_eltsum"
  bottom: "map32_49_conv_end"
  top: "map32_49_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_50_bn_pre"
  type: "BatchNorm"
  bottom: "map32_49_eltsum"
  top: "map32_50_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_50_bn_pre"
  type: "BatchNorm"
  bottom: "map32_49_eltsum"
  top: "map32_50_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_50_pre_scale"
  type: "Scale"
  bottom: "map32_50_bn_pre"
  top: "map32_50_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_50_pre_relu"
  type: "ReLU"
  bottom: "map32_50_bn_pre"
  top: "map32_50_bn_pre"
}
layer {
  name: "map32_50_conv1"
  type: "Convolution"
  bottom: "map32_50_bn_pre"
  top: "map32_50_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_50_bn1"
  type: "BatchNorm"
  bottom: "map32_50_conv1"
  top: "map32_50_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_50_bn1"
  type: "BatchNorm"
  bottom: "map32_50_conv1"
  top: "map32_50_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_50_scale1"
  type: "Scale"
  bottom: "map32_50_conv1"
  top: "map32_50_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_50_relu1"
  type: "ReLU"
  bottom: "map32_50_conv1"
  top: "map32_50_conv1"
}
layer {
  name: "map32_50_conv2"
  type: "Convolution"
  bottom: "map32_50_conv1"
  top: "map32_50_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_50_bn2"
  type: "BatchNorm"
  bottom: "map32_50_conv2"
  top: "map32_50_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_50_bn2"
  type: "BatchNorm"
  bottom: "map32_50_conv2"
  top: "map32_50_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_50_scale2"
  type: "Scale"
  bottom: "map32_50_conv2"
  top: "map32_50_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_50_relu2"
  type: "ReLU"
  bottom: "map32_50_conv2"
  top: "map32_50_conv2"
}
layer {
  name: "map32_50_conv_end"
  type: "Convolution"
  bottom: "map32_50_conv2"
  top: "map32_50_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_50_eltsum"
  type: "Eltwise"
  bottom: "map32_49_eltsum"
  bottom: "map32_50_conv_end"
  top: "map32_50_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_51_bn_pre"
  type: "BatchNorm"
  bottom: "map32_50_eltsum"
  top: "map32_51_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_51_bn_pre"
  type: "BatchNorm"
  bottom: "map32_50_eltsum"
  top: "map32_51_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_51_pre_scale"
  type: "Scale"
  bottom: "map32_51_bn_pre"
  top: "map32_51_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_51_pre_relu"
  type: "ReLU"
  bottom: "map32_51_bn_pre"
  top: "map32_51_bn_pre"
}
layer {
  name: "map32_51_conv1"
  type: "Convolution"
  bottom: "map32_51_bn_pre"
  top: "map32_51_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_51_bn1"
  type: "BatchNorm"
  bottom: "map32_51_conv1"
  top: "map32_51_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_51_bn1"
  type: "BatchNorm"
  bottom: "map32_51_conv1"
  top: "map32_51_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_51_scale1"
  type: "Scale"
  bottom: "map32_51_conv1"
  top: "map32_51_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_51_relu1"
  type: "ReLU"
  bottom: "map32_51_conv1"
  top: "map32_51_conv1"
}
layer {
  name: "map32_51_conv2"
  type: "Convolution"
  bottom: "map32_51_conv1"
  top: "map32_51_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_51_bn2"
  type: "BatchNorm"
  bottom: "map32_51_conv2"
  top: "map32_51_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_51_bn2"
  type: "BatchNorm"
  bottom: "map32_51_conv2"
  top: "map32_51_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_51_scale2"
  type: "Scale"
  bottom: "map32_51_conv2"
  top: "map32_51_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_51_relu2"
  type: "ReLU"
  bottom: "map32_51_conv2"
  top: "map32_51_conv2"
}
layer {
  name: "map32_51_conv_end"
  type: "Convolution"
  bottom: "map32_51_conv2"
  top: "map32_51_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_51_eltsum"
  type: "Eltwise"
  bottom: "map32_50_eltsum"
  bottom: "map32_51_conv_end"
  top: "map32_51_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_52_bn_pre"
  type: "BatchNorm"
  bottom: "map32_51_eltsum"
  top: "map32_52_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_52_bn_pre"
  type: "BatchNorm"
  bottom: "map32_51_eltsum"
  top: "map32_52_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_52_pre_scale"
  type: "Scale"
  bottom: "map32_52_bn_pre"
  top: "map32_52_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_52_pre_relu"
  type: "ReLU"
  bottom: "map32_52_bn_pre"
  top: "map32_52_bn_pre"
}
layer {
  name: "map32_52_conv1"
  type: "Convolution"
  bottom: "map32_52_bn_pre"
  top: "map32_52_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_52_bn1"
  type: "BatchNorm"
  bottom: "map32_52_conv1"
  top: "map32_52_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_52_bn1"
  type: "BatchNorm"
  bottom: "map32_52_conv1"
  top: "map32_52_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_52_scale1"
  type: "Scale"
  bottom: "map32_52_conv1"
  top: "map32_52_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_52_relu1"
  type: "ReLU"
  bottom: "map32_52_conv1"
  top: "map32_52_conv1"
}
layer {
  name: "map32_52_conv2"
  type: "Convolution"
  bottom: "map32_52_conv1"
  top: "map32_52_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_52_bn2"
  type: "BatchNorm"
  bottom: "map32_52_conv2"
  top: "map32_52_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_52_bn2"
  type: "BatchNorm"
  bottom: "map32_52_conv2"
  top: "map32_52_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_52_scale2"
  type: "Scale"
  bottom: "map32_52_conv2"
  top: "map32_52_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_52_relu2"
  type: "ReLU"
  bottom: "map32_52_conv2"
  top: "map32_52_conv2"
}
layer {
  name: "map32_52_conv_end"
  type: "Convolution"
  bottom: "map32_52_conv2"
  top: "map32_52_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_52_eltsum"
  type: "Eltwise"
  bottom: "map32_51_eltsum"
  bottom: "map32_52_conv_end"
  top: "map32_52_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_53_bn_pre"
  type: "BatchNorm"
  bottom: "map32_52_eltsum"
  top: "map32_53_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_53_bn_pre"
  type: "BatchNorm"
  bottom: "map32_52_eltsum"
  top: "map32_53_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_53_pre_scale"
  type: "Scale"
  bottom: "map32_53_bn_pre"
  top: "map32_53_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_53_pre_relu"
  type: "ReLU"
  bottom: "map32_53_bn_pre"
  top: "map32_53_bn_pre"
}
layer {
  name: "map32_53_conv1"
  type: "Convolution"
  bottom: "map32_53_bn_pre"
  top: "map32_53_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_53_bn1"
  type: "BatchNorm"
  bottom: "map32_53_conv1"
  top: "map32_53_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_53_bn1"
  type: "BatchNorm"
  bottom: "map32_53_conv1"
  top: "map32_53_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_53_scale1"
  type: "Scale"
  bottom: "map32_53_conv1"
  top: "map32_53_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_53_relu1"
  type: "ReLU"
  bottom: "map32_53_conv1"
  top: "map32_53_conv1"
}
layer {
  name: "map32_53_conv2"
  type: "Convolution"
  bottom: "map32_53_conv1"
  top: "map32_53_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_53_bn2"
  type: "BatchNorm"
  bottom: "map32_53_conv2"
  top: "map32_53_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_53_bn2"
  type: "BatchNorm"
  bottom: "map32_53_conv2"
  top: "map32_53_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_53_scale2"
  type: "Scale"
  bottom: "map32_53_conv2"
  top: "map32_53_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_53_relu2"
  type: "ReLU"
  bottom: "map32_53_conv2"
  top: "map32_53_conv2"
}
layer {
  name: "map32_53_conv_end"
  type: "Convolution"
  bottom: "map32_53_conv2"
  top: "map32_53_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_53_eltsum"
  type: "Eltwise"
  bottom: "map32_52_eltsum"
  bottom: "map32_53_conv_end"
  top: "map32_53_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_54_bn_pre"
  type: "BatchNorm"
  bottom: "map32_53_eltsum"
  top: "map32_54_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_54_bn_pre"
  type: "BatchNorm"
  bottom: "map32_53_eltsum"
  top: "map32_54_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_54_pre_scale"
  type: "Scale"
  bottom: "map32_54_bn_pre"
  top: "map32_54_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_54_pre_relu"
  type: "ReLU"
  bottom: "map32_54_bn_pre"
  top: "map32_54_bn_pre"
}
layer {
  name: "map32_54_conv1"
  type: "Convolution"
  bottom: "map32_54_bn_pre"
  top: "map32_54_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_54_bn1"
  type: "BatchNorm"
  bottom: "map32_54_conv1"
  top: "map32_54_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_54_bn1"
  type: "BatchNorm"
  bottom: "map32_54_conv1"
  top: "map32_54_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_54_scale1"
  type: "Scale"
  bottom: "map32_54_conv1"
  top: "map32_54_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_54_relu1"
  type: "ReLU"
  bottom: "map32_54_conv1"
  top: "map32_54_conv1"
}
layer {
  name: "map32_54_conv2"
  type: "Convolution"
  bottom: "map32_54_conv1"
  top: "map32_54_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_54_bn2"
  type: "BatchNorm"
  bottom: "map32_54_conv2"
  top: "map32_54_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_54_bn2"
  type: "BatchNorm"
  bottom: "map32_54_conv2"
  top: "map32_54_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_54_scale2"
  type: "Scale"
  bottom: "map32_54_conv2"
  top: "map32_54_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_54_relu2"
  type: "ReLU"
  bottom: "map32_54_conv2"
  top: "map32_54_conv2"
}
layer {
  name: "map32_54_conv_end"
  type: "Convolution"
  bottom: "map32_54_conv2"
  top: "map32_54_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_54_eltsum"
  type: "Eltwise"
  bottom: "map32_53_eltsum"
  bottom: "map32_54_conv_end"
  top: "map32_54_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_55_bn_pre"
  type: "BatchNorm"
  bottom: "map32_54_eltsum"
  top: "map32_55_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_55_bn_pre"
  type: "BatchNorm"
  bottom: "map32_54_eltsum"
  top: "map32_55_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_55_pre_scale"
  type: "Scale"
  bottom: "map32_55_bn_pre"
  top: "map32_55_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_55_pre_relu"
  type: "ReLU"
  bottom: "map32_55_bn_pre"
  top: "map32_55_bn_pre"
}
layer {
  name: "map32_55_conv1"
  type: "Convolution"
  bottom: "map32_55_bn_pre"
  top: "map32_55_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_55_bn1"
  type: "BatchNorm"
  bottom: "map32_55_conv1"
  top: "map32_55_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_55_bn1"
  type: "BatchNorm"
  bottom: "map32_55_conv1"
  top: "map32_55_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_55_scale1"
  type: "Scale"
  bottom: "map32_55_conv1"
  top: "map32_55_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_55_relu1"
  type: "ReLU"
  bottom: "map32_55_conv1"
  top: "map32_55_conv1"
}
layer {
  name: "map32_55_conv2"
  type: "Convolution"
  bottom: "map32_55_conv1"
  top: "map32_55_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_55_bn2"
  type: "BatchNorm"
  bottom: "map32_55_conv2"
  top: "map32_55_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_55_bn2"
  type: "BatchNorm"
  bottom: "map32_55_conv2"
  top: "map32_55_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_55_scale2"
  type: "Scale"
  bottom: "map32_55_conv2"
  top: "map32_55_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_55_relu2"
  type: "ReLU"
  bottom: "map32_55_conv2"
  top: "map32_55_conv2"
}
layer {
  name: "map32_55_conv_end"
  type: "Convolution"
  bottom: "map32_55_conv2"
  top: "map32_55_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_55_eltsum"
  type: "Eltwise"
  bottom: "map32_54_eltsum"
  bottom: "map32_55_conv_end"
  top: "map32_55_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_56_bn_pre"
  type: "BatchNorm"
  bottom: "map32_55_eltsum"
  top: "map32_56_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_56_bn_pre"
  type: "BatchNorm"
  bottom: "map32_55_eltsum"
  top: "map32_56_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_56_pre_scale"
  type: "Scale"
  bottom: "map32_56_bn_pre"
  top: "map32_56_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_56_pre_relu"
  type: "ReLU"
  bottom: "map32_56_bn_pre"
  top: "map32_56_bn_pre"
}
layer {
  name: "map32_56_conv1"
  type: "Convolution"
  bottom: "map32_56_bn_pre"
  top: "map32_56_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_56_bn1"
  type: "BatchNorm"
  bottom: "map32_56_conv1"
  top: "map32_56_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_56_bn1"
  type: "BatchNorm"
  bottom: "map32_56_conv1"
  top: "map32_56_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_56_scale1"
  type: "Scale"
  bottom: "map32_56_conv1"
  top: "map32_56_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_56_relu1"
  type: "ReLU"
  bottom: "map32_56_conv1"
  top: "map32_56_conv1"
}
layer {
  name: "map32_56_conv2"
  type: "Convolution"
  bottom: "map32_56_conv1"
  top: "map32_56_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_56_bn2"
  type: "BatchNorm"
  bottom: "map32_56_conv2"
  top: "map32_56_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_56_bn2"
  type: "BatchNorm"
  bottom: "map32_56_conv2"
  top: "map32_56_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_56_scale2"
  type: "Scale"
  bottom: "map32_56_conv2"
  top: "map32_56_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_56_relu2"
  type: "ReLU"
  bottom: "map32_56_conv2"
  top: "map32_56_conv2"
}
layer {
  name: "map32_56_conv_end"
  type: "Convolution"
  bottom: "map32_56_conv2"
  top: "map32_56_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_56_eltsum"
  type: "Eltwise"
  bottom: "map32_55_eltsum"
  bottom: "map32_56_conv_end"
  top: "map32_56_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_57_bn_pre"
  type: "BatchNorm"
  bottom: "map32_56_eltsum"
  top: "map32_57_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_57_bn_pre"
  type: "BatchNorm"
  bottom: "map32_56_eltsum"
  top: "map32_57_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_57_pre_scale"
  type: "Scale"
  bottom: "map32_57_bn_pre"
  top: "map32_57_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_57_pre_relu"
  type: "ReLU"
  bottom: "map32_57_bn_pre"
  top: "map32_57_bn_pre"
}
layer {
  name: "map32_57_conv1"
  type: "Convolution"
  bottom: "map32_57_bn_pre"
  top: "map32_57_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_57_bn1"
  type: "BatchNorm"
  bottom: "map32_57_conv1"
  top: "map32_57_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_57_bn1"
  type: "BatchNorm"
  bottom: "map32_57_conv1"
  top: "map32_57_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_57_scale1"
  type: "Scale"
  bottom: "map32_57_conv1"
  top: "map32_57_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_57_relu1"
  type: "ReLU"
  bottom: "map32_57_conv1"
  top: "map32_57_conv1"
}
layer {
  name: "map32_57_conv2"
  type: "Convolution"
  bottom: "map32_57_conv1"
  top: "map32_57_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_57_bn2"
  type: "BatchNorm"
  bottom: "map32_57_conv2"
  top: "map32_57_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_57_bn2"
  type: "BatchNorm"
  bottom: "map32_57_conv2"
  top: "map32_57_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_57_scale2"
  type: "Scale"
  bottom: "map32_57_conv2"
  top: "map32_57_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_57_relu2"
  type: "ReLU"
  bottom: "map32_57_conv2"
  top: "map32_57_conv2"
}
layer {
  name: "map32_57_conv_end"
  type: "Convolution"
  bottom: "map32_57_conv2"
  top: "map32_57_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_57_eltsum"
  type: "Eltwise"
  bottom: "map32_56_eltsum"
  bottom: "map32_57_conv_end"
  top: "map32_57_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_58_bn_pre"
  type: "BatchNorm"
  bottom: "map32_57_eltsum"
  top: "map32_58_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_58_bn_pre"
  type: "BatchNorm"
  bottom: "map32_57_eltsum"
  top: "map32_58_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_58_pre_scale"
  type: "Scale"
  bottom: "map32_58_bn_pre"
  top: "map32_58_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_58_pre_relu"
  type: "ReLU"
  bottom: "map32_58_bn_pre"
  top: "map32_58_bn_pre"
}
layer {
  name: "map32_58_conv1"
  type: "Convolution"
  bottom: "map32_58_bn_pre"
  top: "map32_58_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_58_bn1"
  type: "BatchNorm"
  bottom: "map32_58_conv1"
  top: "map32_58_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_58_bn1"
  type: "BatchNorm"
  bottom: "map32_58_conv1"
  top: "map32_58_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_58_scale1"
  type: "Scale"
  bottom: "map32_58_conv1"
  top: "map32_58_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_58_relu1"
  type: "ReLU"
  bottom: "map32_58_conv1"
  top: "map32_58_conv1"
}
layer {
  name: "map32_58_conv2"
  type: "Convolution"
  bottom: "map32_58_conv1"
  top: "map32_58_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_58_bn2"
  type: "BatchNorm"
  bottom: "map32_58_conv2"
  top: "map32_58_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_58_bn2"
  type: "BatchNorm"
  bottom: "map32_58_conv2"
  top: "map32_58_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_58_scale2"
  type: "Scale"
  bottom: "map32_58_conv2"
  top: "map32_58_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_58_relu2"
  type: "ReLU"
  bottom: "map32_58_conv2"
  top: "map32_58_conv2"
}
layer {
  name: "map32_58_conv_end"
  type: "Convolution"
  bottom: "map32_58_conv2"
  top: "map32_58_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_58_eltsum"
  type: "Eltwise"
  bottom: "map32_57_eltsum"
  bottom: "map32_58_conv_end"
  top: "map32_58_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_59_bn_pre"
  type: "BatchNorm"
  bottom: "map32_58_eltsum"
  top: "map32_59_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_59_bn_pre"
  type: "BatchNorm"
  bottom: "map32_58_eltsum"
  top: "map32_59_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_59_pre_scale"
  type: "Scale"
  bottom: "map32_59_bn_pre"
  top: "map32_59_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_59_pre_relu"
  type: "ReLU"
  bottom: "map32_59_bn_pre"
  top: "map32_59_bn_pre"
}
layer {
  name: "map32_59_conv1"
  type: "Convolution"
  bottom: "map32_59_bn_pre"
  top: "map32_59_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_59_bn1"
  type: "BatchNorm"
  bottom: "map32_59_conv1"
  top: "map32_59_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_59_bn1"
  type: "BatchNorm"
  bottom: "map32_59_conv1"
  top: "map32_59_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_59_scale1"
  type: "Scale"
  bottom: "map32_59_conv1"
  top: "map32_59_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_59_relu1"
  type: "ReLU"
  bottom: "map32_59_conv1"
  top: "map32_59_conv1"
}
layer {
  name: "map32_59_conv2"
  type: "Convolution"
  bottom: "map32_59_conv1"
  top: "map32_59_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_59_bn2"
  type: "BatchNorm"
  bottom: "map32_59_conv2"
  top: "map32_59_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_59_bn2"
  type: "BatchNorm"
  bottom: "map32_59_conv2"
  top: "map32_59_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_59_scale2"
  type: "Scale"
  bottom: "map32_59_conv2"
  top: "map32_59_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_59_relu2"
  type: "ReLU"
  bottom: "map32_59_conv2"
  top: "map32_59_conv2"
}
layer {
  name: "map32_59_conv_end"
  type: "Convolution"
  bottom: "map32_59_conv2"
  top: "map32_59_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_59_eltsum"
  type: "Eltwise"
  bottom: "map32_58_eltsum"
  bottom: "map32_59_conv_end"
  top: "map32_59_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_60_bn_pre"
  type: "BatchNorm"
  bottom: "map32_59_eltsum"
  top: "map32_60_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_60_bn_pre"
  type: "BatchNorm"
  bottom: "map32_59_eltsum"
  top: "map32_60_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_60_pre_scale"
  type: "Scale"
  bottom: "map32_60_bn_pre"
  top: "map32_60_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_60_pre_relu"
  type: "ReLU"
  bottom: "map32_60_bn_pre"
  top: "map32_60_bn_pre"
}
layer {
  name: "map32_60_conv1"
  type: "Convolution"
  bottom: "map32_60_bn_pre"
  top: "map32_60_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_60_bn1"
  type: "BatchNorm"
  bottom: "map32_60_conv1"
  top: "map32_60_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_60_bn1"
  type: "BatchNorm"
  bottom: "map32_60_conv1"
  top: "map32_60_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_60_scale1"
  type: "Scale"
  bottom: "map32_60_conv1"
  top: "map32_60_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_60_relu1"
  type: "ReLU"
  bottom: "map32_60_conv1"
  top: "map32_60_conv1"
}
layer {
  name: "map32_60_conv2"
  type: "Convolution"
  bottom: "map32_60_conv1"
  top: "map32_60_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_60_bn2"
  type: "BatchNorm"
  bottom: "map32_60_conv2"
  top: "map32_60_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_60_bn2"
  type: "BatchNorm"
  bottom: "map32_60_conv2"
  top: "map32_60_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_60_scale2"
  type: "Scale"
  bottom: "map32_60_conv2"
  top: "map32_60_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_60_relu2"
  type: "ReLU"
  bottom: "map32_60_conv2"
  top: "map32_60_conv2"
}
layer {
  name: "map32_60_conv_end"
  type: "Convolution"
  bottom: "map32_60_conv2"
  top: "map32_60_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_60_eltsum"
  type: "Eltwise"
  bottom: "map32_59_eltsum"
  bottom: "map32_60_conv_end"
  top: "map32_60_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_61_bn_pre"
  type: "BatchNorm"
  bottom: "map32_60_eltsum"
  top: "map32_61_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_61_bn_pre"
  type: "BatchNorm"
  bottom: "map32_60_eltsum"
  top: "map32_61_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_61_pre_scale"
  type: "Scale"
  bottom: "map32_61_bn_pre"
  top: "map32_61_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_61_pre_relu"
  type: "ReLU"
  bottom: "map32_61_bn_pre"
  top: "map32_61_bn_pre"
}
layer {
  name: "map32_61_conv1"
  type: "Convolution"
  bottom: "map32_61_bn_pre"
  top: "map32_61_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_61_bn1"
  type: "BatchNorm"
  bottom: "map32_61_conv1"
  top: "map32_61_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_61_bn1"
  type: "BatchNorm"
  bottom: "map32_61_conv1"
  top: "map32_61_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_61_scale1"
  type: "Scale"
  bottom: "map32_61_conv1"
  top: "map32_61_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_61_relu1"
  type: "ReLU"
  bottom: "map32_61_conv1"
  top: "map32_61_conv1"
}
layer {
  name: "map32_61_conv2"
  type: "Convolution"
  bottom: "map32_61_conv1"
  top: "map32_61_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_61_bn2"
  type: "BatchNorm"
  bottom: "map32_61_conv2"
  top: "map32_61_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_61_bn2"
  type: "BatchNorm"
  bottom: "map32_61_conv2"
  top: "map32_61_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_61_scale2"
  type: "Scale"
  bottom: "map32_61_conv2"
  top: "map32_61_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_61_relu2"
  type: "ReLU"
  bottom: "map32_61_conv2"
  top: "map32_61_conv2"
}
layer {
  name: "map32_61_conv_end"
  type: "Convolution"
  bottom: "map32_61_conv2"
  top: "map32_61_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_61_eltsum"
  type: "Eltwise"
  bottom: "map32_60_eltsum"
  bottom: "map32_61_conv_end"
  top: "map32_61_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_62_bn_pre"
  type: "BatchNorm"
  bottom: "map32_61_eltsum"
  top: "map32_62_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_62_bn_pre"
  type: "BatchNorm"
  bottom: "map32_61_eltsum"
  top: "map32_62_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_62_pre_scale"
  type: "Scale"
  bottom: "map32_62_bn_pre"
  top: "map32_62_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_62_pre_relu"
  type: "ReLU"
  bottom: "map32_62_bn_pre"
  top: "map32_62_bn_pre"
}
layer {
  name: "map32_62_conv1"
  type: "Convolution"
  bottom: "map32_62_bn_pre"
  top: "map32_62_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_62_bn1"
  type: "BatchNorm"
  bottom: "map32_62_conv1"
  top: "map32_62_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_62_bn1"
  type: "BatchNorm"
  bottom: "map32_62_conv1"
  top: "map32_62_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_62_scale1"
  type: "Scale"
  bottom: "map32_62_conv1"
  top: "map32_62_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_62_relu1"
  type: "ReLU"
  bottom: "map32_62_conv1"
  top: "map32_62_conv1"
}
layer {
  name: "map32_62_conv2"
  type: "Convolution"
  bottom: "map32_62_conv1"
  top: "map32_62_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_62_bn2"
  type: "BatchNorm"
  bottom: "map32_62_conv2"
  top: "map32_62_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_62_bn2"
  type: "BatchNorm"
  bottom: "map32_62_conv2"
  top: "map32_62_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_62_scale2"
  type: "Scale"
  bottom: "map32_62_conv2"
  top: "map32_62_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_62_relu2"
  type: "ReLU"
  bottom: "map32_62_conv2"
  top: "map32_62_conv2"
}
layer {
  name: "map32_62_conv_end"
  type: "Convolution"
  bottom: "map32_62_conv2"
  top: "map32_62_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_62_eltsum"
  type: "Eltwise"
  bottom: "map32_61_eltsum"
  bottom: "map32_62_conv_end"
  top: "map32_62_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_63_bn_pre"
  type: "BatchNorm"
  bottom: "map32_62_eltsum"
  top: "map32_63_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_63_bn_pre"
  type: "BatchNorm"
  bottom: "map32_62_eltsum"
  top: "map32_63_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_63_pre_scale"
  type: "Scale"
  bottom: "map32_63_bn_pre"
  top: "map32_63_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_63_pre_relu"
  type: "ReLU"
  bottom: "map32_63_bn_pre"
  top: "map32_63_bn_pre"
}
layer {
  name: "map32_63_conv1"
  type: "Convolution"
  bottom: "map32_63_bn_pre"
  top: "map32_63_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_63_bn1"
  type: "BatchNorm"
  bottom: "map32_63_conv1"
  top: "map32_63_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_63_bn1"
  type: "BatchNorm"
  bottom: "map32_63_conv1"
  top: "map32_63_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_63_scale1"
  type: "Scale"
  bottom: "map32_63_conv1"
  top: "map32_63_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_63_relu1"
  type: "ReLU"
  bottom: "map32_63_conv1"
  top: "map32_63_conv1"
}
layer {
  name: "map32_63_conv2"
  type: "Convolution"
  bottom: "map32_63_conv1"
  top: "map32_63_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_63_bn2"
  type: "BatchNorm"
  bottom: "map32_63_conv2"
  top: "map32_63_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_63_bn2"
  type: "BatchNorm"
  bottom: "map32_63_conv2"
  top: "map32_63_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_63_scale2"
  type: "Scale"
  bottom: "map32_63_conv2"
  top: "map32_63_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_63_relu2"
  type: "ReLU"
  bottom: "map32_63_conv2"
  top: "map32_63_conv2"
}
layer {
  name: "map32_63_conv_end"
  type: "Convolution"
  bottom: "map32_63_conv2"
  top: "map32_63_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_63_eltsum"
  type: "Eltwise"
  bottom: "map32_62_eltsum"
  bottom: "map32_63_conv_end"
  top: "map32_63_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_64_bn_pre"
  type: "BatchNorm"
  bottom: "map32_63_eltsum"
  top: "map32_64_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_64_bn_pre"
  type: "BatchNorm"
  bottom: "map32_63_eltsum"
  top: "map32_64_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_64_pre_scale"
  type: "Scale"
  bottom: "map32_64_bn_pre"
  top: "map32_64_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_64_pre_relu"
  type: "ReLU"
  bottom: "map32_64_bn_pre"
  top: "map32_64_bn_pre"
}
layer {
  name: "map32_64_conv1"
  type: "Convolution"
  bottom: "map32_64_bn_pre"
  top: "map32_64_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_64_bn1"
  type: "BatchNorm"
  bottom: "map32_64_conv1"
  top: "map32_64_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_64_bn1"
  type: "BatchNorm"
  bottom: "map32_64_conv1"
  top: "map32_64_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_64_scale1"
  type: "Scale"
  bottom: "map32_64_conv1"
  top: "map32_64_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_64_relu1"
  type: "ReLU"
  bottom: "map32_64_conv1"
  top: "map32_64_conv1"
}
layer {
  name: "map32_64_conv2"
  type: "Convolution"
  bottom: "map32_64_conv1"
  top: "map32_64_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_64_bn2"
  type: "BatchNorm"
  bottom: "map32_64_conv2"
  top: "map32_64_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_64_bn2"
  type: "BatchNorm"
  bottom: "map32_64_conv2"
  top: "map32_64_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_64_scale2"
  type: "Scale"
  bottom: "map32_64_conv2"
  top: "map32_64_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_64_relu2"
  type: "ReLU"
  bottom: "map32_64_conv2"
  top: "map32_64_conv2"
}
layer {
  name: "map32_64_conv_end"
  type: "Convolution"
  bottom: "map32_64_conv2"
  top: "map32_64_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_64_eltsum"
  type: "Eltwise"
  bottom: "map32_63_eltsum"
  bottom: "map32_64_conv_end"
  top: "map32_64_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_65_bn_pre"
  type: "BatchNorm"
  bottom: "map32_64_eltsum"
  top: "map32_65_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_65_bn_pre"
  type: "BatchNorm"
  bottom: "map32_64_eltsum"
  top: "map32_65_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_65_pre_scale"
  type: "Scale"
  bottom: "map32_65_bn_pre"
  top: "map32_65_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_65_pre_relu"
  type: "ReLU"
  bottom: "map32_65_bn_pre"
  top: "map32_65_bn_pre"
}
layer {
  name: "map32_65_conv1"
  type: "Convolution"
  bottom: "map32_65_bn_pre"
  top: "map32_65_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_65_bn1"
  type: "BatchNorm"
  bottom: "map32_65_conv1"
  top: "map32_65_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_65_bn1"
  type: "BatchNorm"
  bottom: "map32_65_conv1"
  top: "map32_65_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_65_scale1"
  type: "Scale"
  bottom: "map32_65_conv1"
  top: "map32_65_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_65_relu1"
  type: "ReLU"
  bottom: "map32_65_conv1"
  top: "map32_65_conv1"
}
layer {
  name: "map32_65_conv2"
  type: "Convolution"
  bottom: "map32_65_conv1"
  top: "map32_65_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_65_bn2"
  type: "BatchNorm"
  bottom: "map32_65_conv2"
  top: "map32_65_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_65_bn2"
  type: "BatchNorm"
  bottom: "map32_65_conv2"
  top: "map32_65_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_65_scale2"
  type: "Scale"
  bottom: "map32_65_conv2"
  top: "map32_65_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_65_relu2"
  type: "ReLU"
  bottom: "map32_65_conv2"
  top: "map32_65_conv2"
}
layer {
  name: "map32_65_conv_end"
  type: "Convolution"
  bottom: "map32_65_conv2"
  top: "map32_65_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_65_eltsum"
  type: "Eltwise"
  bottom: "map32_64_eltsum"
  bottom: "map32_65_conv_end"
  top: "map32_65_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_66_bn_pre"
  type: "BatchNorm"
  bottom: "map32_65_eltsum"
  top: "map32_66_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_66_bn_pre"
  type: "BatchNorm"
  bottom: "map32_65_eltsum"
  top: "map32_66_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_66_pre_scale"
  type: "Scale"
  bottom: "map32_66_bn_pre"
  top: "map32_66_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_66_pre_relu"
  type: "ReLU"
  bottom: "map32_66_bn_pre"
  top: "map32_66_bn_pre"
}
layer {
  name: "map32_66_conv1"
  type: "Convolution"
  bottom: "map32_66_bn_pre"
  top: "map32_66_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_66_bn1"
  type: "BatchNorm"
  bottom: "map32_66_conv1"
  top: "map32_66_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_66_bn1"
  type: "BatchNorm"
  bottom: "map32_66_conv1"
  top: "map32_66_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_66_scale1"
  type: "Scale"
  bottom: "map32_66_conv1"
  top: "map32_66_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_66_relu1"
  type: "ReLU"
  bottom: "map32_66_conv1"
  top: "map32_66_conv1"
}
layer {
  name: "map32_66_conv2"
  type: "Convolution"
  bottom: "map32_66_conv1"
  top: "map32_66_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_66_bn2"
  type: "BatchNorm"
  bottom: "map32_66_conv2"
  top: "map32_66_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_66_bn2"
  type: "BatchNorm"
  bottom: "map32_66_conv2"
  top: "map32_66_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_66_scale2"
  type: "Scale"
  bottom: "map32_66_conv2"
  top: "map32_66_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_66_relu2"
  type: "ReLU"
  bottom: "map32_66_conv2"
  top: "map32_66_conv2"
}
layer {
  name: "map32_66_conv_end"
  type: "Convolution"
  bottom: "map32_66_conv2"
  top: "map32_66_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_66_eltsum"
  type: "Eltwise"
  bottom: "map32_65_eltsum"
  bottom: "map32_66_conv_end"
  top: "map32_66_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_67_bn_pre"
  type: "BatchNorm"
  bottom: "map32_66_eltsum"
  top: "map32_67_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_67_bn_pre"
  type: "BatchNorm"
  bottom: "map32_66_eltsum"
  top: "map32_67_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_67_pre_scale"
  type: "Scale"
  bottom: "map32_67_bn_pre"
  top: "map32_67_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_67_pre_relu"
  type: "ReLU"
  bottom: "map32_67_bn_pre"
  top: "map32_67_bn_pre"
}
layer {
  name: "map32_67_conv1"
  type: "Convolution"
  bottom: "map32_67_bn_pre"
  top: "map32_67_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_67_bn1"
  type: "BatchNorm"
  bottom: "map32_67_conv1"
  top: "map32_67_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_67_bn1"
  type: "BatchNorm"
  bottom: "map32_67_conv1"
  top: "map32_67_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_67_scale1"
  type: "Scale"
  bottom: "map32_67_conv1"
  top: "map32_67_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_67_relu1"
  type: "ReLU"
  bottom: "map32_67_conv1"
  top: "map32_67_conv1"
}
layer {
  name: "map32_67_conv2"
  type: "Convolution"
  bottom: "map32_67_conv1"
  top: "map32_67_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_67_bn2"
  type: "BatchNorm"
  bottom: "map32_67_conv2"
  top: "map32_67_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_67_bn2"
  type: "BatchNorm"
  bottom: "map32_67_conv2"
  top: "map32_67_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_67_scale2"
  type: "Scale"
  bottom: "map32_67_conv2"
  top: "map32_67_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_67_relu2"
  type: "ReLU"
  bottom: "map32_67_conv2"
  top: "map32_67_conv2"
}
layer {
  name: "map32_67_conv_end"
  type: "Convolution"
  bottom: "map32_67_conv2"
  top: "map32_67_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_67_eltsum"
  type: "Eltwise"
  bottom: "map32_66_eltsum"
  bottom: "map32_67_conv_end"
  top: "map32_67_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_68_bn_pre"
  type: "BatchNorm"
  bottom: "map32_67_eltsum"
  top: "map32_68_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_68_bn_pre"
  type: "BatchNorm"
  bottom: "map32_67_eltsum"
  top: "map32_68_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_68_pre_scale"
  type: "Scale"
  bottom: "map32_68_bn_pre"
  top: "map32_68_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_68_pre_relu"
  type: "ReLU"
  bottom: "map32_68_bn_pre"
  top: "map32_68_bn_pre"
}
layer {
  name: "map32_68_conv1"
  type: "Convolution"
  bottom: "map32_68_bn_pre"
  top: "map32_68_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_68_bn1"
  type: "BatchNorm"
  bottom: "map32_68_conv1"
  top: "map32_68_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_68_bn1"
  type: "BatchNorm"
  bottom: "map32_68_conv1"
  top: "map32_68_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_68_scale1"
  type: "Scale"
  bottom: "map32_68_conv1"
  top: "map32_68_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_68_relu1"
  type: "ReLU"
  bottom: "map32_68_conv1"
  top: "map32_68_conv1"
}
layer {
  name: "map32_68_conv2"
  type: "Convolution"
  bottom: "map32_68_conv1"
  top: "map32_68_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_68_bn2"
  type: "BatchNorm"
  bottom: "map32_68_conv2"
  top: "map32_68_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_68_bn2"
  type: "BatchNorm"
  bottom: "map32_68_conv2"
  top: "map32_68_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_68_scale2"
  type: "Scale"
  bottom: "map32_68_conv2"
  top: "map32_68_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_68_relu2"
  type: "ReLU"
  bottom: "map32_68_conv2"
  top: "map32_68_conv2"
}
layer {
  name: "map32_68_conv_end"
  type: "Convolution"
  bottom: "map32_68_conv2"
  top: "map32_68_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_68_eltsum"
  type: "Eltwise"
  bottom: "map32_67_eltsum"
  bottom: "map32_68_conv_end"
  top: "map32_68_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_69_bn_pre"
  type: "BatchNorm"
  bottom: "map32_68_eltsum"
  top: "map32_69_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_69_bn_pre"
  type: "BatchNorm"
  bottom: "map32_68_eltsum"
  top: "map32_69_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_69_pre_scale"
  type: "Scale"
  bottom: "map32_69_bn_pre"
  top: "map32_69_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_69_pre_relu"
  type: "ReLU"
  bottom: "map32_69_bn_pre"
  top: "map32_69_bn_pre"
}
layer {
  name: "map32_69_conv1"
  type: "Convolution"
  bottom: "map32_69_bn_pre"
  top: "map32_69_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_69_bn1"
  type: "BatchNorm"
  bottom: "map32_69_conv1"
  top: "map32_69_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_69_bn1"
  type: "BatchNorm"
  bottom: "map32_69_conv1"
  top: "map32_69_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_69_scale1"
  type: "Scale"
  bottom: "map32_69_conv1"
  top: "map32_69_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_69_relu1"
  type: "ReLU"
  bottom: "map32_69_conv1"
  top: "map32_69_conv1"
}
layer {
  name: "map32_69_conv2"
  type: "Convolution"
  bottom: "map32_69_conv1"
  top: "map32_69_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_69_bn2"
  type: "BatchNorm"
  bottom: "map32_69_conv2"
  top: "map32_69_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_69_bn2"
  type: "BatchNorm"
  bottom: "map32_69_conv2"
  top: "map32_69_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_69_scale2"
  type: "Scale"
  bottom: "map32_69_conv2"
  top: "map32_69_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_69_relu2"
  type: "ReLU"
  bottom: "map32_69_conv2"
  top: "map32_69_conv2"
}
layer {
  name: "map32_69_conv_end"
  type: "Convolution"
  bottom: "map32_69_conv2"
  top: "map32_69_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_69_eltsum"
  type: "Eltwise"
  bottom: "map32_68_eltsum"
  bottom: "map32_69_conv_end"
  top: "map32_69_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_70_bn_pre"
  type: "BatchNorm"
  bottom: "map32_69_eltsum"
  top: "map32_70_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_70_bn_pre"
  type: "BatchNorm"
  bottom: "map32_69_eltsum"
  top: "map32_70_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_70_pre_scale"
  type: "Scale"
  bottom: "map32_70_bn_pre"
  top: "map32_70_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_70_pre_relu"
  type: "ReLU"
  bottom: "map32_70_bn_pre"
  top: "map32_70_bn_pre"
}
layer {
  name: "map32_70_conv1"
  type: "Convolution"
  bottom: "map32_70_bn_pre"
  top: "map32_70_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_70_bn1"
  type: "BatchNorm"
  bottom: "map32_70_conv1"
  top: "map32_70_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_70_bn1"
  type: "BatchNorm"
  bottom: "map32_70_conv1"
  top: "map32_70_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_70_scale1"
  type: "Scale"
  bottom: "map32_70_conv1"
  top: "map32_70_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_70_relu1"
  type: "ReLU"
  bottom: "map32_70_conv1"
  top: "map32_70_conv1"
}
layer {
  name: "map32_70_conv2"
  type: "Convolution"
  bottom: "map32_70_conv1"
  top: "map32_70_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_70_bn2"
  type: "BatchNorm"
  bottom: "map32_70_conv2"
  top: "map32_70_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_70_bn2"
  type: "BatchNorm"
  bottom: "map32_70_conv2"
  top: "map32_70_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_70_scale2"
  type: "Scale"
  bottom: "map32_70_conv2"
  top: "map32_70_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_70_relu2"
  type: "ReLU"
  bottom: "map32_70_conv2"
  top: "map32_70_conv2"
}
layer {
  name: "map32_70_conv_end"
  type: "Convolution"
  bottom: "map32_70_conv2"
  top: "map32_70_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_70_eltsum"
  type: "Eltwise"
  bottom: "map32_69_eltsum"
  bottom: "map32_70_conv_end"
  top: "map32_70_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_71_bn_pre"
  type: "BatchNorm"
  bottom: "map32_70_eltsum"
  top: "map32_71_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_71_bn_pre"
  type: "BatchNorm"
  bottom: "map32_70_eltsum"
  top: "map32_71_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_71_pre_scale"
  type: "Scale"
  bottom: "map32_71_bn_pre"
  top: "map32_71_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_71_pre_relu"
  type: "ReLU"
  bottom: "map32_71_bn_pre"
  top: "map32_71_bn_pre"
}
layer {
  name: "map32_71_conv1"
  type: "Convolution"
  bottom: "map32_71_bn_pre"
  top: "map32_71_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_71_bn1"
  type: "BatchNorm"
  bottom: "map32_71_conv1"
  top: "map32_71_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_71_bn1"
  type: "BatchNorm"
  bottom: "map32_71_conv1"
  top: "map32_71_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_71_scale1"
  type: "Scale"
  bottom: "map32_71_conv1"
  top: "map32_71_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_71_relu1"
  type: "ReLU"
  bottom: "map32_71_conv1"
  top: "map32_71_conv1"
}
layer {
  name: "map32_71_conv2"
  type: "Convolution"
  bottom: "map32_71_conv1"
  top: "map32_71_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_71_bn2"
  type: "BatchNorm"
  bottom: "map32_71_conv2"
  top: "map32_71_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_71_bn2"
  type: "BatchNorm"
  bottom: "map32_71_conv2"
  top: "map32_71_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_71_scale2"
  type: "Scale"
  bottom: "map32_71_conv2"
  top: "map32_71_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_71_relu2"
  type: "ReLU"
  bottom: "map32_71_conv2"
  top: "map32_71_conv2"
}
layer {
  name: "map32_71_conv_end"
  type: "Convolution"
  bottom: "map32_71_conv2"
  top: "map32_71_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_71_eltsum"
  type: "Eltwise"
  bottom: "map32_70_eltsum"
  bottom: "map32_71_conv_end"
  top: "map32_71_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_72_bn_pre"
  type: "BatchNorm"
  bottom: "map32_71_eltsum"
  top: "map32_72_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_72_bn_pre"
  type: "BatchNorm"
  bottom: "map32_71_eltsum"
  top: "map32_72_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_72_pre_scale"
  type: "Scale"
  bottom: "map32_72_bn_pre"
  top: "map32_72_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_72_pre_relu"
  type: "ReLU"
  bottom: "map32_72_bn_pre"
  top: "map32_72_bn_pre"
}
layer {
  name: "map32_72_conv1"
  type: "Convolution"
  bottom: "map32_72_bn_pre"
  top: "map32_72_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_72_bn1"
  type: "BatchNorm"
  bottom: "map32_72_conv1"
  top: "map32_72_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_72_bn1"
  type: "BatchNorm"
  bottom: "map32_72_conv1"
  top: "map32_72_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_72_scale1"
  type: "Scale"
  bottom: "map32_72_conv1"
  top: "map32_72_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_72_relu1"
  type: "ReLU"
  bottom: "map32_72_conv1"
  top: "map32_72_conv1"
}
layer {
  name: "map32_72_conv2"
  type: "Convolution"
  bottom: "map32_72_conv1"
  top: "map32_72_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_72_bn2"
  type: "BatchNorm"
  bottom: "map32_72_conv2"
  top: "map32_72_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_72_bn2"
  type: "BatchNorm"
  bottom: "map32_72_conv2"
  top: "map32_72_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_72_scale2"
  type: "Scale"
  bottom: "map32_72_conv2"
  top: "map32_72_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_72_relu2"
  type: "ReLU"
  bottom: "map32_72_conv2"
  top: "map32_72_conv2"
}
layer {
  name: "map32_72_conv_end"
  type: "Convolution"
  bottom: "map32_72_conv2"
  top: "map32_72_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_72_eltsum"
  type: "Eltwise"
  bottom: "map32_71_eltsum"
  bottom: "map32_72_conv_end"
  top: "map32_72_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_73_bn_pre"
  type: "BatchNorm"
  bottom: "map32_72_eltsum"
  top: "map32_73_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_73_bn_pre"
  type: "BatchNorm"
  bottom: "map32_72_eltsum"
  top: "map32_73_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_73_pre_scale"
  type: "Scale"
  bottom: "map32_73_bn_pre"
  top: "map32_73_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_73_pre_relu"
  type: "ReLU"
  bottom: "map32_73_bn_pre"
  top: "map32_73_bn_pre"
}
layer {
  name: "map32_73_conv1"
  type: "Convolution"
  bottom: "map32_73_bn_pre"
  top: "map32_73_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_73_bn1"
  type: "BatchNorm"
  bottom: "map32_73_conv1"
  top: "map32_73_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_73_bn1"
  type: "BatchNorm"
  bottom: "map32_73_conv1"
  top: "map32_73_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_73_scale1"
  type: "Scale"
  bottom: "map32_73_conv1"
  top: "map32_73_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_73_relu1"
  type: "ReLU"
  bottom: "map32_73_conv1"
  top: "map32_73_conv1"
}
layer {
  name: "map32_73_conv2"
  type: "Convolution"
  bottom: "map32_73_conv1"
  top: "map32_73_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_73_bn2"
  type: "BatchNorm"
  bottom: "map32_73_conv2"
  top: "map32_73_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_73_bn2"
  type: "BatchNorm"
  bottom: "map32_73_conv2"
  top: "map32_73_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_73_scale2"
  type: "Scale"
  bottom: "map32_73_conv2"
  top: "map32_73_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_73_relu2"
  type: "ReLU"
  bottom: "map32_73_conv2"
  top: "map32_73_conv2"
}
layer {
  name: "map32_73_conv_end"
  type: "Convolution"
  bottom: "map32_73_conv2"
  top: "map32_73_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_73_eltsum"
  type: "Eltwise"
  bottom: "map32_72_eltsum"
  bottom: "map32_73_conv_end"
  top: "map32_73_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_74_bn_pre"
  type: "BatchNorm"
  bottom: "map32_73_eltsum"
  top: "map32_74_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_74_bn_pre"
  type: "BatchNorm"
  bottom: "map32_73_eltsum"
  top: "map32_74_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_74_pre_scale"
  type: "Scale"
  bottom: "map32_74_bn_pre"
  top: "map32_74_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_74_pre_relu"
  type: "ReLU"
  bottom: "map32_74_bn_pre"
  top: "map32_74_bn_pre"
}
layer {
  name: "map32_74_conv1"
  type: "Convolution"
  bottom: "map32_74_bn_pre"
  top: "map32_74_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_74_bn1"
  type: "BatchNorm"
  bottom: "map32_74_conv1"
  top: "map32_74_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_74_bn1"
  type: "BatchNorm"
  bottom: "map32_74_conv1"
  top: "map32_74_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_74_scale1"
  type: "Scale"
  bottom: "map32_74_conv1"
  top: "map32_74_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_74_relu1"
  type: "ReLU"
  bottom: "map32_74_conv1"
  top: "map32_74_conv1"
}
layer {
  name: "map32_74_conv2"
  type: "Convolution"
  bottom: "map32_74_conv1"
  top: "map32_74_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_74_bn2"
  type: "BatchNorm"
  bottom: "map32_74_conv2"
  top: "map32_74_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_74_bn2"
  type: "BatchNorm"
  bottom: "map32_74_conv2"
  top: "map32_74_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_74_scale2"
  type: "Scale"
  bottom: "map32_74_conv2"
  top: "map32_74_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_74_relu2"
  type: "ReLU"
  bottom: "map32_74_conv2"
  top: "map32_74_conv2"
}
layer {
  name: "map32_74_conv_end"
  type: "Convolution"
  bottom: "map32_74_conv2"
  top: "map32_74_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_74_eltsum"
  type: "Eltwise"
  bottom: "map32_73_eltsum"
  bottom: "map32_74_conv_end"
  top: "map32_74_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_75_bn_pre"
  type: "BatchNorm"
  bottom: "map32_74_eltsum"
  top: "map32_75_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_75_bn_pre"
  type: "BatchNorm"
  bottom: "map32_74_eltsum"
  top: "map32_75_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_75_pre_scale"
  type: "Scale"
  bottom: "map32_75_bn_pre"
  top: "map32_75_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_75_pre_relu"
  type: "ReLU"
  bottom: "map32_75_bn_pre"
  top: "map32_75_bn_pre"
}
layer {
  name: "map32_75_conv1"
  type: "Convolution"
  bottom: "map32_75_bn_pre"
  top: "map32_75_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_75_bn1"
  type: "BatchNorm"
  bottom: "map32_75_conv1"
  top: "map32_75_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_75_bn1"
  type: "BatchNorm"
  bottom: "map32_75_conv1"
  top: "map32_75_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_75_scale1"
  type: "Scale"
  bottom: "map32_75_conv1"
  top: "map32_75_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_75_relu1"
  type: "ReLU"
  bottom: "map32_75_conv1"
  top: "map32_75_conv1"
}
layer {
  name: "map32_75_conv2"
  type: "Convolution"
  bottom: "map32_75_conv1"
  top: "map32_75_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_75_bn2"
  type: "BatchNorm"
  bottom: "map32_75_conv2"
  top: "map32_75_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_75_bn2"
  type: "BatchNorm"
  bottom: "map32_75_conv2"
  top: "map32_75_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_75_scale2"
  type: "Scale"
  bottom: "map32_75_conv2"
  top: "map32_75_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_75_relu2"
  type: "ReLU"
  bottom: "map32_75_conv2"
  top: "map32_75_conv2"
}
layer {
  name: "map32_75_conv_end"
  type: "Convolution"
  bottom: "map32_75_conv2"
  top: "map32_75_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_75_eltsum"
  type: "Eltwise"
  bottom: "map32_74_eltsum"
  bottom: "map32_75_conv_end"
  top: "map32_75_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_76_bn_pre"
  type: "BatchNorm"
  bottom: "map32_75_eltsum"
  top: "map32_76_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_76_bn_pre"
  type: "BatchNorm"
  bottom: "map32_75_eltsum"
  top: "map32_76_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_76_pre_scale"
  type: "Scale"
  bottom: "map32_76_bn_pre"
  top: "map32_76_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_76_pre_relu"
  type: "ReLU"
  bottom: "map32_76_bn_pre"
  top: "map32_76_bn_pre"
}
layer {
  name: "map32_76_conv1"
  type: "Convolution"
  bottom: "map32_76_bn_pre"
  top: "map32_76_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_76_bn1"
  type: "BatchNorm"
  bottom: "map32_76_conv1"
  top: "map32_76_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_76_bn1"
  type: "BatchNorm"
  bottom: "map32_76_conv1"
  top: "map32_76_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_76_scale1"
  type: "Scale"
  bottom: "map32_76_conv1"
  top: "map32_76_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_76_relu1"
  type: "ReLU"
  bottom: "map32_76_conv1"
  top: "map32_76_conv1"
}
layer {
  name: "map32_76_conv2"
  type: "Convolution"
  bottom: "map32_76_conv1"
  top: "map32_76_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_76_bn2"
  type: "BatchNorm"
  bottom: "map32_76_conv2"
  top: "map32_76_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_76_bn2"
  type: "BatchNorm"
  bottom: "map32_76_conv2"
  top: "map32_76_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_76_scale2"
  type: "Scale"
  bottom: "map32_76_conv2"
  top: "map32_76_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_76_relu2"
  type: "ReLU"
  bottom: "map32_76_conv2"
  top: "map32_76_conv2"
}
layer {
  name: "map32_76_conv_end"
  type: "Convolution"
  bottom: "map32_76_conv2"
  top: "map32_76_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_76_eltsum"
  type: "Eltwise"
  bottom: "map32_75_eltsum"
  bottom: "map32_76_conv_end"
  top: "map32_76_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_77_bn_pre"
  type: "BatchNorm"
  bottom: "map32_76_eltsum"
  top: "map32_77_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_77_bn_pre"
  type: "BatchNorm"
  bottom: "map32_76_eltsum"
  top: "map32_77_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_77_pre_scale"
  type: "Scale"
  bottom: "map32_77_bn_pre"
  top: "map32_77_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_77_pre_relu"
  type: "ReLU"
  bottom: "map32_77_bn_pre"
  top: "map32_77_bn_pre"
}
layer {
  name: "map32_77_conv1"
  type: "Convolution"
  bottom: "map32_77_bn_pre"
  top: "map32_77_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_77_bn1"
  type: "BatchNorm"
  bottom: "map32_77_conv1"
  top: "map32_77_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_77_bn1"
  type: "BatchNorm"
  bottom: "map32_77_conv1"
  top: "map32_77_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_77_scale1"
  type: "Scale"
  bottom: "map32_77_conv1"
  top: "map32_77_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_77_relu1"
  type: "ReLU"
  bottom: "map32_77_conv1"
  top: "map32_77_conv1"
}
layer {
  name: "map32_77_conv2"
  type: "Convolution"
  bottom: "map32_77_conv1"
  top: "map32_77_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_77_bn2"
  type: "BatchNorm"
  bottom: "map32_77_conv2"
  top: "map32_77_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_77_bn2"
  type: "BatchNorm"
  bottom: "map32_77_conv2"
  top: "map32_77_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_77_scale2"
  type: "Scale"
  bottom: "map32_77_conv2"
  top: "map32_77_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_77_relu2"
  type: "ReLU"
  bottom: "map32_77_conv2"
  top: "map32_77_conv2"
}
layer {
  name: "map32_77_conv_end"
  type: "Convolution"
  bottom: "map32_77_conv2"
  top: "map32_77_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_77_eltsum"
  type: "Eltwise"
  bottom: "map32_76_eltsum"
  bottom: "map32_77_conv_end"
  top: "map32_77_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_78_bn_pre"
  type: "BatchNorm"
  bottom: "map32_77_eltsum"
  top: "map32_78_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_78_bn_pre"
  type: "BatchNorm"
  bottom: "map32_77_eltsum"
  top: "map32_78_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_78_pre_scale"
  type: "Scale"
  bottom: "map32_78_bn_pre"
  top: "map32_78_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_78_pre_relu"
  type: "ReLU"
  bottom: "map32_78_bn_pre"
  top: "map32_78_bn_pre"
}
layer {
  name: "map32_78_conv1"
  type: "Convolution"
  bottom: "map32_78_bn_pre"
  top: "map32_78_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_78_bn1"
  type: "BatchNorm"
  bottom: "map32_78_conv1"
  top: "map32_78_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_78_bn1"
  type: "BatchNorm"
  bottom: "map32_78_conv1"
  top: "map32_78_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_78_scale1"
  type: "Scale"
  bottom: "map32_78_conv1"
  top: "map32_78_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_78_relu1"
  type: "ReLU"
  bottom: "map32_78_conv1"
  top: "map32_78_conv1"
}
layer {
  name: "map32_78_conv2"
  type: "Convolution"
  bottom: "map32_78_conv1"
  top: "map32_78_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_78_bn2"
  type: "BatchNorm"
  bottom: "map32_78_conv2"
  top: "map32_78_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_78_bn2"
  type: "BatchNorm"
  bottom: "map32_78_conv2"
  top: "map32_78_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_78_scale2"
  type: "Scale"
  bottom: "map32_78_conv2"
  top: "map32_78_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_78_relu2"
  type: "ReLU"
  bottom: "map32_78_conv2"
  top: "map32_78_conv2"
}
layer {
  name: "map32_78_conv_end"
  type: "Convolution"
  bottom: "map32_78_conv2"
  top: "map32_78_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_78_eltsum"
  type: "Eltwise"
  bottom: "map32_77_eltsum"
  bottom: "map32_78_conv_end"
  top: "map32_78_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_79_bn_pre"
  type: "BatchNorm"
  bottom: "map32_78_eltsum"
  top: "map32_79_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_79_bn_pre"
  type: "BatchNorm"
  bottom: "map32_78_eltsum"
  top: "map32_79_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_79_pre_scale"
  type: "Scale"
  bottom: "map32_79_bn_pre"
  top: "map32_79_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_79_pre_relu"
  type: "ReLU"
  bottom: "map32_79_bn_pre"
  top: "map32_79_bn_pre"
}
layer {
  name: "map32_79_conv1"
  type: "Convolution"
  bottom: "map32_79_bn_pre"
  top: "map32_79_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_79_bn1"
  type: "BatchNorm"
  bottom: "map32_79_conv1"
  top: "map32_79_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_79_bn1"
  type: "BatchNorm"
  bottom: "map32_79_conv1"
  top: "map32_79_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_79_scale1"
  type: "Scale"
  bottom: "map32_79_conv1"
  top: "map32_79_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_79_relu1"
  type: "ReLU"
  bottom: "map32_79_conv1"
  top: "map32_79_conv1"
}
layer {
  name: "map32_79_conv2"
  type: "Convolution"
  bottom: "map32_79_conv1"
  top: "map32_79_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_79_bn2"
  type: "BatchNorm"
  bottom: "map32_79_conv2"
  top: "map32_79_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_79_bn2"
  type: "BatchNorm"
  bottom: "map32_79_conv2"
  top: "map32_79_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_79_scale2"
  type: "Scale"
  bottom: "map32_79_conv2"
  top: "map32_79_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_79_relu2"
  type: "ReLU"
  bottom: "map32_79_conv2"
  top: "map32_79_conv2"
}
layer {
  name: "map32_79_conv_end"
  type: "Convolution"
  bottom: "map32_79_conv2"
  top: "map32_79_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_79_eltsum"
  type: "Eltwise"
  bottom: "map32_78_eltsum"
  bottom: "map32_79_conv_end"
  top: "map32_79_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_80_bn_pre"
  type: "BatchNorm"
  bottom: "map32_79_eltsum"
  top: "map32_80_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_80_bn_pre"
  type: "BatchNorm"
  bottom: "map32_79_eltsum"
  top: "map32_80_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_80_pre_scale"
  type: "Scale"
  bottom: "map32_80_bn_pre"
  top: "map32_80_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_80_pre_relu"
  type: "ReLU"
  bottom: "map32_80_bn_pre"
  top: "map32_80_bn_pre"
}
layer {
  name: "map32_80_conv1"
  type: "Convolution"
  bottom: "map32_80_bn_pre"
  top: "map32_80_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_80_bn1"
  type: "BatchNorm"
  bottom: "map32_80_conv1"
  top: "map32_80_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_80_bn1"
  type: "BatchNorm"
  bottom: "map32_80_conv1"
  top: "map32_80_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_80_scale1"
  type: "Scale"
  bottom: "map32_80_conv1"
  top: "map32_80_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_80_relu1"
  type: "ReLU"
  bottom: "map32_80_conv1"
  top: "map32_80_conv1"
}
layer {
  name: "map32_80_conv2"
  type: "Convolution"
  bottom: "map32_80_conv1"
  top: "map32_80_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_80_bn2"
  type: "BatchNorm"
  bottom: "map32_80_conv2"
  top: "map32_80_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_80_bn2"
  type: "BatchNorm"
  bottom: "map32_80_conv2"
  top: "map32_80_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_80_scale2"
  type: "Scale"
  bottom: "map32_80_conv2"
  top: "map32_80_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_80_relu2"
  type: "ReLU"
  bottom: "map32_80_conv2"
  top: "map32_80_conv2"
}
layer {
  name: "map32_80_conv_end"
  type: "Convolution"
  bottom: "map32_80_conv2"
  top: "map32_80_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_80_eltsum"
  type: "Eltwise"
  bottom: "map32_79_eltsum"
  bottom: "map32_80_conv_end"
  top: "map32_80_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_81_bn_pre"
  type: "BatchNorm"
  bottom: "map32_80_eltsum"
  top: "map32_81_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_81_bn_pre"
  type: "BatchNorm"
  bottom: "map32_80_eltsum"
  top: "map32_81_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_81_pre_scale"
  type: "Scale"
  bottom: "map32_81_bn_pre"
  top: "map32_81_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_81_pre_relu"
  type: "ReLU"
  bottom: "map32_81_bn_pre"
  top: "map32_81_bn_pre"
}
layer {
  name: "map32_81_conv1"
  type: "Convolution"
  bottom: "map32_81_bn_pre"
  top: "map32_81_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_81_bn1"
  type: "BatchNorm"
  bottom: "map32_81_conv1"
  top: "map32_81_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_81_bn1"
  type: "BatchNorm"
  bottom: "map32_81_conv1"
  top: "map32_81_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_81_scale1"
  type: "Scale"
  bottom: "map32_81_conv1"
  top: "map32_81_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_81_relu1"
  type: "ReLU"
  bottom: "map32_81_conv1"
  top: "map32_81_conv1"
}
layer {
  name: "map32_81_conv2"
  type: "Convolution"
  bottom: "map32_81_conv1"
  top: "map32_81_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_81_bn2"
  type: "BatchNorm"
  bottom: "map32_81_conv2"
  top: "map32_81_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_81_bn2"
  type: "BatchNorm"
  bottom: "map32_81_conv2"
  top: "map32_81_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_81_scale2"
  type: "Scale"
  bottom: "map32_81_conv2"
  top: "map32_81_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_81_relu2"
  type: "ReLU"
  bottom: "map32_81_conv2"
  top: "map32_81_conv2"
}
layer {
  name: "map32_81_conv_end"
  type: "Convolution"
  bottom: "map32_81_conv2"
  top: "map32_81_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_81_eltsum"
  type: "Eltwise"
  bottom: "map32_80_eltsum"
  bottom: "map32_81_conv_end"
  top: "map32_81_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_82_bn_pre"
  type: "BatchNorm"
  bottom: "map32_81_eltsum"
  top: "map32_82_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_82_bn_pre"
  type: "BatchNorm"
  bottom: "map32_81_eltsum"
  top: "map32_82_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_82_pre_scale"
  type: "Scale"
  bottom: "map32_82_bn_pre"
  top: "map32_82_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_82_pre_relu"
  type: "ReLU"
  bottom: "map32_82_bn_pre"
  top: "map32_82_bn_pre"
}
layer {
  name: "map32_82_conv1"
  type: "Convolution"
  bottom: "map32_82_bn_pre"
  top: "map32_82_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_82_bn1"
  type: "BatchNorm"
  bottom: "map32_82_conv1"
  top: "map32_82_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_82_bn1"
  type: "BatchNorm"
  bottom: "map32_82_conv1"
  top: "map32_82_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_82_scale1"
  type: "Scale"
  bottom: "map32_82_conv1"
  top: "map32_82_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_82_relu1"
  type: "ReLU"
  bottom: "map32_82_conv1"
  top: "map32_82_conv1"
}
layer {
  name: "map32_82_conv2"
  type: "Convolution"
  bottom: "map32_82_conv1"
  top: "map32_82_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_82_bn2"
  type: "BatchNorm"
  bottom: "map32_82_conv2"
  top: "map32_82_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_82_bn2"
  type: "BatchNorm"
  bottom: "map32_82_conv2"
  top: "map32_82_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_82_scale2"
  type: "Scale"
  bottom: "map32_82_conv2"
  top: "map32_82_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_82_relu2"
  type: "ReLU"
  bottom: "map32_82_conv2"
  top: "map32_82_conv2"
}
layer {
  name: "map32_82_conv_end"
  type: "Convolution"
  bottom: "map32_82_conv2"
  top: "map32_82_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_82_eltsum"
  type: "Eltwise"
  bottom: "map32_81_eltsum"
  bottom: "map32_82_conv_end"
  top: "map32_82_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_83_bn_pre"
  type: "BatchNorm"
  bottom: "map32_82_eltsum"
  top: "map32_83_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_83_bn_pre"
  type: "BatchNorm"
  bottom: "map32_82_eltsum"
  top: "map32_83_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_83_pre_scale"
  type: "Scale"
  bottom: "map32_83_bn_pre"
  top: "map32_83_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_83_pre_relu"
  type: "ReLU"
  bottom: "map32_83_bn_pre"
  top: "map32_83_bn_pre"
}
layer {
  name: "map32_83_conv1"
  type: "Convolution"
  bottom: "map32_83_bn_pre"
  top: "map32_83_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_83_bn1"
  type: "BatchNorm"
  bottom: "map32_83_conv1"
  top: "map32_83_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_83_bn1"
  type: "BatchNorm"
  bottom: "map32_83_conv1"
  top: "map32_83_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_83_scale1"
  type: "Scale"
  bottom: "map32_83_conv1"
  top: "map32_83_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_83_relu1"
  type: "ReLU"
  bottom: "map32_83_conv1"
  top: "map32_83_conv1"
}
layer {
  name: "map32_83_conv2"
  type: "Convolution"
  bottom: "map32_83_conv1"
  top: "map32_83_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_83_bn2"
  type: "BatchNorm"
  bottom: "map32_83_conv2"
  top: "map32_83_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_83_bn2"
  type: "BatchNorm"
  bottom: "map32_83_conv2"
  top: "map32_83_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_83_scale2"
  type: "Scale"
  bottom: "map32_83_conv2"
  top: "map32_83_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_83_relu2"
  type: "ReLU"
  bottom: "map32_83_conv2"
  top: "map32_83_conv2"
}
layer {
  name: "map32_83_conv_end"
  type: "Convolution"
  bottom: "map32_83_conv2"
  top: "map32_83_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_83_eltsum"
  type: "Eltwise"
  bottom: "map32_82_eltsum"
  bottom: "map32_83_conv_end"
  top: "map32_83_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_84_bn_pre"
  type: "BatchNorm"
  bottom: "map32_83_eltsum"
  top: "map32_84_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_84_bn_pre"
  type: "BatchNorm"
  bottom: "map32_83_eltsum"
  top: "map32_84_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_84_pre_scale"
  type: "Scale"
  bottom: "map32_84_bn_pre"
  top: "map32_84_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_84_pre_relu"
  type: "ReLU"
  bottom: "map32_84_bn_pre"
  top: "map32_84_bn_pre"
}
layer {
  name: "map32_84_conv1"
  type: "Convolution"
  bottom: "map32_84_bn_pre"
  top: "map32_84_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_84_bn1"
  type: "BatchNorm"
  bottom: "map32_84_conv1"
  top: "map32_84_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_84_bn1"
  type: "BatchNorm"
  bottom: "map32_84_conv1"
  top: "map32_84_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_84_scale1"
  type: "Scale"
  bottom: "map32_84_conv1"
  top: "map32_84_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_84_relu1"
  type: "ReLU"
  bottom: "map32_84_conv1"
  top: "map32_84_conv1"
}
layer {
  name: "map32_84_conv2"
  type: "Convolution"
  bottom: "map32_84_conv1"
  top: "map32_84_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_84_bn2"
  type: "BatchNorm"
  bottom: "map32_84_conv2"
  top: "map32_84_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_84_bn2"
  type: "BatchNorm"
  bottom: "map32_84_conv2"
  top: "map32_84_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_84_scale2"
  type: "Scale"
  bottom: "map32_84_conv2"
  top: "map32_84_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_84_relu2"
  type: "ReLU"
  bottom: "map32_84_conv2"
  top: "map32_84_conv2"
}
layer {
  name: "map32_84_conv_end"
  type: "Convolution"
  bottom: "map32_84_conv2"
  top: "map32_84_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_84_eltsum"
  type: "Eltwise"
  bottom: "map32_83_eltsum"
  bottom: "map32_84_conv_end"
  top: "map32_84_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_85_bn_pre"
  type: "BatchNorm"
  bottom: "map32_84_eltsum"
  top: "map32_85_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_85_bn_pre"
  type: "BatchNorm"
  bottom: "map32_84_eltsum"
  top: "map32_85_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_85_pre_scale"
  type: "Scale"
  bottom: "map32_85_bn_pre"
  top: "map32_85_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_85_pre_relu"
  type: "ReLU"
  bottom: "map32_85_bn_pre"
  top: "map32_85_bn_pre"
}
layer {
  name: "map32_85_conv1"
  type: "Convolution"
  bottom: "map32_85_bn_pre"
  top: "map32_85_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_85_bn1"
  type: "BatchNorm"
  bottom: "map32_85_conv1"
  top: "map32_85_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_85_bn1"
  type: "BatchNorm"
  bottom: "map32_85_conv1"
  top: "map32_85_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_85_scale1"
  type: "Scale"
  bottom: "map32_85_conv1"
  top: "map32_85_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_85_relu1"
  type: "ReLU"
  bottom: "map32_85_conv1"
  top: "map32_85_conv1"
}
layer {
  name: "map32_85_conv2"
  type: "Convolution"
  bottom: "map32_85_conv1"
  top: "map32_85_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_85_bn2"
  type: "BatchNorm"
  bottom: "map32_85_conv2"
  top: "map32_85_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_85_bn2"
  type: "BatchNorm"
  bottom: "map32_85_conv2"
  top: "map32_85_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_85_scale2"
  type: "Scale"
  bottom: "map32_85_conv2"
  top: "map32_85_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_85_relu2"
  type: "ReLU"
  bottom: "map32_85_conv2"
  top: "map32_85_conv2"
}
layer {
  name: "map32_85_conv_end"
  type: "Convolution"
  bottom: "map32_85_conv2"
  top: "map32_85_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_85_eltsum"
  type: "Eltwise"
  bottom: "map32_84_eltsum"
  bottom: "map32_85_conv_end"
  top: "map32_85_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_86_bn_pre"
  type: "BatchNorm"
  bottom: "map32_85_eltsum"
  top: "map32_86_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_86_bn_pre"
  type: "BatchNorm"
  bottom: "map32_85_eltsum"
  top: "map32_86_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_86_pre_scale"
  type: "Scale"
  bottom: "map32_86_bn_pre"
  top: "map32_86_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_86_pre_relu"
  type: "ReLU"
  bottom: "map32_86_bn_pre"
  top: "map32_86_bn_pre"
}
layer {
  name: "map32_86_conv1"
  type: "Convolution"
  bottom: "map32_86_bn_pre"
  top: "map32_86_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_86_bn1"
  type: "BatchNorm"
  bottom: "map32_86_conv1"
  top: "map32_86_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_86_bn1"
  type: "BatchNorm"
  bottom: "map32_86_conv1"
  top: "map32_86_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_86_scale1"
  type: "Scale"
  bottom: "map32_86_conv1"
  top: "map32_86_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_86_relu1"
  type: "ReLU"
  bottom: "map32_86_conv1"
  top: "map32_86_conv1"
}
layer {
  name: "map32_86_conv2"
  type: "Convolution"
  bottom: "map32_86_conv1"
  top: "map32_86_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_86_bn2"
  type: "BatchNorm"
  bottom: "map32_86_conv2"
  top: "map32_86_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_86_bn2"
  type: "BatchNorm"
  bottom: "map32_86_conv2"
  top: "map32_86_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_86_scale2"
  type: "Scale"
  bottom: "map32_86_conv2"
  top: "map32_86_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_86_relu2"
  type: "ReLU"
  bottom: "map32_86_conv2"
  top: "map32_86_conv2"
}
layer {
  name: "map32_86_conv_end"
  type: "Convolution"
  bottom: "map32_86_conv2"
  top: "map32_86_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_86_eltsum"
  type: "Eltwise"
  bottom: "map32_85_eltsum"
  bottom: "map32_86_conv_end"
  top: "map32_86_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_87_bn_pre"
  type: "BatchNorm"
  bottom: "map32_86_eltsum"
  top: "map32_87_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_87_bn_pre"
  type: "BatchNorm"
  bottom: "map32_86_eltsum"
  top: "map32_87_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_87_pre_scale"
  type: "Scale"
  bottom: "map32_87_bn_pre"
  top: "map32_87_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_87_pre_relu"
  type: "ReLU"
  bottom: "map32_87_bn_pre"
  top: "map32_87_bn_pre"
}
layer {
  name: "map32_87_conv1"
  type: "Convolution"
  bottom: "map32_87_bn_pre"
  top: "map32_87_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_87_bn1"
  type: "BatchNorm"
  bottom: "map32_87_conv1"
  top: "map32_87_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_87_bn1"
  type: "BatchNorm"
  bottom: "map32_87_conv1"
  top: "map32_87_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_87_scale1"
  type: "Scale"
  bottom: "map32_87_conv1"
  top: "map32_87_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_87_relu1"
  type: "ReLU"
  bottom: "map32_87_conv1"
  top: "map32_87_conv1"
}
layer {
  name: "map32_87_conv2"
  type: "Convolution"
  bottom: "map32_87_conv1"
  top: "map32_87_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_87_bn2"
  type: "BatchNorm"
  bottom: "map32_87_conv2"
  top: "map32_87_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_87_bn2"
  type: "BatchNorm"
  bottom: "map32_87_conv2"
  top: "map32_87_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_87_scale2"
  type: "Scale"
  bottom: "map32_87_conv2"
  top: "map32_87_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_87_relu2"
  type: "ReLU"
  bottom: "map32_87_conv2"
  top: "map32_87_conv2"
}
layer {
  name: "map32_87_conv_end"
  type: "Convolution"
  bottom: "map32_87_conv2"
  top: "map32_87_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_87_eltsum"
  type: "Eltwise"
  bottom: "map32_86_eltsum"
  bottom: "map32_87_conv_end"
  top: "map32_87_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_88_bn_pre"
  type: "BatchNorm"
  bottom: "map32_87_eltsum"
  top: "map32_88_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_88_bn_pre"
  type: "BatchNorm"
  bottom: "map32_87_eltsum"
  top: "map32_88_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_88_pre_scale"
  type: "Scale"
  bottom: "map32_88_bn_pre"
  top: "map32_88_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_88_pre_relu"
  type: "ReLU"
  bottom: "map32_88_bn_pre"
  top: "map32_88_bn_pre"
}
layer {
  name: "map32_88_conv1"
  type: "Convolution"
  bottom: "map32_88_bn_pre"
  top: "map32_88_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_88_bn1"
  type: "BatchNorm"
  bottom: "map32_88_conv1"
  top: "map32_88_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_88_bn1"
  type: "BatchNorm"
  bottom: "map32_88_conv1"
  top: "map32_88_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_88_scale1"
  type: "Scale"
  bottom: "map32_88_conv1"
  top: "map32_88_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_88_relu1"
  type: "ReLU"
  bottom: "map32_88_conv1"
  top: "map32_88_conv1"
}
layer {
  name: "map32_88_conv2"
  type: "Convolution"
  bottom: "map32_88_conv1"
  top: "map32_88_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_88_bn2"
  type: "BatchNorm"
  bottom: "map32_88_conv2"
  top: "map32_88_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_88_bn2"
  type: "BatchNorm"
  bottom: "map32_88_conv2"
  top: "map32_88_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_88_scale2"
  type: "Scale"
  bottom: "map32_88_conv2"
  top: "map32_88_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_88_relu2"
  type: "ReLU"
  bottom: "map32_88_conv2"
  top: "map32_88_conv2"
}
layer {
  name: "map32_88_conv_end"
  type: "Convolution"
  bottom: "map32_88_conv2"
  top: "map32_88_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_88_eltsum"
  type: "Eltwise"
  bottom: "map32_87_eltsum"
  bottom: "map32_88_conv_end"
  top: "map32_88_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_89_bn_pre"
  type: "BatchNorm"
  bottom: "map32_88_eltsum"
  top: "map32_89_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_89_bn_pre"
  type: "BatchNorm"
  bottom: "map32_88_eltsum"
  top: "map32_89_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_89_pre_scale"
  type: "Scale"
  bottom: "map32_89_bn_pre"
  top: "map32_89_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_89_pre_relu"
  type: "ReLU"
  bottom: "map32_89_bn_pre"
  top: "map32_89_bn_pre"
}
layer {
  name: "map32_89_conv1"
  type: "Convolution"
  bottom: "map32_89_bn_pre"
  top: "map32_89_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_89_bn1"
  type: "BatchNorm"
  bottom: "map32_89_conv1"
  top: "map32_89_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_89_bn1"
  type: "BatchNorm"
  bottom: "map32_89_conv1"
  top: "map32_89_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_89_scale1"
  type: "Scale"
  bottom: "map32_89_conv1"
  top: "map32_89_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_89_relu1"
  type: "ReLU"
  bottom: "map32_89_conv1"
  top: "map32_89_conv1"
}
layer {
  name: "map32_89_conv2"
  type: "Convolution"
  bottom: "map32_89_conv1"
  top: "map32_89_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_89_bn2"
  type: "BatchNorm"
  bottom: "map32_89_conv2"
  top: "map32_89_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_89_bn2"
  type: "BatchNorm"
  bottom: "map32_89_conv2"
  top: "map32_89_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_89_scale2"
  type: "Scale"
  bottom: "map32_89_conv2"
  top: "map32_89_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_89_relu2"
  type: "ReLU"
  bottom: "map32_89_conv2"
  top: "map32_89_conv2"
}
layer {
  name: "map32_89_conv_end"
  type: "Convolution"
  bottom: "map32_89_conv2"
  top: "map32_89_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_89_eltsum"
  type: "Eltwise"
  bottom: "map32_88_eltsum"
  bottom: "map32_89_conv_end"
  top: "map32_89_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_90_bn_pre"
  type: "BatchNorm"
  bottom: "map32_89_eltsum"
  top: "map32_90_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_90_bn_pre"
  type: "BatchNorm"
  bottom: "map32_89_eltsum"
  top: "map32_90_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_90_pre_scale"
  type: "Scale"
  bottom: "map32_90_bn_pre"
  top: "map32_90_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_90_pre_relu"
  type: "ReLU"
  bottom: "map32_90_bn_pre"
  top: "map32_90_bn_pre"
}
layer {
  name: "map32_90_conv1"
  type: "Convolution"
  bottom: "map32_90_bn_pre"
  top: "map32_90_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_90_bn1"
  type: "BatchNorm"
  bottom: "map32_90_conv1"
  top: "map32_90_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_90_bn1"
  type: "BatchNorm"
  bottom: "map32_90_conv1"
  top: "map32_90_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_90_scale1"
  type: "Scale"
  bottom: "map32_90_conv1"
  top: "map32_90_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_90_relu1"
  type: "ReLU"
  bottom: "map32_90_conv1"
  top: "map32_90_conv1"
}
layer {
  name: "map32_90_conv2"
  type: "Convolution"
  bottom: "map32_90_conv1"
  top: "map32_90_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_90_bn2"
  type: "BatchNorm"
  bottom: "map32_90_conv2"
  top: "map32_90_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_90_bn2"
  type: "BatchNorm"
  bottom: "map32_90_conv2"
  top: "map32_90_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_90_scale2"
  type: "Scale"
  bottom: "map32_90_conv2"
  top: "map32_90_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_90_relu2"
  type: "ReLU"
  bottom: "map32_90_conv2"
  top: "map32_90_conv2"
}
layer {
  name: "map32_90_conv_end"
  type: "Convolution"
  bottom: "map32_90_conv2"
  top: "map32_90_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_90_eltsum"
  type: "Eltwise"
  bottom: "map32_89_eltsum"
  bottom: "map32_90_conv_end"
  top: "map32_90_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_91_bn_pre"
  type: "BatchNorm"
  bottom: "map32_90_eltsum"
  top: "map32_91_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_91_bn_pre"
  type: "BatchNorm"
  bottom: "map32_90_eltsum"
  top: "map32_91_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_91_pre_scale"
  type: "Scale"
  bottom: "map32_91_bn_pre"
  top: "map32_91_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_91_pre_relu"
  type: "ReLU"
  bottom: "map32_91_bn_pre"
  top: "map32_91_bn_pre"
}
layer {
  name: "map32_91_conv1"
  type: "Convolution"
  bottom: "map32_91_bn_pre"
  top: "map32_91_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_91_bn1"
  type: "BatchNorm"
  bottom: "map32_91_conv1"
  top: "map32_91_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_91_bn1"
  type: "BatchNorm"
  bottom: "map32_91_conv1"
  top: "map32_91_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_91_scale1"
  type: "Scale"
  bottom: "map32_91_conv1"
  top: "map32_91_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_91_relu1"
  type: "ReLU"
  bottom: "map32_91_conv1"
  top: "map32_91_conv1"
}
layer {
  name: "map32_91_conv2"
  type: "Convolution"
  bottom: "map32_91_conv1"
  top: "map32_91_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_91_bn2"
  type: "BatchNorm"
  bottom: "map32_91_conv2"
  top: "map32_91_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_91_bn2"
  type: "BatchNorm"
  bottom: "map32_91_conv2"
  top: "map32_91_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_91_scale2"
  type: "Scale"
  bottom: "map32_91_conv2"
  top: "map32_91_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_91_relu2"
  type: "ReLU"
  bottom: "map32_91_conv2"
  top: "map32_91_conv2"
}
layer {
  name: "map32_91_conv_end"
  type: "Convolution"
  bottom: "map32_91_conv2"
  top: "map32_91_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_91_eltsum"
  type: "Eltwise"
  bottom: "map32_90_eltsum"
  bottom: "map32_91_conv_end"
  top: "map32_91_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_92_bn_pre"
  type: "BatchNorm"
  bottom: "map32_91_eltsum"
  top: "map32_92_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_92_bn_pre"
  type: "BatchNorm"
  bottom: "map32_91_eltsum"
  top: "map32_92_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_92_pre_scale"
  type: "Scale"
  bottom: "map32_92_bn_pre"
  top: "map32_92_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_92_pre_relu"
  type: "ReLU"
  bottom: "map32_92_bn_pre"
  top: "map32_92_bn_pre"
}
layer {
  name: "map32_92_conv1"
  type: "Convolution"
  bottom: "map32_92_bn_pre"
  top: "map32_92_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_92_bn1"
  type: "BatchNorm"
  bottom: "map32_92_conv1"
  top: "map32_92_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_92_bn1"
  type: "BatchNorm"
  bottom: "map32_92_conv1"
  top: "map32_92_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_92_scale1"
  type: "Scale"
  bottom: "map32_92_conv1"
  top: "map32_92_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_92_relu1"
  type: "ReLU"
  bottom: "map32_92_conv1"
  top: "map32_92_conv1"
}
layer {
  name: "map32_92_conv2"
  type: "Convolution"
  bottom: "map32_92_conv1"
  top: "map32_92_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_92_bn2"
  type: "BatchNorm"
  bottom: "map32_92_conv2"
  top: "map32_92_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_92_bn2"
  type: "BatchNorm"
  bottom: "map32_92_conv2"
  top: "map32_92_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_92_scale2"
  type: "Scale"
  bottom: "map32_92_conv2"
  top: "map32_92_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_92_relu2"
  type: "ReLU"
  bottom: "map32_92_conv2"
  top: "map32_92_conv2"
}
layer {
  name: "map32_92_conv_end"
  type: "Convolution"
  bottom: "map32_92_conv2"
  top: "map32_92_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_92_eltsum"
  type: "Eltwise"
  bottom: "map32_91_eltsum"
  bottom: "map32_92_conv_end"
  top: "map32_92_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_93_bn_pre"
  type: "BatchNorm"
  bottom: "map32_92_eltsum"
  top: "map32_93_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_93_bn_pre"
  type: "BatchNorm"
  bottom: "map32_92_eltsum"
  top: "map32_93_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_93_pre_scale"
  type: "Scale"
  bottom: "map32_93_bn_pre"
  top: "map32_93_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_93_pre_relu"
  type: "ReLU"
  bottom: "map32_93_bn_pre"
  top: "map32_93_bn_pre"
}
layer {
  name: "map32_93_conv1"
  type: "Convolution"
  bottom: "map32_93_bn_pre"
  top: "map32_93_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_93_bn1"
  type: "BatchNorm"
  bottom: "map32_93_conv1"
  top: "map32_93_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_93_bn1"
  type: "BatchNorm"
  bottom: "map32_93_conv1"
  top: "map32_93_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_93_scale1"
  type: "Scale"
  bottom: "map32_93_conv1"
  top: "map32_93_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_93_relu1"
  type: "ReLU"
  bottom: "map32_93_conv1"
  top: "map32_93_conv1"
}
layer {
  name: "map32_93_conv2"
  type: "Convolution"
  bottom: "map32_93_conv1"
  top: "map32_93_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_93_bn2"
  type: "BatchNorm"
  bottom: "map32_93_conv2"
  top: "map32_93_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_93_bn2"
  type: "BatchNorm"
  bottom: "map32_93_conv2"
  top: "map32_93_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_93_scale2"
  type: "Scale"
  bottom: "map32_93_conv2"
  top: "map32_93_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_93_relu2"
  type: "ReLU"
  bottom: "map32_93_conv2"
  top: "map32_93_conv2"
}
layer {
  name: "map32_93_conv_end"
  type: "Convolution"
  bottom: "map32_93_conv2"
  top: "map32_93_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_93_eltsum"
  type: "Eltwise"
  bottom: "map32_92_eltsum"
  bottom: "map32_93_conv_end"
  top: "map32_93_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_94_bn_pre"
  type: "BatchNorm"
  bottom: "map32_93_eltsum"
  top: "map32_94_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_94_bn_pre"
  type: "BatchNorm"
  bottom: "map32_93_eltsum"
  top: "map32_94_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_94_pre_scale"
  type: "Scale"
  bottom: "map32_94_bn_pre"
  top: "map32_94_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_94_pre_relu"
  type: "ReLU"
  bottom: "map32_94_bn_pre"
  top: "map32_94_bn_pre"
}
layer {
  name: "map32_94_conv1"
  type: "Convolution"
  bottom: "map32_94_bn_pre"
  top: "map32_94_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_94_bn1"
  type: "BatchNorm"
  bottom: "map32_94_conv1"
  top: "map32_94_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_94_bn1"
  type: "BatchNorm"
  bottom: "map32_94_conv1"
  top: "map32_94_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_94_scale1"
  type: "Scale"
  bottom: "map32_94_conv1"
  top: "map32_94_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_94_relu1"
  type: "ReLU"
  bottom: "map32_94_conv1"
  top: "map32_94_conv1"
}
layer {
  name: "map32_94_conv2"
  type: "Convolution"
  bottom: "map32_94_conv1"
  top: "map32_94_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_94_bn2"
  type: "BatchNorm"
  bottom: "map32_94_conv2"
  top: "map32_94_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_94_bn2"
  type: "BatchNorm"
  bottom: "map32_94_conv2"
  top: "map32_94_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_94_scale2"
  type: "Scale"
  bottom: "map32_94_conv2"
  top: "map32_94_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_94_relu2"
  type: "ReLU"
  bottom: "map32_94_conv2"
  top: "map32_94_conv2"
}
layer {
  name: "map32_94_conv_end"
  type: "Convolution"
  bottom: "map32_94_conv2"
  top: "map32_94_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_94_eltsum"
  type: "Eltwise"
  bottom: "map32_93_eltsum"
  bottom: "map32_94_conv_end"
  top: "map32_94_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_95_bn_pre"
  type: "BatchNorm"
  bottom: "map32_94_eltsum"
  top: "map32_95_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_95_bn_pre"
  type: "BatchNorm"
  bottom: "map32_94_eltsum"
  top: "map32_95_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_95_pre_scale"
  type: "Scale"
  bottom: "map32_95_bn_pre"
  top: "map32_95_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_95_pre_relu"
  type: "ReLU"
  bottom: "map32_95_bn_pre"
  top: "map32_95_bn_pre"
}
layer {
  name: "map32_95_conv1"
  type: "Convolution"
  bottom: "map32_95_bn_pre"
  top: "map32_95_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_95_bn1"
  type: "BatchNorm"
  bottom: "map32_95_conv1"
  top: "map32_95_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_95_bn1"
  type: "BatchNorm"
  bottom: "map32_95_conv1"
  top: "map32_95_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_95_scale1"
  type: "Scale"
  bottom: "map32_95_conv1"
  top: "map32_95_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_95_relu1"
  type: "ReLU"
  bottom: "map32_95_conv1"
  top: "map32_95_conv1"
}
layer {
  name: "map32_95_conv2"
  type: "Convolution"
  bottom: "map32_95_conv1"
  top: "map32_95_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_95_bn2"
  type: "BatchNorm"
  bottom: "map32_95_conv2"
  top: "map32_95_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_95_bn2"
  type: "BatchNorm"
  bottom: "map32_95_conv2"
  top: "map32_95_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_95_scale2"
  type: "Scale"
  bottom: "map32_95_conv2"
  top: "map32_95_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_95_relu2"
  type: "ReLU"
  bottom: "map32_95_conv2"
  top: "map32_95_conv2"
}
layer {
  name: "map32_95_conv_end"
  type: "Convolution"
  bottom: "map32_95_conv2"
  top: "map32_95_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_95_eltsum"
  type: "Eltwise"
  bottom: "map32_94_eltsum"
  bottom: "map32_95_conv_end"
  top: "map32_95_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_96_bn_pre"
  type: "BatchNorm"
  bottom: "map32_95_eltsum"
  top: "map32_96_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_96_bn_pre"
  type: "BatchNorm"
  bottom: "map32_95_eltsum"
  top: "map32_96_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_96_pre_scale"
  type: "Scale"
  bottom: "map32_96_bn_pre"
  top: "map32_96_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_96_pre_relu"
  type: "ReLU"
  bottom: "map32_96_bn_pre"
  top: "map32_96_bn_pre"
}
layer {
  name: "map32_96_conv1"
  type: "Convolution"
  bottom: "map32_96_bn_pre"
  top: "map32_96_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_96_bn1"
  type: "BatchNorm"
  bottom: "map32_96_conv1"
  top: "map32_96_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_96_bn1"
  type: "BatchNorm"
  bottom: "map32_96_conv1"
  top: "map32_96_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_96_scale1"
  type: "Scale"
  bottom: "map32_96_conv1"
  top: "map32_96_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_96_relu1"
  type: "ReLU"
  bottom: "map32_96_conv1"
  top: "map32_96_conv1"
}
layer {
  name: "map32_96_conv2"
  type: "Convolution"
  bottom: "map32_96_conv1"
  top: "map32_96_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_96_bn2"
  type: "BatchNorm"
  bottom: "map32_96_conv2"
  top: "map32_96_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_96_bn2"
  type: "BatchNorm"
  bottom: "map32_96_conv2"
  top: "map32_96_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_96_scale2"
  type: "Scale"
  bottom: "map32_96_conv2"
  top: "map32_96_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_96_relu2"
  type: "ReLU"
  bottom: "map32_96_conv2"
  top: "map32_96_conv2"
}
layer {
  name: "map32_96_conv_end"
  type: "Convolution"
  bottom: "map32_96_conv2"
  top: "map32_96_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_96_eltsum"
  type: "Eltwise"
  bottom: "map32_95_eltsum"
  bottom: "map32_96_conv_end"
  top: "map32_96_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_97_bn_pre"
  type: "BatchNorm"
  bottom: "map32_96_eltsum"
  top: "map32_97_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_97_bn_pre"
  type: "BatchNorm"
  bottom: "map32_96_eltsum"
  top: "map32_97_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_97_pre_scale"
  type: "Scale"
  bottom: "map32_97_bn_pre"
  top: "map32_97_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_97_pre_relu"
  type: "ReLU"
  bottom: "map32_97_bn_pre"
  top: "map32_97_bn_pre"
}
layer {
  name: "map32_97_conv1"
  type: "Convolution"
  bottom: "map32_97_bn_pre"
  top: "map32_97_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_97_bn1"
  type: "BatchNorm"
  bottom: "map32_97_conv1"
  top: "map32_97_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_97_bn1"
  type: "BatchNorm"
  bottom: "map32_97_conv1"
  top: "map32_97_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_97_scale1"
  type: "Scale"
  bottom: "map32_97_conv1"
  top: "map32_97_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_97_relu1"
  type: "ReLU"
  bottom: "map32_97_conv1"
  top: "map32_97_conv1"
}
layer {
  name: "map32_97_conv2"
  type: "Convolution"
  bottom: "map32_97_conv1"
  top: "map32_97_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_97_bn2"
  type: "BatchNorm"
  bottom: "map32_97_conv2"
  top: "map32_97_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_97_bn2"
  type: "BatchNorm"
  bottom: "map32_97_conv2"
  top: "map32_97_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_97_scale2"
  type: "Scale"
  bottom: "map32_97_conv2"
  top: "map32_97_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_97_relu2"
  type: "ReLU"
  bottom: "map32_97_conv2"
  top: "map32_97_conv2"
}
layer {
  name: "map32_97_conv_end"
  type: "Convolution"
  bottom: "map32_97_conv2"
  top: "map32_97_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_97_eltsum"
  type: "Eltwise"
  bottom: "map32_96_eltsum"
  bottom: "map32_97_conv_end"
  top: "map32_97_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_98_bn_pre"
  type: "BatchNorm"
  bottom: "map32_97_eltsum"
  top: "map32_98_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_98_bn_pre"
  type: "BatchNorm"
  bottom: "map32_97_eltsum"
  top: "map32_98_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_98_pre_scale"
  type: "Scale"
  bottom: "map32_98_bn_pre"
  top: "map32_98_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_98_pre_relu"
  type: "ReLU"
  bottom: "map32_98_bn_pre"
  top: "map32_98_bn_pre"
}
layer {
  name: "map32_98_conv1"
  type: "Convolution"
  bottom: "map32_98_bn_pre"
  top: "map32_98_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_98_bn1"
  type: "BatchNorm"
  bottom: "map32_98_conv1"
  top: "map32_98_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_98_bn1"
  type: "BatchNorm"
  bottom: "map32_98_conv1"
  top: "map32_98_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_98_scale1"
  type: "Scale"
  bottom: "map32_98_conv1"
  top: "map32_98_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_98_relu1"
  type: "ReLU"
  bottom: "map32_98_conv1"
  top: "map32_98_conv1"
}
layer {
  name: "map32_98_conv2"
  type: "Convolution"
  bottom: "map32_98_conv1"
  top: "map32_98_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_98_bn2"
  type: "BatchNorm"
  bottom: "map32_98_conv2"
  top: "map32_98_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_98_bn2"
  type: "BatchNorm"
  bottom: "map32_98_conv2"
  top: "map32_98_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_98_scale2"
  type: "Scale"
  bottom: "map32_98_conv2"
  top: "map32_98_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_98_relu2"
  type: "ReLU"
  bottom: "map32_98_conv2"
  top: "map32_98_conv2"
}
layer {
  name: "map32_98_conv_end"
  type: "Convolution"
  bottom: "map32_98_conv2"
  top: "map32_98_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_98_eltsum"
  type: "Eltwise"
  bottom: "map32_97_eltsum"
  bottom: "map32_98_conv_end"
  top: "map32_98_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_99_bn_pre"
  type: "BatchNorm"
  bottom: "map32_98_eltsum"
  top: "map32_99_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_99_bn_pre"
  type: "BatchNorm"
  bottom: "map32_98_eltsum"
  top: "map32_99_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_99_pre_scale"
  type: "Scale"
  bottom: "map32_99_bn_pre"
  top: "map32_99_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_99_pre_relu"
  type: "ReLU"
  bottom: "map32_99_bn_pre"
  top: "map32_99_bn_pre"
}
layer {
  name: "map32_99_conv1"
  type: "Convolution"
  bottom: "map32_99_bn_pre"
  top: "map32_99_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_99_bn1"
  type: "BatchNorm"
  bottom: "map32_99_conv1"
  top: "map32_99_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_99_bn1"
  type: "BatchNorm"
  bottom: "map32_99_conv1"
  top: "map32_99_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_99_scale1"
  type: "Scale"
  bottom: "map32_99_conv1"
  top: "map32_99_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_99_relu1"
  type: "ReLU"
  bottom: "map32_99_conv1"
  top: "map32_99_conv1"
}
layer {
  name: "map32_99_conv2"
  type: "Convolution"
  bottom: "map32_99_conv1"
  top: "map32_99_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_99_bn2"
  type: "BatchNorm"
  bottom: "map32_99_conv2"
  top: "map32_99_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_99_bn2"
  type: "BatchNorm"
  bottom: "map32_99_conv2"
  top: "map32_99_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_99_scale2"
  type: "Scale"
  bottom: "map32_99_conv2"
  top: "map32_99_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_99_relu2"
  type: "ReLU"
  bottom: "map32_99_conv2"
  top: "map32_99_conv2"
}
layer {
  name: "map32_99_conv_end"
  type: "Convolution"
  bottom: "map32_99_conv2"
  top: "map32_99_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_99_eltsum"
  type: "Eltwise"
  bottom: "map32_98_eltsum"
  bottom: "map32_99_conv_end"
  top: "map32_99_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_100_bn_pre"
  type: "BatchNorm"
  bottom: "map32_99_eltsum"
  top: "map32_100_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_100_bn_pre"
  type: "BatchNorm"
  bottom: "map32_99_eltsum"
  top: "map32_100_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_100_pre_scale"
  type: "Scale"
  bottom: "map32_100_bn_pre"
  top: "map32_100_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_100_pre_relu"
  type: "ReLU"
  bottom: "map32_100_bn_pre"
  top: "map32_100_bn_pre"
}
layer {
  name: "map32_100_conv1"
  type: "Convolution"
  bottom: "map32_100_bn_pre"
  top: "map32_100_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_100_bn1"
  type: "BatchNorm"
  bottom: "map32_100_conv1"
  top: "map32_100_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_100_bn1"
  type: "BatchNorm"
  bottom: "map32_100_conv1"
  top: "map32_100_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_100_scale1"
  type: "Scale"
  bottom: "map32_100_conv1"
  top: "map32_100_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_100_relu1"
  type: "ReLU"
  bottom: "map32_100_conv1"
  top: "map32_100_conv1"
}
layer {
  name: "map32_100_conv2"
  type: "Convolution"
  bottom: "map32_100_conv1"
  top: "map32_100_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_100_bn2"
  type: "BatchNorm"
  bottom: "map32_100_conv2"
  top: "map32_100_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_100_bn2"
  type: "BatchNorm"
  bottom: "map32_100_conv2"
  top: "map32_100_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_100_scale2"
  type: "Scale"
  bottom: "map32_100_conv2"
  top: "map32_100_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_100_relu2"
  type: "ReLU"
  bottom: "map32_100_conv2"
  top: "map32_100_conv2"
}
layer {
  name: "map32_100_conv_end"
  type: "Convolution"
  bottom: "map32_100_conv2"
  top: "map32_100_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_100_eltsum"
  type: "Eltwise"
  bottom: "map32_99_eltsum"
  bottom: "map32_100_conv_end"
  top: "map32_100_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_101_bn_pre"
  type: "BatchNorm"
  bottom: "map32_100_eltsum"
  top: "map32_101_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_101_bn_pre"
  type: "BatchNorm"
  bottom: "map32_100_eltsum"
  top: "map32_101_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_101_pre_scale"
  type: "Scale"
  bottom: "map32_101_bn_pre"
  top: "map32_101_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_101_pre_relu"
  type: "ReLU"
  bottom: "map32_101_bn_pre"
  top: "map32_101_bn_pre"
}
layer {
  name: "map32_101_conv1"
  type: "Convolution"
  bottom: "map32_101_bn_pre"
  top: "map32_101_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_101_bn1"
  type: "BatchNorm"
  bottom: "map32_101_conv1"
  top: "map32_101_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_101_bn1"
  type: "BatchNorm"
  bottom: "map32_101_conv1"
  top: "map32_101_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_101_scale1"
  type: "Scale"
  bottom: "map32_101_conv1"
  top: "map32_101_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_101_relu1"
  type: "ReLU"
  bottom: "map32_101_conv1"
  top: "map32_101_conv1"
}
layer {
  name: "map32_101_conv2"
  type: "Convolution"
  bottom: "map32_101_conv1"
  top: "map32_101_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_101_bn2"
  type: "BatchNorm"
  bottom: "map32_101_conv2"
  top: "map32_101_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_101_bn2"
  type: "BatchNorm"
  bottom: "map32_101_conv2"
  top: "map32_101_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_101_scale2"
  type: "Scale"
  bottom: "map32_101_conv2"
  top: "map32_101_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_101_relu2"
  type: "ReLU"
  bottom: "map32_101_conv2"
  top: "map32_101_conv2"
}
layer {
  name: "map32_101_conv_end"
  type: "Convolution"
  bottom: "map32_101_conv2"
  top: "map32_101_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_101_eltsum"
  type: "Eltwise"
  bottom: "map32_100_eltsum"
  bottom: "map32_101_conv_end"
  top: "map32_101_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_102_bn_pre"
  type: "BatchNorm"
  bottom: "map32_101_eltsum"
  top: "map32_102_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_102_bn_pre"
  type: "BatchNorm"
  bottom: "map32_101_eltsum"
  top: "map32_102_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_102_pre_scale"
  type: "Scale"
  bottom: "map32_102_bn_pre"
  top: "map32_102_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_102_pre_relu"
  type: "ReLU"
  bottom: "map32_102_bn_pre"
  top: "map32_102_bn_pre"
}
layer {
  name: "map32_102_conv1"
  type: "Convolution"
  bottom: "map32_102_bn_pre"
  top: "map32_102_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_102_bn1"
  type: "BatchNorm"
  bottom: "map32_102_conv1"
  top: "map32_102_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_102_bn1"
  type: "BatchNorm"
  bottom: "map32_102_conv1"
  top: "map32_102_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_102_scale1"
  type: "Scale"
  bottom: "map32_102_conv1"
  top: "map32_102_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_102_relu1"
  type: "ReLU"
  bottom: "map32_102_conv1"
  top: "map32_102_conv1"
}
layer {
  name: "map32_102_conv2"
  type: "Convolution"
  bottom: "map32_102_conv1"
  top: "map32_102_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_102_bn2"
  type: "BatchNorm"
  bottom: "map32_102_conv2"
  top: "map32_102_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_102_bn2"
  type: "BatchNorm"
  bottom: "map32_102_conv2"
  top: "map32_102_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_102_scale2"
  type: "Scale"
  bottom: "map32_102_conv2"
  top: "map32_102_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_102_relu2"
  type: "ReLU"
  bottom: "map32_102_conv2"
  top: "map32_102_conv2"
}
layer {
  name: "map32_102_conv_end"
  type: "Convolution"
  bottom: "map32_102_conv2"
  top: "map32_102_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_102_eltsum"
  type: "Eltwise"
  bottom: "map32_101_eltsum"
  bottom: "map32_102_conv_end"
  top: "map32_102_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_103_bn_pre"
  type: "BatchNorm"
  bottom: "map32_102_eltsum"
  top: "map32_103_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_103_bn_pre"
  type: "BatchNorm"
  bottom: "map32_102_eltsum"
  top: "map32_103_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_103_pre_scale"
  type: "Scale"
  bottom: "map32_103_bn_pre"
  top: "map32_103_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_103_pre_relu"
  type: "ReLU"
  bottom: "map32_103_bn_pre"
  top: "map32_103_bn_pre"
}
layer {
  name: "map32_103_conv1"
  type: "Convolution"
  bottom: "map32_103_bn_pre"
  top: "map32_103_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_103_bn1"
  type: "BatchNorm"
  bottom: "map32_103_conv1"
  top: "map32_103_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_103_bn1"
  type: "BatchNorm"
  bottom: "map32_103_conv1"
  top: "map32_103_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_103_scale1"
  type: "Scale"
  bottom: "map32_103_conv1"
  top: "map32_103_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_103_relu1"
  type: "ReLU"
  bottom: "map32_103_conv1"
  top: "map32_103_conv1"
}
layer {
  name: "map32_103_conv2"
  type: "Convolution"
  bottom: "map32_103_conv1"
  top: "map32_103_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_103_bn2"
  type: "BatchNorm"
  bottom: "map32_103_conv2"
  top: "map32_103_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_103_bn2"
  type: "BatchNorm"
  bottom: "map32_103_conv2"
  top: "map32_103_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_103_scale2"
  type: "Scale"
  bottom: "map32_103_conv2"
  top: "map32_103_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_103_relu2"
  type: "ReLU"
  bottom: "map32_103_conv2"
  top: "map32_103_conv2"
}
layer {
  name: "map32_103_conv_end"
  type: "Convolution"
  bottom: "map32_103_conv2"
  top: "map32_103_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_103_eltsum"
  type: "Eltwise"
  bottom: "map32_102_eltsum"
  bottom: "map32_103_conv_end"
  top: "map32_103_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_104_bn_pre"
  type: "BatchNorm"
  bottom: "map32_103_eltsum"
  top: "map32_104_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_104_bn_pre"
  type: "BatchNorm"
  bottom: "map32_103_eltsum"
  top: "map32_104_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_104_pre_scale"
  type: "Scale"
  bottom: "map32_104_bn_pre"
  top: "map32_104_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_104_pre_relu"
  type: "ReLU"
  bottom: "map32_104_bn_pre"
  top: "map32_104_bn_pre"
}
layer {
  name: "map32_104_conv1"
  type: "Convolution"
  bottom: "map32_104_bn_pre"
  top: "map32_104_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_104_bn1"
  type: "BatchNorm"
  bottom: "map32_104_conv1"
  top: "map32_104_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_104_bn1"
  type: "BatchNorm"
  bottom: "map32_104_conv1"
  top: "map32_104_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_104_scale1"
  type: "Scale"
  bottom: "map32_104_conv1"
  top: "map32_104_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_104_relu1"
  type: "ReLU"
  bottom: "map32_104_conv1"
  top: "map32_104_conv1"
}
layer {
  name: "map32_104_conv2"
  type: "Convolution"
  bottom: "map32_104_conv1"
  top: "map32_104_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_104_bn2"
  type: "BatchNorm"
  bottom: "map32_104_conv2"
  top: "map32_104_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_104_bn2"
  type: "BatchNorm"
  bottom: "map32_104_conv2"
  top: "map32_104_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_104_scale2"
  type: "Scale"
  bottom: "map32_104_conv2"
  top: "map32_104_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_104_relu2"
  type: "ReLU"
  bottom: "map32_104_conv2"
  top: "map32_104_conv2"
}
layer {
  name: "map32_104_conv_end"
  type: "Convolution"
  bottom: "map32_104_conv2"
  top: "map32_104_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_104_eltsum"
  type: "Eltwise"
  bottom: "map32_103_eltsum"
  bottom: "map32_104_conv_end"
  top: "map32_104_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_105_bn_pre"
  type: "BatchNorm"
  bottom: "map32_104_eltsum"
  top: "map32_105_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_105_bn_pre"
  type: "BatchNorm"
  bottom: "map32_104_eltsum"
  top: "map32_105_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_105_pre_scale"
  type: "Scale"
  bottom: "map32_105_bn_pre"
  top: "map32_105_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_105_pre_relu"
  type: "ReLU"
  bottom: "map32_105_bn_pre"
  top: "map32_105_bn_pre"
}
layer {
  name: "map32_105_conv1"
  type: "Convolution"
  bottom: "map32_105_bn_pre"
  top: "map32_105_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_105_bn1"
  type: "BatchNorm"
  bottom: "map32_105_conv1"
  top: "map32_105_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_105_bn1"
  type: "BatchNorm"
  bottom: "map32_105_conv1"
  top: "map32_105_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_105_scale1"
  type: "Scale"
  bottom: "map32_105_conv1"
  top: "map32_105_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_105_relu1"
  type: "ReLU"
  bottom: "map32_105_conv1"
  top: "map32_105_conv1"
}
layer {
  name: "map32_105_conv2"
  type: "Convolution"
  bottom: "map32_105_conv1"
  top: "map32_105_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_105_bn2"
  type: "BatchNorm"
  bottom: "map32_105_conv2"
  top: "map32_105_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_105_bn2"
  type: "BatchNorm"
  bottom: "map32_105_conv2"
  top: "map32_105_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_105_scale2"
  type: "Scale"
  bottom: "map32_105_conv2"
  top: "map32_105_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_105_relu2"
  type: "ReLU"
  bottom: "map32_105_conv2"
  top: "map32_105_conv2"
}
layer {
  name: "map32_105_conv_end"
  type: "Convolution"
  bottom: "map32_105_conv2"
  top: "map32_105_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_105_eltsum"
  type: "Eltwise"
  bottom: "map32_104_eltsum"
  bottom: "map32_105_conv_end"
  top: "map32_105_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_106_bn_pre"
  type: "BatchNorm"
  bottom: "map32_105_eltsum"
  top: "map32_106_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_106_bn_pre"
  type: "BatchNorm"
  bottom: "map32_105_eltsum"
  top: "map32_106_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_106_pre_scale"
  type: "Scale"
  bottom: "map32_106_bn_pre"
  top: "map32_106_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_106_pre_relu"
  type: "ReLU"
  bottom: "map32_106_bn_pre"
  top: "map32_106_bn_pre"
}
layer {
  name: "map32_106_conv1"
  type: "Convolution"
  bottom: "map32_106_bn_pre"
  top: "map32_106_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_106_bn1"
  type: "BatchNorm"
  bottom: "map32_106_conv1"
  top: "map32_106_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_106_bn1"
  type: "BatchNorm"
  bottom: "map32_106_conv1"
  top: "map32_106_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_106_scale1"
  type: "Scale"
  bottom: "map32_106_conv1"
  top: "map32_106_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_106_relu1"
  type: "ReLU"
  bottom: "map32_106_conv1"
  top: "map32_106_conv1"
}
layer {
  name: "map32_106_conv2"
  type: "Convolution"
  bottom: "map32_106_conv1"
  top: "map32_106_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_106_bn2"
  type: "BatchNorm"
  bottom: "map32_106_conv2"
  top: "map32_106_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_106_bn2"
  type: "BatchNorm"
  bottom: "map32_106_conv2"
  top: "map32_106_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_106_scale2"
  type: "Scale"
  bottom: "map32_106_conv2"
  top: "map32_106_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_106_relu2"
  type: "ReLU"
  bottom: "map32_106_conv2"
  top: "map32_106_conv2"
}
layer {
  name: "map32_106_conv_end"
  type: "Convolution"
  bottom: "map32_106_conv2"
  top: "map32_106_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_106_eltsum"
  type: "Eltwise"
  bottom: "map32_105_eltsum"
  bottom: "map32_106_conv_end"
  top: "map32_106_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_107_bn_pre"
  type: "BatchNorm"
  bottom: "map32_106_eltsum"
  top: "map32_107_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_107_bn_pre"
  type: "BatchNorm"
  bottom: "map32_106_eltsum"
  top: "map32_107_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_107_pre_scale"
  type: "Scale"
  bottom: "map32_107_bn_pre"
  top: "map32_107_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_107_pre_relu"
  type: "ReLU"
  bottom: "map32_107_bn_pre"
  top: "map32_107_bn_pre"
}
layer {
  name: "map32_107_conv1"
  type: "Convolution"
  bottom: "map32_107_bn_pre"
  top: "map32_107_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_107_bn1"
  type: "BatchNorm"
  bottom: "map32_107_conv1"
  top: "map32_107_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_107_bn1"
  type: "BatchNorm"
  bottom: "map32_107_conv1"
  top: "map32_107_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_107_scale1"
  type: "Scale"
  bottom: "map32_107_conv1"
  top: "map32_107_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_107_relu1"
  type: "ReLU"
  bottom: "map32_107_conv1"
  top: "map32_107_conv1"
}
layer {
  name: "map32_107_conv2"
  type: "Convolution"
  bottom: "map32_107_conv1"
  top: "map32_107_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_107_bn2"
  type: "BatchNorm"
  bottom: "map32_107_conv2"
  top: "map32_107_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_107_bn2"
  type: "BatchNorm"
  bottom: "map32_107_conv2"
  top: "map32_107_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_107_scale2"
  type: "Scale"
  bottom: "map32_107_conv2"
  top: "map32_107_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_107_relu2"
  type: "ReLU"
  bottom: "map32_107_conv2"
  top: "map32_107_conv2"
}
layer {
  name: "map32_107_conv_end"
  type: "Convolution"
  bottom: "map32_107_conv2"
  top: "map32_107_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_107_eltsum"
  type: "Eltwise"
  bottom: "map32_106_eltsum"
  bottom: "map32_107_conv_end"
  top: "map32_107_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_108_bn_pre"
  type: "BatchNorm"
  bottom: "map32_107_eltsum"
  top: "map32_108_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_108_bn_pre"
  type: "BatchNorm"
  bottom: "map32_107_eltsum"
  top: "map32_108_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_108_pre_scale"
  type: "Scale"
  bottom: "map32_108_bn_pre"
  top: "map32_108_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_108_pre_relu"
  type: "ReLU"
  bottom: "map32_108_bn_pre"
  top: "map32_108_bn_pre"
}
layer {
  name: "map32_108_conv1"
  type: "Convolution"
  bottom: "map32_108_bn_pre"
  top: "map32_108_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_108_bn1"
  type: "BatchNorm"
  bottom: "map32_108_conv1"
  top: "map32_108_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_108_bn1"
  type: "BatchNorm"
  bottom: "map32_108_conv1"
  top: "map32_108_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_108_scale1"
  type: "Scale"
  bottom: "map32_108_conv1"
  top: "map32_108_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_108_relu1"
  type: "ReLU"
  bottom: "map32_108_conv1"
  top: "map32_108_conv1"
}
layer {
  name: "map32_108_conv2"
  type: "Convolution"
  bottom: "map32_108_conv1"
  top: "map32_108_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_108_bn2"
  type: "BatchNorm"
  bottom: "map32_108_conv2"
  top: "map32_108_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_108_bn2"
  type: "BatchNorm"
  bottom: "map32_108_conv2"
  top: "map32_108_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_108_scale2"
  type: "Scale"
  bottom: "map32_108_conv2"
  top: "map32_108_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_108_relu2"
  type: "ReLU"
  bottom: "map32_108_conv2"
  top: "map32_108_conv2"
}
layer {
  name: "map32_108_conv_end"
  type: "Convolution"
  bottom: "map32_108_conv2"
  top: "map32_108_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_108_eltsum"
  type: "Eltwise"
  bottom: "map32_107_eltsum"
  bottom: "map32_108_conv_end"
  top: "map32_108_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_109_bn_pre"
  type: "BatchNorm"
  bottom: "map32_108_eltsum"
  top: "map32_109_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_109_bn_pre"
  type: "BatchNorm"
  bottom: "map32_108_eltsum"
  top: "map32_109_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_109_pre_scale"
  type: "Scale"
  bottom: "map32_109_bn_pre"
  top: "map32_109_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_109_pre_relu"
  type: "ReLU"
  bottom: "map32_109_bn_pre"
  top: "map32_109_bn_pre"
}
layer {
  name: "map32_109_conv1"
  type: "Convolution"
  bottom: "map32_109_bn_pre"
  top: "map32_109_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_109_bn1"
  type: "BatchNorm"
  bottom: "map32_109_conv1"
  top: "map32_109_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_109_bn1"
  type: "BatchNorm"
  bottom: "map32_109_conv1"
  top: "map32_109_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_109_scale1"
  type: "Scale"
  bottom: "map32_109_conv1"
  top: "map32_109_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_109_relu1"
  type: "ReLU"
  bottom: "map32_109_conv1"
  top: "map32_109_conv1"
}
layer {
  name: "map32_109_conv2"
  type: "Convolution"
  bottom: "map32_109_conv1"
  top: "map32_109_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_109_bn2"
  type: "BatchNorm"
  bottom: "map32_109_conv2"
  top: "map32_109_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_109_bn2"
  type: "BatchNorm"
  bottom: "map32_109_conv2"
  top: "map32_109_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_109_scale2"
  type: "Scale"
  bottom: "map32_109_conv2"
  top: "map32_109_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_109_relu2"
  type: "ReLU"
  bottom: "map32_109_conv2"
  top: "map32_109_conv2"
}
layer {
  name: "map32_109_conv_end"
  type: "Convolution"
  bottom: "map32_109_conv2"
  top: "map32_109_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_109_eltsum"
  type: "Eltwise"
  bottom: "map32_108_eltsum"
  bottom: "map32_109_conv_end"
  top: "map32_109_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_110_bn_pre"
  type: "BatchNorm"
  bottom: "map32_109_eltsum"
  top: "map32_110_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_110_bn_pre"
  type: "BatchNorm"
  bottom: "map32_109_eltsum"
  top: "map32_110_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_110_pre_scale"
  type: "Scale"
  bottom: "map32_110_bn_pre"
  top: "map32_110_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_110_pre_relu"
  type: "ReLU"
  bottom: "map32_110_bn_pre"
  top: "map32_110_bn_pre"
}
layer {
  name: "map32_110_conv1"
  type: "Convolution"
  bottom: "map32_110_bn_pre"
  top: "map32_110_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_110_bn1"
  type: "BatchNorm"
  bottom: "map32_110_conv1"
  top: "map32_110_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_110_bn1"
  type: "BatchNorm"
  bottom: "map32_110_conv1"
  top: "map32_110_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_110_scale1"
  type: "Scale"
  bottom: "map32_110_conv1"
  top: "map32_110_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_110_relu1"
  type: "ReLU"
  bottom: "map32_110_conv1"
  top: "map32_110_conv1"
}
layer {
  name: "map32_110_conv2"
  type: "Convolution"
  bottom: "map32_110_conv1"
  top: "map32_110_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_110_bn2"
  type: "BatchNorm"
  bottom: "map32_110_conv2"
  top: "map32_110_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_110_bn2"
  type: "BatchNorm"
  bottom: "map32_110_conv2"
  top: "map32_110_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_110_scale2"
  type: "Scale"
  bottom: "map32_110_conv2"
  top: "map32_110_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_110_relu2"
  type: "ReLU"
  bottom: "map32_110_conv2"
  top: "map32_110_conv2"
}
layer {
  name: "map32_110_conv_end"
  type: "Convolution"
  bottom: "map32_110_conv2"
  top: "map32_110_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_110_eltsum"
  type: "Eltwise"
  bottom: "map32_109_eltsum"
  bottom: "map32_110_conv_end"
  top: "map32_110_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map32_111_bn_pre"
  type: "BatchNorm"
  bottom: "map32_110_eltsum"
  top: "map32_111_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_111_bn_pre"
  type: "BatchNorm"
  bottom: "map32_110_eltsum"
  top: "map32_111_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_111_pre_scale"
  type: "Scale"
  bottom: "map32_111_bn_pre"
  top: "map32_111_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_111_pre_relu"
  type: "ReLU"
  bottom: "map32_111_bn_pre"
  top: "map32_111_bn_pre"
}
layer {
  name: "map32_111_conv1"
  type: "Convolution"
  bottom: "map32_111_bn_pre"
  top: "map32_111_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_111_bn1"
  type: "BatchNorm"
  bottom: "map32_111_conv1"
  top: "map32_111_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_111_bn1"
  type: "BatchNorm"
  bottom: "map32_111_conv1"
  top: "map32_111_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_111_scale1"
  type: "Scale"
  bottom: "map32_111_conv1"
  top: "map32_111_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_111_relu1"
  type: "ReLU"
  bottom: "map32_111_conv1"
  top: "map32_111_conv1"
}
layer {
  name: "map32_111_conv2"
  type: "Convolution"
  bottom: "map32_111_conv1"
  top: "map32_111_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_111_bn2"
  type: "BatchNorm"
  bottom: "map32_111_conv2"
  top: "map32_111_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map32_111_bn2"
  type: "BatchNorm"
  bottom: "map32_111_conv2"
  top: "map32_111_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map32_111_scale2"
  type: "Scale"
  bottom: "map32_111_conv2"
  top: "map32_111_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map32_111_relu2"
  type: "ReLU"
  bottom: "map32_111_conv2"
  top: "map32_111_conv2"
}
layer {
  name: "map32_111_conv_end"
  type: "Convolution"
  bottom: "map32_111_conv2"
  top: "map32_111_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map32_111_eltsum"
  type: "Eltwise"
  bottom: "map32_110_eltsum"
  bottom: "map32_111_conv_end"
  top: "map32_111_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_1_bn_pre"
  type: "BatchNorm"
  bottom: "map32_111_eltsum"
  top: "map64_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn_pre"
  type: "BatchNorm"
  bottom: "map32_111_eltsum"
  top: "map64_1_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_pre_scale"
  type: "Scale"
  bottom: "map64_1_bn_pre"
  top: "map64_1_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_pre_relu"
  type: "ReLU"
  bottom: "map64_1_bn_pre"
  top: "map64_1_bn_pre"
}
layer {
  name: "map64_1_conv1"
  type: "Convolution"
  bottom: "map64_1_bn_pre"
  top: "map64_1_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn1"
  type: "BatchNorm"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn1"
  type: "BatchNorm"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale1"
  type: "Scale"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_relu1"
  type: "ReLU"
  bottom: "map64_1_conv1"
  top: "map64_1_conv1"
}
layer {
  name: "map64_1_conv2"
  type: "Convolution"
  bottom: "map64_1_conv1"
  top: "map64_1_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_bn2"
  type: "BatchNorm"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_1_bn2"
  type: "BatchNorm"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_1_scale2"
  type: "Scale"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_1_relu2"
  type: "ReLU"
  bottom: "map64_1_conv2"
  top: "map64_1_conv2"
}
layer {
  name: "map64_1_conv_end"
  type: "Convolution"
  bottom: "map64_1_conv2"
  top: "map64_1_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "map64_1_bn_pre"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_1_eltsum"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "map64_1_conv_end"
  top: "map64_1_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_2_bn_pre"
  type: "BatchNorm"
  bottom: "map64_1_eltsum"
  top: "map64_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn_pre"
  type: "BatchNorm"
  bottom: "map64_1_eltsum"
  top: "map64_2_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_pre_scale"
  type: "Scale"
  bottom: "map64_2_bn_pre"
  top: "map64_2_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_pre_relu"
  type: "ReLU"
  bottom: "map64_2_bn_pre"
  top: "map64_2_bn_pre"
}
layer {
  name: "map64_2_conv1"
  type: "Convolution"
  bottom: "map64_2_bn_pre"
  top: "map64_2_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_bn1"
  type: "BatchNorm"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn1"
  type: "BatchNorm"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_scale1"
  type: "Scale"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_relu1"
  type: "ReLU"
  bottom: "map64_2_conv1"
  top: "map64_2_conv1"
}
layer {
  name: "map64_2_conv2"
  type: "Convolution"
  bottom: "map64_2_conv1"
  top: "map64_2_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_bn2"
  type: "BatchNorm"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_2_bn2"
  type: "BatchNorm"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_2_scale2"
  type: "Scale"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_2_relu2"
  type: "ReLU"
  bottom: "map64_2_conv2"
  top: "map64_2_conv2"
}
layer {
  name: "map64_2_conv_end"
  type: "Convolution"
  bottom: "map64_2_conv2"
  top: "map64_2_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_2_eltsum"
  type: "Eltwise"
  bottom: "map64_1_eltsum"
  bottom: "map64_2_conv_end"
  top: "map64_2_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_3_bn_pre"
  type: "BatchNorm"
  bottom: "map64_2_eltsum"
  top: "map64_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn_pre"
  type: "BatchNorm"
  bottom: "map64_2_eltsum"
  top: "map64_3_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_pre_scale"
  type: "Scale"
  bottom: "map64_3_bn_pre"
  top: "map64_3_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_pre_relu"
  type: "ReLU"
  bottom: "map64_3_bn_pre"
  top: "map64_3_bn_pre"
}
layer {
  name: "map64_3_conv1"
  type: "Convolution"
  bottom: "map64_3_bn_pre"
  top: "map64_3_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_bn1"
  type: "BatchNorm"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn1"
  type: "BatchNorm"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_scale1"
  type: "Scale"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_relu1"
  type: "ReLU"
  bottom: "map64_3_conv1"
  top: "map64_3_conv1"
}
layer {
  name: "map64_3_conv2"
  type: "Convolution"
  bottom: "map64_3_conv1"
  top: "map64_3_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_bn2"
  type: "BatchNorm"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_3_bn2"
  type: "BatchNorm"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_3_scale2"
  type: "Scale"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_3_relu2"
  type: "ReLU"
  bottom: "map64_3_conv2"
  top: "map64_3_conv2"
}
layer {
  name: "map64_3_conv_end"
  type: "Convolution"
  bottom: "map64_3_conv2"
  top: "map64_3_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_3_eltsum"
  type: "Eltwise"
  bottom: "map64_2_eltsum"
  bottom: "map64_3_conv_end"
  top: "map64_3_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_4_bn_pre"
  type: "BatchNorm"
  bottom: "map64_3_eltsum"
  top: "map64_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_4_bn_pre"
  type: "BatchNorm"
  bottom: "map64_3_eltsum"
  top: "map64_4_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_4_pre_scale"
  type: "Scale"
  bottom: "map64_4_bn_pre"
  top: "map64_4_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_4_pre_relu"
  type: "ReLU"
  bottom: "map64_4_bn_pre"
  top: "map64_4_bn_pre"
}
layer {
  name: "map64_4_conv1"
  type: "Convolution"
  bottom: "map64_4_bn_pre"
  top: "map64_4_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_4_bn1"
  type: "BatchNorm"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_4_bn1"
  type: "BatchNorm"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_4_scale1"
  type: "Scale"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_4_relu1"
  type: "ReLU"
  bottom: "map64_4_conv1"
  top: "map64_4_conv1"
}
layer {
  name: "map64_4_conv2"
  type: "Convolution"
  bottom: "map64_4_conv1"
  top: "map64_4_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_4_bn2"
  type: "BatchNorm"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_4_bn2"
  type: "BatchNorm"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_4_scale2"
  type: "Scale"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_4_relu2"
  type: "ReLU"
  bottom: "map64_4_conv2"
  top: "map64_4_conv2"
}
layer {
  name: "map64_4_conv_end"
  type: "Convolution"
  bottom: "map64_4_conv2"
  top: "map64_4_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_4_eltsum"
  type: "Eltwise"
  bottom: "map64_3_eltsum"
  bottom: "map64_4_conv_end"
  top: "map64_4_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_5_bn_pre"
  type: "BatchNorm"
  bottom: "map64_4_eltsum"
  top: "map64_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_5_bn_pre"
  type: "BatchNorm"
  bottom: "map64_4_eltsum"
  top: "map64_5_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_5_pre_scale"
  type: "Scale"
  bottom: "map64_5_bn_pre"
  top: "map64_5_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_5_pre_relu"
  type: "ReLU"
  bottom: "map64_5_bn_pre"
  top: "map64_5_bn_pre"
}
layer {
  name: "map64_5_conv1"
  type: "Convolution"
  bottom: "map64_5_bn_pre"
  top: "map64_5_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_5_bn1"
  type: "BatchNorm"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_5_bn1"
  type: "BatchNorm"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_5_scale1"
  type: "Scale"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_5_relu1"
  type: "ReLU"
  bottom: "map64_5_conv1"
  top: "map64_5_conv1"
}
layer {
  name: "map64_5_conv2"
  type: "Convolution"
  bottom: "map64_5_conv1"
  top: "map64_5_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_5_bn2"
  type: "BatchNorm"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_5_bn2"
  type: "BatchNorm"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_5_scale2"
  type: "Scale"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_5_relu2"
  type: "ReLU"
  bottom: "map64_5_conv2"
  top: "map64_5_conv2"
}
layer {
  name: "map64_5_conv_end"
  type: "Convolution"
  bottom: "map64_5_conv2"
  top: "map64_5_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_5_eltsum"
  type: "Eltwise"
  bottom: "map64_4_eltsum"
  bottom: "map64_5_conv_end"
  top: "map64_5_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_6_bn_pre"
  type: "BatchNorm"
  bottom: "map64_5_eltsum"
  top: "map64_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_6_bn_pre"
  type: "BatchNorm"
  bottom: "map64_5_eltsum"
  top: "map64_6_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_6_pre_scale"
  type: "Scale"
  bottom: "map64_6_bn_pre"
  top: "map64_6_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_6_pre_relu"
  type: "ReLU"
  bottom: "map64_6_bn_pre"
  top: "map64_6_bn_pre"
}
layer {
  name: "map64_6_conv1"
  type: "Convolution"
  bottom: "map64_6_bn_pre"
  top: "map64_6_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_6_bn1"
  type: "BatchNorm"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_6_bn1"
  type: "BatchNorm"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_6_scale1"
  type: "Scale"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_6_relu1"
  type: "ReLU"
  bottom: "map64_6_conv1"
  top: "map64_6_conv1"
}
layer {
  name: "map64_6_conv2"
  type: "Convolution"
  bottom: "map64_6_conv1"
  top: "map64_6_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_6_bn2"
  type: "BatchNorm"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_6_bn2"
  type: "BatchNorm"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_6_scale2"
  type: "Scale"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_6_relu2"
  type: "ReLU"
  bottom: "map64_6_conv2"
  top: "map64_6_conv2"
}
layer {
  name: "map64_6_conv_end"
  type: "Convolution"
  bottom: "map64_6_conv2"
  top: "map64_6_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_6_eltsum"
  type: "Eltwise"
  bottom: "map64_5_eltsum"
  bottom: "map64_6_conv_end"
  top: "map64_6_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_7_bn_pre"
  type: "BatchNorm"
  bottom: "map64_6_eltsum"
  top: "map64_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_7_bn_pre"
  type: "BatchNorm"
  bottom: "map64_6_eltsum"
  top: "map64_7_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_7_pre_scale"
  type: "Scale"
  bottom: "map64_7_bn_pre"
  top: "map64_7_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_7_pre_relu"
  type: "ReLU"
  bottom: "map64_7_bn_pre"
  top: "map64_7_bn_pre"
}
layer {
  name: "map64_7_conv1"
  type: "Convolution"
  bottom: "map64_7_bn_pre"
  top: "map64_7_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_7_bn1"
  type: "BatchNorm"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_7_bn1"
  type: "BatchNorm"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_7_scale1"
  type: "Scale"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_7_relu1"
  type: "ReLU"
  bottom: "map64_7_conv1"
  top: "map64_7_conv1"
}
layer {
  name: "map64_7_conv2"
  type: "Convolution"
  bottom: "map64_7_conv1"
  top: "map64_7_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_7_bn2"
  type: "BatchNorm"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_7_bn2"
  type: "BatchNorm"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_7_scale2"
  type: "Scale"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_7_relu2"
  type: "ReLU"
  bottom: "map64_7_conv2"
  top: "map64_7_conv2"
}
layer {
  name: "map64_7_conv_end"
  type: "Convolution"
  bottom: "map64_7_conv2"
  top: "map64_7_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_7_eltsum"
  type: "Eltwise"
  bottom: "map64_6_eltsum"
  bottom: "map64_7_conv_end"
  top: "map64_7_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_8_bn_pre"
  type: "BatchNorm"
  bottom: "map64_7_eltsum"
  top: "map64_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_8_bn_pre"
  type: "BatchNorm"
  bottom: "map64_7_eltsum"
  top: "map64_8_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_8_pre_scale"
  type: "Scale"
  bottom: "map64_8_bn_pre"
  top: "map64_8_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_8_pre_relu"
  type: "ReLU"
  bottom: "map64_8_bn_pre"
  top: "map64_8_bn_pre"
}
layer {
  name: "map64_8_conv1"
  type: "Convolution"
  bottom: "map64_8_bn_pre"
  top: "map64_8_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_8_bn1"
  type: "BatchNorm"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_8_bn1"
  type: "BatchNorm"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_8_scale1"
  type: "Scale"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_8_relu1"
  type: "ReLU"
  bottom: "map64_8_conv1"
  top: "map64_8_conv1"
}
layer {
  name: "map64_8_conv2"
  type: "Convolution"
  bottom: "map64_8_conv1"
  top: "map64_8_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_8_bn2"
  type: "BatchNorm"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_8_bn2"
  type: "BatchNorm"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_8_scale2"
  type: "Scale"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_8_relu2"
  type: "ReLU"
  bottom: "map64_8_conv2"
  top: "map64_8_conv2"
}
layer {
  name: "map64_8_conv_end"
  type: "Convolution"
  bottom: "map64_8_conv2"
  top: "map64_8_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_8_eltsum"
  type: "Eltwise"
  bottom: "map64_7_eltsum"
  bottom: "map64_8_conv_end"
  top: "map64_8_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_9_bn_pre"
  type: "BatchNorm"
  bottom: "map64_8_eltsum"
  top: "map64_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_9_bn_pre"
  type: "BatchNorm"
  bottom: "map64_8_eltsum"
  top: "map64_9_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_9_pre_scale"
  type: "Scale"
  bottom: "map64_9_bn_pre"
  top: "map64_9_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_9_pre_relu"
  type: "ReLU"
  bottom: "map64_9_bn_pre"
  top: "map64_9_bn_pre"
}
layer {
  name: "map64_9_conv1"
  type: "Convolution"
  bottom: "map64_9_bn_pre"
  top: "map64_9_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_9_bn1"
  type: "BatchNorm"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_9_bn1"
  type: "BatchNorm"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_9_scale1"
  type: "Scale"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_9_relu1"
  type: "ReLU"
  bottom: "map64_9_conv1"
  top: "map64_9_conv1"
}
layer {
  name: "map64_9_conv2"
  type: "Convolution"
  bottom: "map64_9_conv1"
  top: "map64_9_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_9_bn2"
  type: "BatchNorm"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_9_bn2"
  type: "BatchNorm"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_9_scale2"
  type: "Scale"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_9_relu2"
  type: "ReLU"
  bottom: "map64_9_conv2"
  top: "map64_9_conv2"
}
layer {
  name: "map64_9_conv_end"
  type: "Convolution"
  bottom: "map64_9_conv2"
  top: "map64_9_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_9_eltsum"
  type: "Eltwise"
  bottom: "map64_8_eltsum"
  bottom: "map64_9_conv_end"
  top: "map64_9_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_10_bn_pre"
  type: "BatchNorm"
  bottom: "map64_9_eltsum"
  top: "map64_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_10_bn_pre"
  type: "BatchNorm"
  bottom: "map64_9_eltsum"
  top: "map64_10_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_10_pre_scale"
  type: "Scale"
  bottom: "map64_10_bn_pre"
  top: "map64_10_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_10_pre_relu"
  type: "ReLU"
  bottom: "map64_10_bn_pre"
  top: "map64_10_bn_pre"
}
layer {
  name: "map64_10_conv1"
  type: "Convolution"
  bottom: "map64_10_bn_pre"
  top: "map64_10_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_10_bn1"
  type: "BatchNorm"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_10_bn1"
  type: "BatchNorm"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_10_scale1"
  type: "Scale"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_10_relu1"
  type: "ReLU"
  bottom: "map64_10_conv1"
  top: "map64_10_conv1"
}
layer {
  name: "map64_10_conv2"
  type: "Convolution"
  bottom: "map64_10_conv1"
  top: "map64_10_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_10_bn2"
  type: "BatchNorm"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_10_bn2"
  type: "BatchNorm"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_10_scale2"
  type: "Scale"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_10_relu2"
  type: "ReLU"
  bottom: "map64_10_conv2"
  top: "map64_10_conv2"
}
layer {
  name: "map64_10_conv_end"
  type: "Convolution"
  bottom: "map64_10_conv2"
  top: "map64_10_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_10_eltsum"
  type: "Eltwise"
  bottom: "map64_9_eltsum"
  bottom: "map64_10_conv_end"
  top: "map64_10_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_11_bn_pre"
  type: "BatchNorm"
  bottom: "map64_10_eltsum"
  top: "map64_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_11_bn_pre"
  type: "BatchNorm"
  bottom: "map64_10_eltsum"
  top: "map64_11_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_11_pre_scale"
  type: "Scale"
  bottom: "map64_11_bn_pre"
  top: "map64_11_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_11_pre_relu"
  type: "ReLU"
  bottom: "map64_11_bn_pre"
  top: "map64_11_bn_pre"
}
layer {
  name: "map64_11_conv1"
  type: "Convolution"
  bottom: "map64_11_bn_pre"
  top: "map64_11_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_11_bn1"
  type: "BatchNorm"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_11_bn1"
  type: "BatchNorm"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_11_scale1"
  type: "Scale"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_11_relu1"
  type: "ReLU"
  bottom: "map64_11_conv1"
  top: "map64_11_conv1"
}
layer {
  name: "map64_11_conv2"
  type: "Convolution"
  bottom: "map64_11_conv1"
  top: "map64_11_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_11_bn2"
  type: "BatchNorm"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_11_bn2"
  type: "BatchNorm"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_11_scale2"
  type: "Scale"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_11_relu2"
  type: "ReLU"
  bottom: "map64_11_conv2"
  top: "map64_11_conv2"
}
layer {
  name: "map64_11_conv_end"
  type: "Convolution"
  bottom: "map64_11_conv2"
  top: "map64_11_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_11_eltsum"
  type: "Eltwise"
  bottom: "map64_10_eltsum"
  bottom: "map64_11_conv_end"
  top: "map64_11_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_12_bn_pre"
  type: "BatchNorm"
  bottom: "map64_11_eltsum"
  top: "map64_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_12_bn_pre"
  type: "BatchNorm"
  bottom: "map64_11_eltsum"
  top: "map64_12_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_12_pre_scale"
  type: "Scale"
  bottom: "map64_12_bn_pre"
  top: "map64_12_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_12_pre_relu"
  type: "ReLU"
  bottom: "map64_12_bn_pre"
  top: "map64_12_bn_pre"
}
layer {
  name: "map64_12_conv1"
  type: "Convolution"
  bottom: "map64_12_bn_pre"
  top: "map64_12_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_12_bn1"
  type: "BatchNorm"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_12_bn1"
  type: "BatchNorm"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_12_scale1"
  type: "Scale"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_12_relu1"
  type: "ReLU"
  bottom: "map64_12_conv1"
  top: "map64_12_conv1"
}
layer {
  name: "map64_12_conv2"
  type: "Convolution"
  bottom: "map64_12_conv1"
  top: "map64_12_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_12_bn2"
  type: "BatchNorm"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_12_bn2"
  type: "BatchNorm"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_12_scale2"
  type: "Scale"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_12_relu2"
  type: "ReLU"
  bottom: "map64_12_conv2"
  top: "map64_12_conv2"
}
layer {
  name: "map64_12_conv_end"
  type: "Convolution"
  bottom: "map64_12_conv2"
  top: "map64_12_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_12_eltsum"
  type: "Eltwise"
  bottom: "map64_11_eltsum"
  bottom: "map64_12_conv_end"
  top: "map64_12_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_13_bn_pre"
  type: "BatchNorm"
  bottom: "map64_12_eltsum"
  top: "map64_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_13_bn_pre"
  type: "BatchNorm"
  bottom: "map64_12_eltsum"
  top: "map64_13_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_13_pre_scale"
  type: "Scale"
  bottom: "map64_13_bn_pre"
  top: "map64_13_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_13_pre_relu"
  type: "ReLU"
  bottom: "map64_13_bn_pre"
  top: "map64_13_bn_pre"
}
layer {
  name: "map64_13_conv1"
  type: "Convolution"
  bottom: "map64_13_bn_pre"
  top: "map64_13_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_13_bn1"
  type: "BatchNorm"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_13_bn1"
  type: "BatchNorm"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_13_scale1"
  type: "Scale"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_13_relu1"
  type: "ReLU"
  bottom: "map64_13_conv1"
  top: "map64_13_conv1"
}
layer {
  name: "map64_13_conv2"
  type: "Convolution"
  bottom: "map64_13_conv1"
  top: "map64_13_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_13_bn2"
  type: "BatchNorm"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_13_bn2"
  type: "BatchNorm"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_13_scale2"
  type: "Scale"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_13_relu2"
  type: "ReLU"
  bottom: "map64_13_conv2"
  top: "map64_13_conv2"
}
layer {
  name: "map64_13_conv_end"
  type: "Convolution"
  bottom: "map64_13_conv2"
  top: "map64_13_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_13_eltsum"
  type: "Eltwise"
  bottom: "map64_12_eltsum"
  bottom: "map64_13_conv_end"
  top: "map64_13_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_14_bn_pre"
  type: "BatchNorm"
  bottom: "map64_13_eltsum"
  top: "map64_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_14_bn_pre"
  type: "BatchNorm"
  bottom: "map64_13_eltsum"
  top: "map64_14_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_14_pre_scale"
  type: "Scale"
  bottom: "map64_14_bn_pre"
  top: "map64_14_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_14_pre_relu"
  type: "ReLU"
  bottom: "map64_14_bn_pre"
  top: "map64_14_bn_pre"
}
layer {
  name: "map64_14_conv1"
  type: "Convolution"
  bottom: "map64_14_bn_pre"
  top: "map64_14_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_14_bn1"
  type: "BatchNorm"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_14_bn1"
  type: "BatchNorm"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_14_scale1"
  type: "Scale"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_14_relu1"
  type: "ReLU"
  bottom: "map64_14_conv1"
  top: "map64_14_conv1"
}
layer {
  name: "map64_14_conv2"
  type: "Convolution"
  bottom: "map64_14_conv1"
  top: "map64_14_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_14_bn2"
  type: "BatchNorm"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_14_bn2"
  type: "BatchNorm"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_14_scale2"
  type: "Scale"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_14_relu2"
  type: "ReLU"
  bottom: "map64_14_conv2"
  top: "map64_14_conv2"
}
layer {
  name: "map64_14_conv_end"
  type: "Convolution"
  bottom: "map64_14_conv2"
  top: "map64_14_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_14_eltsum"
  type: "Eltwise"
  bottom: "map64_13_eltsum"
  bottom: "map64_14_conv_end"
  top: "map64_14_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_15_bn_pre"
  type: "BatchNorm"
  bottom: "map64_14_eltsum"
  top: "map64_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_15_bn_pre"
  type: "BatchNorm"
  bottom: "map64_14_eltsum"
  top: "map64_15_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_15_pre_scale"
  type: "Scale"
  bottom: "map64_15_bn_pre"
  top: "map64_15_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_15_pre_relu"
  type: "ReLU"
  bottom: "map64_15_bn_pre"
  top: "map64_15_bn_pre"
}
layer {
  name: "map64_15_conv1"
  type: "Convolution"
  bottom: "map64_15_bn_pre"
  top: "map64_15_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_15_bn1"
  type: "BatchNorm"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_15_bn1"
  type: "BatchNorm"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_15_scale1"
  type: "Scale"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_15_relu1"
  type: "ReLU"
  bottom: "map64_15_conv1"
  top: "map64_15_conv1"
}
layer {
  name: "map64_15_conv2"
  type: "Convolution"
  bottom: "map64_15_conv1"
  top: "map64_15_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_15_bn2"
  type: "BatchNorm"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_15_bn2"
  type: "BatchNorm"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_15_scale2"
  type: "Scale"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_15_relu2"
  type: "ReLU"
  bottom: "map64_15_conv2"
  top: "map64_15_conv2"
}
layer {
  name: "map64_15_conv_end"
  type: "Convolution"
  bottom: "map64_15_conv2"
  top: "map64_15_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_15_eltsum"
  type: "Eltwise"
  bottom: "map64_14_eltsum"
  bottom: "map64_15_conv_end"
  top: "map64_15_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_16_bn_pre"
  type: "BatchNorm"
  bottom: "map64_15_eltsum"
  top: "map64_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_16_bn_pre"
  type: "BatchNorm"
  bottom: "map64_15_eltsum"
  top: "map64_16_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_16_pre_scale"
  type: "Scale"
  bottom: "map64_16_bn_pre"
  top: "map64_16_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_16_pre_relu"
  type: "ReLU"
  bottom: "map64_16_bn_pre"
  top: "map64_16_bn_pre"
}
layer {
  name: "map64_16_conv1"
  type: "Convolution"
  bottom: "map64_16_bn_pre"
  top: "map64_16_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_16_bn1"
  type: "BatchNorm"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_16_bn1"
  type: "BatchNorm"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_16_scale1"
  type: "Scale"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_16_relu1"
  type: "ReLU"
  bottom: "map64_16_conv1"
  top: "map64_16_conv1"
}
layer {
  name: "map64_16_conv2"
  type: "Convolution"
  bottom: "map64_16_conv1"
  top: "map64_16_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_16_bn2"
  type: "BatchNorm"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_16_bn2"
  type: "BatchNorm"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_16_scale2"
  type: "Scale"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_16_relu2"
  type: "ReLU"
  bottom: "map64_16_conv2"
  top: "map64_16_conv2"
}
layer {
  name: "map64_16_conv_end"
  type: "Convolution"
  bottom: "map64_16_conv2"
  top: "map64_16_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_16_eltsum"
  type: "Eltwise"
  bottom: "map64_15_eltsum"
  bottom: "map64_16_conv_end"
  top: "map64_16_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_17_bn_pre"
  type: "BatchNorm"
  bottom: "map64_16_eltsum"
  top: "map64_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_17_bn_pre"
  type: "BatchNorm"
  bottom: "map64_16_eltsum"
  top: "map64_17_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_17_pre_scale"
  type: "Scale"
  bottom: "map64_17_bn_pre"
  top: "map64_17_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_17_pre_relu"
  type: "ReLU"
  bottom: "map64_17_bn_pre"
  top: "map64_17_bn_pre"
}
layer {
  name: "map64_17_conv1"
  type: "Convolution"
  bottom: "map64_17_bn_pre"
  top: "map64_17_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_17_bn1"
  type: "BatchNorm"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_17_bn1"
  type: "BatchNorm"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_17_scale1"
  type: "Scale"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_17_relu1"
  type: "ReLU"
  bottom: "map64_17_conv1"
  top: "map64_17_conv1"
}
layer {
  name: "map64_17_conv2"
  type: "Convolution"
  bottom: "map64_17_conv1"
  top: "map64_17_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_17_bn2"
  type: "BatchNorm"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_17_bn2"
  type: "BatchNorm"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_17_scale2"
  type: "Scale"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_17_relu2"
  type: "ReLU"
  bottom: "map64_17_conv2"
  top: "map64_17_conv2"
}
layer {
  name: "map64_17_conv_end"
  type: "Convolution"
  bottom: "map64_17_conv2"
  top: "map64_17_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_17_eltsum"
  type: "Eltwise"
  bottom: "map64_16_eltsum"
  bottom: "map64_17_conv_end"
  top: "map64_17_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_18_bn_pre"
  type: "BatchNorm"
  bottom: "map64_17_eltsum"
  top: "map64_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_18_bn_pre"
  type: "BatchNorm"
  bottom: "map64_17_eltsum"
  top: "map64_18_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_18_pre_scale"
  type: "Scale"
  bottom: "map64_18_bn_pre"
  top: "map64_18_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_18_pre_relu"
  type: "ReLU"
  bottom: "map64_18_bn_pre"
  top: "map64_18_bn_pre"
}
layer {
  name: "map64_18_conv1"
  type: "Convolution"
  bottom: "map64_18_bn_pre"
  top: "map64_18_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_18_bn1"
  type: "BatchNorm"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_18_bn1"
  type: "BatchNorm"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_18_scale1"
  type: "Scale"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_18_relu1"
  type: "ReLU"
  bottom: "map64_18_conv1"
  top: "map64_18_conv1"
}
layer {
  name: "map64_18_conv2"
  type: "Convolution"
  bottom: "map64_18_conv1"
  top: "map64_18_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_18_bn2"
  type: "BatchNorm"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_18_bn2"
  type: "BatchNorm"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_18_scale2"
  type: "Scale"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_18_relu2"
  type: "ReLU"
  bottom: "map64_18_conv2"
  top: "map64_18_conv2"
}
layer {
  name: "map64_18_conv_end"
  type: "Convolution"
  bottom: "map64_18_conv2"
  top: "map64_18_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_18_eltsum"
  type: "Eltwise"
  bottom: "map64_17_eltsum"
  bottom: "map64_18_conv_end"
  top: "map64_18_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_19_bn_pre"
  type: "BatchNorm"
  bottom: "map64_18_eltsum"
  top: "map64_19_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_19_bn_pre"
  type: "BatchNorm"
  bottom: "map64_18_eltsum"
  top: "map64_19_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_19_pre_scale"
  type: "Scale"
  bottom: "map64_19_bn_pre"
  top: "map64_19_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_19_pre_relu"
  type: "ReLU"
  bottom: "map64_19_bn_pre"
  top: "map64_19_bn_pre"
}
layer {
  name: "map64_19_conv1"
  type: "Convolution"
  bottom: "map64_19_bn_pre"
  top: "map64_19_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_19_bn1"
  type: "BatchNorm"
  bottom: "map64_19_conv1"
  top: "map64_19_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_19_bn1"
  type: "BatchNorm"
  bottom: "map64_19_conv1"
  top: "map64_19_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_19_scale1"
  type: "Scale"
  bottom: "map64_19_conv1"
  top: "map64_19_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_19_relu1"
  type: "ReLU"
  bottom: "map64_19_conv1"
  top: "map64_19_conv1"
}
layer {
  name: "map64_19_conv2"
  type: "Convolution"
  bottom: "map64_19_conv1"
  top: "map64_19_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_19_bn2"
  type: "BatchNorm"
  bottom: "map64_19_conv2"
  top: "map64_19_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_19_bn2"
  type: "BatchNorm"
  bottom: "map64_19_conv2"
  top: "map64_19_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_19_scale2"
  type: "Scale"
  bottom: "map64_19_conv2"
  top: "map64_19_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_19_relu2"
  type: "ReLU"
  bottom: "map64_19_conv2"
  top: "map64_19_conv2"
}
layer {
  name: "map64_19_conv_end"
  type: "Convolution"
  bottom: "map64_19_conv2"
  top: "map64_19_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_19_eltsum"
  type: "Eltwise"
  bottom: "map64_18_eltsum"
  bottom: "map64_19_conv_end"
  top: "map64_19_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_20_bn_pre"
  type: "BatchNorm"
  bottom: "map64_19_eltsum"
  top: "map64_20_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_20_bn_pre"
  type: "BatchNorm"
  bottom: "map64_19_eltsum"
  top: "map64_20_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_20_pre_scale"
  type: "Scale"
  bottom: "map64_20_bn_pre"
  top: "map64_20_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_20_pre_relu"
  type: "ReLU"
  bottom: "map64_20_bn_pre"
  top: "map64_20_bn_pre"
}
layer {
  name: "map64_20_conv1"
  type: "Convolution"
  bottom: "map64_20_bn_pre"
  top: "map64_20_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_20_bn1"
  type: "BatchNorm"
  bottom: "map64_20_conv1"
  top: "map64_20_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_20_bn1"
  type: "BatchNorm"
  bottom: "map64_20_conv1"
  top: "map64_20_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_20_scale1"
  type: "Scale"
  bottom: "map64_20_conv1"
  top: "map64_20_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_20_relu1"
  type: "ReLU"
  bottom: "map64_20_conv1"
  top: "map64_20_conv1"
}
layer {
  name: "map64_20_conv2"
  type: "Convolution"
  bottom: "map64_20_conv1"
  top: "map64_20_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_20_bn2"
  type: "BatchNorm"
  bottom: "map64_20_conv2"
  top: "map64_20_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_20_bn2"
  type: "BatchNorm"
  bottom: "map64_20_conv2"
  top: "map64_20_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_20_scale2"
  type: "Scale"
  bottom: "map64_20_conv2"
  top: "map64_20_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_20_relu2"
  type: "ReLU"
  bottom: "map64_20_conv2"
  top: "map64_20_conv2"
}
layer {
  name: "map64_20_conv_end"
  type: "Convolution"
  bottom: "map64_20_conv2"
  top: "map64_20_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_20_eltsum"
  type: "Eltwise"
  bottom: "map64_19_eltsum"
  bottom: "map64_20_conv_end"
  top: "map64_20_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_21_bn_pre"
  type: "BatchNorm"
  bottom: "map64_20_eltsum"
  top: "map64_21_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_21_bn_pre"
  type: "BatchNorm"
  bottom: "map64_20_eltsum"
  top: "map64_21_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_21_pre_scale"
  type: "Scale"
  bottom: "map64_21_bn_pre"
  top: "map64_21_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_21_pre_relu"
  type: "ReLU"
  bottom: "map64_21_bn_pre"
  top: "map64_21_bn_pre"
}
layer {
  name: "map64_21_conv1"
  type: "Convolution"
  bottom: "map64_21_bn_pre"
  top: "map64_21_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_21_bn1"
  type: "BatchNorm"
  bottom: "map64_21_conv1"
  top: "map64_21_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_21_bn1"
  type: "BatchNorm"
  bottom: "map64_21_conv1"
  top: "map64_21_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_21_scale1"
  type: "Scale"
  bottom: "map64_21_conv1"
  top: "map64_21_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_21_relu1"
  type: "ReLU"
  bottom: "map64_21_conv1"
  top: "map64_21_conv1"
}
layer {
  name: "map64_21_conv2"
  type: "Convolution"
  bottom: "map64_21_conv1"
  top: "map64_21_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_21_bn2"
  type: "BatchNorm"
  bottom: "map64_21_conv2"
  top: "map64_21_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_21_bn2"
  type: "BatchNorm"
  bottom: "map64_21_conv2"
  top: "map64_21_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_21_scale2"
  type: "Scale"
  bottom: "map64_21_conv2"
  top: "map64_21_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_21_relu2"
  type: "ReLU"
  bottom: "map64_21_conv2"
  top: "map64_21_conv2"
}
layer {
  name: "map64_21_conv_end"
  type: "Convolution"
  bottom: "map64_21_conv2"
  top: "map64_21_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_21_eltsum"
  type: "Eltwise"
  bottom: "map64_20_eltsum"
  bottom: "map64_21_conv_end"
  top: "map64_21_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_22_bn_pre"
  type: "BatchNorm"
  bottom: "map64_21_eltsum"
  top: "map64_22_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_22_bn_pre"
  type: "BatchNorm"
  bottom: "map64_21_eltsum"
  top: "map64_22_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_22_pre_scale"
  type: "Scale"
  bottom: "map64_22_bn_pre"
  top: "map64_22_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_22_pre_relu"
  type: "ReLU"
  bottom: "map64_22_bn_pre"
  top: "map64_22_bn_pre"
}
layer {
  name: "map64_22_conv1"
  type: "Convolution"
  bottom: "map64_22_bn_pre"
  top: "map64_22_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_22_bn1"
  type: "BatchNorm"
  bottom: "map64_22_conv1"
  top: "map64_22_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_22_bn1"
  type: "BatchNorm"
  bottom: "map64_22_conv1"
  top: "map64_22_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_22_scale1"
  type: "Scale"
  bottom: "map64_22_conv1"
  top: "map64_22_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_22_relu1"
  type: "ReLU"
  bottom: "map64_22_conv1"
  top: "map64_22_conv1"
}
layer {
  name: "map64_22_conv2"
  type: "Convolution"
  bottom: "map64_22_conv1"
  top: "map64_22_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_22_bn2"
  type: "BatchNorm"
  bottom: "map64_22_conv2"
  top: "map64_22_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_22_bn2"
  type: "BatchNorm"
  bottom: "map64_22_conv2"
  top: "map64_22_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_22_scale2"
  type: "Scale"
  bottom: "map64_22_conv2"
  top: "map64_22_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_22_relu2"
  type: "ReLU"
  bottom: "map64_22_conv2"
  top: "map64_22_conv2"
}
layer {
  name: "map64_22_conv_end"
  type: "Convolution"
  bottom: "map64_22_conv2"
  top: "map64_22_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_22_eltsum"
  type: "Eltwise"
  bottom: "map64_21_eltsum"
  bottom: "map64_22_conv_end"
  top: "map64_22_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_23_bn_pre"
  type: "BatchNorm"
  bottom: "map64_22_eltsum"
  top: "map64_23_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_23_bn_pre"
  type: "BatchNorm"
  bottom: "map64_22_eltsum"
  top: "map64_23_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_23_pre_scale"
  type: "Scale"
  bottom: "map64_23_bn_pre"
  top: "map64_23_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_23_pre_relu"
  type: "ReLU"
  bottom: "map64_23_bn_pre"
  top: "map64_23_bn_pre"
}
layer {
  name: "map64_23_conv1"
  type: "Convolution"
  bottom: "map64_23_bn_pre"
  top: "map64_23_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_23_bn1"
  type: "BatchNorm"
  bottom: "map64_23_conv1"
  top: "map64_23_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_23_bn1"
  type: "BatchNorm"
  bottom: "map64_23_conv1"
  top: "map64_23_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_23_scale1"
  type: "Scale"
  bottom: "map64_23_conv1"
  top: "map64_23_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_23_relu1"
  type: "ReLU"
  bottom: "map64_23_conv1"
  top: "map64_23_conv1"
}
layer {
  name: "map64_23_conv2"
  type: "Convolution"
  bottom: "map64_23_conv1"
  top: "map64_23_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_23_bn2"
  type: "BatchNorm"
  bottom: "map64_23_conv2"
  top: "map64_23_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_23_bn2"
  type: "BatchNorm"
  bottom: "map64_23_conv2"
  top: "map64_23_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_23_scale2"
  type: "Scale"
  bottom: "map64_23_conv2"
  top: "map64_23_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_23_relu2"
  type: "ReLU"
  bottom: "map64_23_conv2"
  top: "map64_23_conv2"
}
layer {
  name: "map64_23_conv_end"
  type: "Convolution"
  bottom: "map64_23_conv2"
  top: "map64_23_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_23_eltsum"
  type: "Eltwise"
  bottom: "map64_22_eltsum"
  bottom: "map64_23_conv_end"
  top: "map64_23_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_24_bn_pre"
  type: "BatchNorm"
  bottom: "map64_23_eltsum"
  top: "map64_24_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_24_bn_pre"
  type: "BatchNorm"
  bottom: "map64_23_eltsum"
  top: "map64_24_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_24_pre_scale"
  type: "Scale"
  bottom: "map64_24_bn_pre"
  top: "map64_24_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_24_pre_relu"
  type: "ReLU"
  bottom: "map64_24_bn_pre"
  top: "map64_24_bn_pre"
}
layer {
  name: "map64_24_conv1"
  type: "Convolution"
  bottom: "map64_24_bn_pre"
  top: "map64_24_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_24_bn1"
  type: "BatchNorm"
  bottom: "map64_24_conv1"
  top: "map64_24_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_24_bn1"
  type: "BatchNorm"
  bottom: "map64_24_conv1"
  top: "map64_24_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_24_scale1"
  type: "Scale"
  bottom: "map64_24_conv1"
  top: "map64_24_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_24_relu1"
  type: "ReLU"
  bottom: "map64_24_conv1"
  top: "map64_24_conv1"
}
layer {
  name: "map64_24_conv2"
  type: "Convolution"
  bottom: "map64_24_conv1"
  top: "map64_24_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_24_bn2"
  type: "BatchNorm"
  bottom: "map64_24_conv2"
  top: "map64_24_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_24_bn2"
  type: "BatchNorm"
  bottom: "map64_24_conv2"
  top: "map64_24_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_24_scale2"
  type: "Scale"
  bottom: "map64_24_conv2"
  top: "map64_24_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_24_relu2"
  type: "ReLU"
  bottom: "map64_24_conv2"
  top: "map64_24_conv2"
}
layer {
  name: "map64_24_conv_end"
  type: "Convolution"
  bottom: "map64_24_conv2"
  top: "map64_24_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_24_eltsum"
  type: "Eltwise"
  bottom: "map64_23_eltsum"
  bottom: "map64_24_conv_end"
  top: "map64_24_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_25_bn_pre"
  type: "BatchNorm"
  bottom: "map64_24_eltsum"
  top: "map64_25_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_25_bn_pre"
  type: "BatchNorm"
  bottom: "map64_24_eltsum"
  top: "map64_25_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_25_pre_scale"
  type: "Scale"
  bottom: "map64_25_bn_pre"
  top: "map64_25_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_25_pre_relu"
  type: "ReLU"
  bottom: "map64_25_bn_pre"
  top: "map64_25_bn_pre"
}
layer {
  name: "map64_25_conv1"
  type: "Convolution"
  bottom: "map64_25_bn_pre"
  top: "map64_25_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_25_bn1"
  type: "BatchNorm"
  bottom: "map64_25_conv1"
  top: "map64_25_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_25_bn1"
  type: "BatchNorm"
  bottom: "map64_25_conv1"
  top: "map64_25_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_25_scale1"
  type: "Scale"
  bottom: "map64_25_conv1"
  top: "map64_25_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_25_relu1"
  type: "ReLU"
  bottom: "map64_25_conv1"
  top: "map64_25_conv1"
}
layer {
  name: "map64_25_conv2"
  type: "Convolution"
  bottom: "map64_25_conv1"
  top: "map64_25_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_25_bn2"
  type: "BatchNorm"
  bottom: "map64_25_conv2"
  top: "map64_25_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_25_bn2"
  type: "BatchNorm"
  bottom: "map64_25_conv2"
  top: "map64_25_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_25_scale2"
  type: "Scale"
  bottom: "map64_25_conv2"
  top: "map64_25_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_25_relu2"
  type: "ReLU"
  bottom: "map64_25_conv2"
  top: "map64_25_conv2"
}
layer {
  name: "map64_25_conv_end"
  type: "Convolution"
  bottom: "map64_25_conv2"
  top: "map64_25_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_25_eltsum"
  type: "Eltwise"
  bottom: "map64_24_eltsum"
  bottom: "map64_25_conv_end"
  top: "map64_25_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_26_bn_pre"
  type: "BatchNorm"
  bottom: "map64_25_eltsum"
  top: "map64_26_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_26_bn_pre"
  type: "BatchNorm"
  bottom: "map64_25_eltsum"
  top: "map64_26_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_26_pre_scale"
  type: "Scale"
  bottom: "map64_26_bn_pre"
  top: "map64_26_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_26_pre_relu"
  type: "ReLU"
  bottom: "map64_26_bn_pre"
  top: "map64_26_bn_pre"
}
layer {
  name: "map64_26_conv1"
  type: "Convolution"
  bottom: "map64_26_bn_pre"
  top: "map64_26_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_26_bn1"
  type: "BatchNorm"
  bottom: "map64_26_conv1"
  top: "map64_26_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_26_bn1"
  type: "BatchNorm"
  bottom: "map64_26_conv1"
  top: "map64_26_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_26_scale1"
  type: "Scale"
  bottom: "map64_26_conv1"
  top: "map64_26_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_26_relu1"
  type: "ReLU"
  bottom: "map64_26_conv1"
  top: "map64_26_conv1"
}
layer {
  name: "map64_26_conv2"
  type: "Convolution"
  bottom: "map64_26_conv1"
  top: "map64_26_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_26_bn2"
  type: "BatchNorm"
  bottom: "map64_26_conv2"
  top: "map64_26_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_26_bn2"
  type: "BatchNorm"
  bottom: "map64_26_conv2"
  top: "map64_26_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_26_scale2"
  type: "Scale"
  bottom: "map64_26_conv2"
  top: "map64_26_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_26_relu2"
  type: "ReLU"
  bottom: "map64_26_conv2"
  top: "map64_26_conv2"
}
layer {
  name: "map64_26_conv_end"
  type: "Convolution"
  bottom: "map64_26_conv2"
  top: "map64_26_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_26_eltsum"
  type: "Eltwise"
  bottom: "map64_25_eltsum"
  bottom: "map64_26_conv_end"
  top: "map64_26_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_27_bn_pre"
  type: "BatchNorm"
  bottom: "map64_26_eltsum"
  top: "map64_27_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_27_bn_pre"
  type: "BatchNorm"
  bottom: "map64_26_eltsum"
  top: "map64_27_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_27_pre_scale"
  type: "Scale"
  bottom: "map64_27_bn_pre"
  top: "map64_27_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_27_pre_relu"
  type: "ReLU"
  bottom: "map64_27_bn_pre"
  top: "map64_27_bn_pre"
}
layer {
  name: "map64_27_conv1"
  type: "Convolution"
  bottom: "map64_27_bn_pre"
  top: "map64_27_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_27_bn1"
  type: "BatchNorm"
  bottom: "map64_27_conv1"
  top: "map64_27_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_27_bn1"
  type: "BatchNorm"
  bottom: "map64_27_conv1"
  top: "map64_27_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_27_scale1"
  type: "Scale"
  bottom: "map64_27_conv1"
  top: "map64_27_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_27_relu1"
  type: "ReLU"
  bottom: "map64_27_conv1"
  top: "map64_27_conv1"
}
layer {
  name: "map64_27_conv2"
  type: "Convolution"
  bottom: "map64_27_conv1"
  top: "map64_27_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_27_bn2"
  type: "BatchNorm"
  bottom: "map64_27_conv2"
  top: "map64_27_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_27_bn2"
  type: "BatchNorm"
  bottom: "map64_27_conv2"
  top: "map64_27_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_27_scale2"
  type: "Scale"
  bottom: "map64_27_conv2"
  top: "map64_27_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_27_relu2"
  type: "ReLU"
  bottom: "map64_27_conv2"
  top: "map64_27_conv2"
}
layer {
  name: "map64_27_conv_end"
  type: "Convolution"
  bottom: "map64_27_conv2"
  top: "map64_27_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_27_eltsum"
  type: "Eltwise"
  bottom: "map64_26_eltsum"
  bottom: "map64_27_conv_end"
  top: "map64_27_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_28_bn_pre"
  type: "BatchNorm"
  bottom: "map64_27_eltsum"
  top: "map64_28_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_28_bn_pre"
  type: "BatchNorm"
  bottom: "map64_27_eltsum"
  top: "map64_28_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_28_pre_scale"
  type: "Scale"
  bottom: "map64_28_bn_pre"
  top: "map64_28_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_28_pre_relu"
  type: "ReLU"
  bottom: "map64_28_bn_pre"
  top: "map64_28_bn_pre"
}
layer {
  name: "map64_28_conv1"
  type: "Convolution"
  bottom: "map64_28_bn_pre"
  top: "map64_28_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_28_bn1"
  type: "BatchNorm"
  bottom: "map64_28_conv1"
  top: "map64_28_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_28_bn1"
  type: "BatchNorm"
  bottom: "map64_28_conv1"
  top: "map64_28_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_28_scale1"
  type: "Scale"
  bottom: "map64_28_conv1"
  top: "map64_28_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_28_relu1"
  type: "ReLU"
  bottom: "map64_28_conv1"
  top: "map64_28_conv1"
}
layer {
  name: "map64_28_conv2"
  type: "Convolution"
  bottom: "map64_28_conv1"
  top: "map64_28_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_28_bn2"
  type: "BatchNorm"
  bottom: "map64_28_conv2"
  top: "map64_28_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_28_bn2"
  type: "BatchNorm"
  bottom: "map64_28_conv2"
  top: "map64_28_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_28_scale2"
  type: "Scale"
  bottom: "map64_28_conv2"
  top: "map64_28_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_28_relu2"
  type: "ReLU"
  bottom: "map64_28_conv2"
  top: "map64_28_conv2"
}
layer {
  name: "map64_28_conv_end"
  type: "Convolution"
  bottom: "map64_28_conv2"
  top: "map64_28_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_28_eltsum"
  type: "Eltwise"
  bottom: "map64_27_eltsum"
  bottom: "map64_28_conv_end"
  top: "map64_28_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_29_bn_pre"
  type: "BatchNorm"
  bottom: "map64_28_eltsum"
  top: "map64_29_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_29_bn_pre"
  type: "BatchNorm"
  bottom: "map64_28_eltsum"
  top: "map64_29_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_29_pre_scale"
  type: "Scale"
  bottom: "map64_29_bn_pre"
  top: "map64_29_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_29_pre_relu"
  type: "ReLU"
  bottom: "map64_29_bn_pre"
  top: "map64_29_bn_pre"
}
layer {
  name: "map64_29_conv1"
  type: "Convolution"
  bottom: "map64_29_bn_pre"
  top: "map64_29_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_29_bn1"
  type: "BatchNorm"
  bottom: "map64_29_conv1"
  top: "map64_29_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_29_bn1"
  type: "BatchNorm"
  bottom: "map64_29_conv1"
  top: "map64_29_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_29_scale1"
  type: "Scale"
  bottom: "map64_29_conv1"
  top: "map64_29_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_29_relu1"
  type: "ReLU"
  bottom: "map64_29_conv1"
  top: "map64_29_conv1"
}
layer {
  name: "map64_29_conv2"
  type: "Convolution"
  bottom: "map64_29_conv1"
  top: "map64_29_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_29_bn2"
  type: "BatchNorm"
  bottom: "map64_29_conv2"
  top: "map64_29_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_29_bn2"
  type: "BatchNorm"
  bottom: "map64_29_conv2"
  top: "map64_29_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_29_scale2"
  type: "Scale"
  bottom: "map64_29_conv2"
  top: "map64_29_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_29_relu2"
  type: "ReLU"
  bottom: "map64_29_conv2"
  top: "map64_29_conv2"
}
layer {
  name: "map64_29_conv_end"
  type: "Convolution"
  bottom: "map64_29_conv2"
  top: "map64_29_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_29_eltsum"
  type: "Eltwise"
  bottom: "map64_28_eltsum"
  bottom: "map64_29_conv_end"
  top: "map64_29_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_30_bn_pre"
  type: "BatchNorm"
  bottom: "map64_29_eltsum"
  top: "map64_30_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_30_bn_pre"
  type: "BatchNorm"
  bottom: "map64_29_eltsum"
  top: "map64_30_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_30_pre_scale"
  type: "Scale"
  bottom: "map64_30_bn_pre"
  top: "map64_30_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_30_pre_relu"
  type: "ReLU"
  bottom: "map64_30_bn_pre"
  top: "map64_30_bn_pre"
}
layer {
  name: "map64_30_conv1"
  type: "Convolution"
  bottom: "map64_30_bn_pre"
  top: "map64_30_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_30_bn1"
  type: "BatchNorm"
  bottom: "map64_30_conv1"
  top: "map64_30_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_30_bn1"
  type: "BatchNorm"
  bottom: "map64_30_conv1"
  top: "map64_30_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_30_scale1"
  type: "Scale"
  bottom: "map64_30_conv1"
  top: "map64_30_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_30_relu1"
  type: "ReLU"
  bottom: "map64_30_conv1"
  top: "map64_30_conv1"
}
layer {
  name: "map64_30_conv2"
  type: "Convolution"
  bottom: "map64_30_conv1"
  top: "map64_30_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_30_bn2"
  type: "BatchNorm"
  bottom: "map64_30_conv2"
  top: "map64_30_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_30_bn2"
  type: "BatchNorm"
  bottom: "map64_30_conv2"
  top: "map64_30_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_30_scale2"
  type: "Scale"
  bottom: "map64_30_conv2"
  top: "map64_30_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_30_relu2"
  type: "ReLU"
  bottom: "map64_30_conv2"
  top: "map64_30_conv2"
}
layer {
  name: "map64_30_conv_end"
  type: "Convolution"
  bottom: "map64_30_conv2"
  top: "map64_30_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_30_eltsum"
  type: "Eltwise"
  bottom: "map64_29_eltsum"
  bottom: "map64_30_conv_end"
  top: "map64_30_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_31_bn_pre"
  type: "BatchNorm"
  bottom: "map64_30_eltsum"
  top: "map64_31_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_31_bn_pre"
  type: "BatchNorm"
  bottom: "map64_30_eltsum"
  top: "map64_31_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_31_pre_scale"
  type: "Scale"
  bottom: "map64_31_bn_pre"
  top: "map64_31_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_31_pre_relu"
  type: "ReLU"
  bottom: "map64_31_bn_pre"
  top: "map64_31_bn_pre"
}
layer {
  name: "map64_31_conv1"
  type: "Convolution"
  bottom: "map64_31_bn_pre"
  top: "map64_31_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_31_bn1"
  type: "BatchNorm"
  bottom: "map64_31_conv1"
  top: "map64_31_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_31_bn1"
  type: "BatchNorm"
  bottom: "map64_31_conv1"
  top: "map64_31_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_31_scale1"
  type: "Scale"
  bottom: "map64_31_conv1"
  top: "map64_31_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_31_relu1"
  type: "ReLU"
  bottom: "map64_31_conv1"
  top: "map64_31_conv1"
}
layer {
  name: "map64_31_conv2"
  type: "Convolution"
  bottom: "map64_31_conv1"
  top: "map64_31_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_31_bn2"
  type: "BatchNorm"
  bottom: "map64_31_conv2"
  top: "map64_31_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_31_bn2"
  type: "BatchNorm"
  bottom: "map64_31_conv2"
  top: "map64_31_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_31_scale2"
  type: "Scale"
  bottom: "map64_31_conv2"
  top: "map64_31_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_31_relu2"
  type: "ReLU"
  bottom: "map64_31_conv2"
  top: "map64_31_conv2"
}
layer {
  name: "map64_31_conv_end"
  type: "Convolution"
  bottom: "map64_31_conv2"
  top: "map64_31_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_31_eltsum"
  type: "Eltwise"
  bottom: "map64_30_eltsum"
  bottom: "map64_31_conv_end"
  top: "map64_31_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_32_bn_pre"
  type: "BatchNorm"
  bottom: "map64_31_eltsum"
  top: "map64_32_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_32_bn_pre"
  type: "BatchNorm"
  bottom: "map64_31_eltsum"
  top: "map64_32_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_32_pre_scale"
  type: "Scale"
  bottom: "map64_32_bn_pre"
  top: "map64_32_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_32_pre_relu"
  type: "ReLU"
  bottom: "map64_32_bn_pre"
  top: "map64_32_bn_pre"
}
layer {
  name: "map64_32_conv1"
  type: "Convolution"
  bottom: "map64_32_bn_pre"
  top: "map64_32_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_32_bn1"
  type: "BatchNorm"
  bottom: "map64_32_conv1"
  top: "map64_32_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_32_bn1"
  type: "BatchNorm"
  bottom: "map64_32_conv1"
  top: "map64_32_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_32_scale1"
  type: "Scale"
  bottom: "map64_32_conv1"
  top: "map64_32_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_32_relu1"
  type: "ReLU"
  bottom: "map64_32_conv1"
  top: "map64_32_conv1"
}
layer {
  name: "map64_32_conv2"
  type: "Convolution"
  bottom: "map64_32_conv1"
  top: "map64_32_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_32_bn2"
  type: "BatchNorm"
  bottom: "map64_32_conv2"
  top: "map64_32_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_32_bn2"
  type: "BatchNorm"
  bottom: "map64_32_conv2"
  top: "map64_32_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_32_scale2"
  type: "Scale"
  bottom: "map64_32_conv2"
  top: "map64_32_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_32_relu2"
  type: "ReLU"
  bottom: "map64_32_conv2"
  top: "map64_32_conv2"
}
layer {
  name: "map64_32_conv_end"
  type: "Convolution"
  bottom: "map64_32_conv2"
  top: "map64_32_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_32_eltsum"
  type: "Eltwise"
  bottom: "map64_31_eltsum"
  bottom: "map64_32_conv_end"
  top: "map64_32_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_33_bn_pre"
  type: "BatchNorm"
  bottom: "map64_32_eltsum"
  top: "map64_33_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_33_bn_pre"
  type: "BatchNorm"
  bottom: "map64_32_eltsum"
  top: "map64_33_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_33_pre_scale"
  type: "Scale"
  bottom: "map64_33_bn_pre"
  top: "map64_33_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_33_pre_relu"
  type: "ReLU"
  bottom: "map64_33_bn_pre"
  top: "map64_33_bn_pre"
}
layer {
  name: "map64_33_conv1"
  type: "Convolution"
  bottom: "map64_33_bn_pre"
  top: "map64_33_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_33_bn1"
  type: "BatchNorm"
  bottom: "map64_33_conv1"
  top: "map64_33_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_33_bn1"
  type: "BatchNorm"
  bottom: "map64_33_conv1"
  top: "map64_33_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_33_scale1"
  type: "Scale"
  bottom: "map64_33_conv1"
  top: "map64_33_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_33_relu1"
  type: "ReLU"
  bottom: "map64_33_conv1"
  top: "map64_33_conv1"
}
layer {
  name: "map64_33_conv2"
  type: "Convolution"
  bottom: "map64_33_conv1"
  top: "map64_33_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_33_bn2"
  type: "BatchNorm"
  bottom: "map64_33_conv2"
  top: "map64_33_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_33_bn2"
  type: "BatchNorm"
  bottom: "map64_33_conv2"
  top: "map64_33_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_33_scale2"
  type: "Scale"
  bottom: "map64_33_conv2"
  top: "map64_33_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_33_relu2"
  type: "ReLU"
  bottom: "map64_33_conv2"
  top: "map64_33_conv2"
}
layer {
  name: "map64_33_conv_end"
  type: "Convolution"
  bottom: "map64_33_conv2"
  top: "map64_33_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_33_eltsum"
  type: "Eltwise"
  bottom: "map64_32_eltsum"
  bottom: "map64_33_conv_end"
  top: "map64_33_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_34_bn_pre"
  type: "BatchNorm"
  bottom: "map64_33_eltsum"
  top: "map64_34_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_34_bn_pre"
  type: "BatchNorm"
  bottom: "map64_33_eltsum"
  top: "map64_34_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_34_pre_scale"
  type: "Scale"
  bottom: "map64_34_bn_pre"
  top: "map64_34_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_34_pre_relu"
  type: "ReLU"
  bottom: "map64_34_bn_pre"
  top: "map64_34_bn_pre"
}
layer {
  name: "map64_34_conv1"
  type: "Convolution"
  bottom: "map64_34_bn_pre"
  top: "map64_34_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_34_bn1"
  type: "BatchNorm"
  bottom: "map64_34_conv1"
  top: "map64_34_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_34_bn1"
  type: "BatchNorm"
  bottom: "map64_34_conv1"
  top: "map64_34_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_34_scale1"
  type: "Scale"
  bottom: "map64_34_conv1"
  top: "map64_34_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_34_relu1"
  type: "ReLU"
  bottom: "map64_34_conv1"
  top: "map64_34_conv1"
}
layer {
  name: "map64_34_conv2"
  type: "Convolution"
  bottom: "map64_34_conv1"
  top: "map64_34_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_34_bn2"
  type: "BatchNorm"
  bottom: "map64_34_conv2"
  top: "map64_34_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_34_bn2"
  type: "BatchNorm"
  bottom: "map64_34_conv2"
  top: "map64_34_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_34_scale2"
  type: "Scale"
  bottom: "map64_34_conv2"
  top: "map64_34_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_34_relu2"
  type: "ReLU"
  bottom: "map64_34_conv2"
  top: "map64_34_conv2"
}
layer {
  name: "map64_34_conv_end"
  type: "Convolution"
  bottom: "map64_34_conv2"
  top: "map64_34_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_34_eltsum"
  type: "Eltwise"
  bottom: "map64_33_eltsum"
  bottom: "map64_34_conv_end"
  top: "map64_34_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_35_bn_pre"
  type: "BatchNorm"
  bottom: "map64_34_eltsum"
  top: "map64_35_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_35_bn_pre"
  type: "BatchNorm"
  bottom: "map64_34_eltsum"
  top: "map64_35_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_35_pre_scale"
  type: "Scale"
  bottom: "map64_35_bn_pre"
  top: "map64_35_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_35_pre_relu"
  type: "ReLU"
  bottom: "map64_35_bn_pre"
  top: "map64_35_bn_pre"
}
layer {
  name: "map64_35_conv1"
  type: "Convolution"
  bottom: "map64_35_bn_pre"
  top: "map64_35_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_35_bn1"
  type: "BatchNorm"
  bottom: "map64_35_conv1"
  top: "map64_35_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_35_bn1"
  type: "BatchNorm"
  bottom: "map64_35_conv1"
  top: "map64_35_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_35_scale1"
  type: "Scale"
  bottom: "map64_35_conv1"
  top: "map64_35_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_35_relu1"
  type: "ReLU"
  bottom: "map64_35_conv1"
  top: "map64_35_conv1"
}
layer {
  name: "map64_35_conv2"
  type: "Convolution"
  bottom: "map64_35_conv1"
  top: "map64_35_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_35_bn2"
  type: "BatchNorm"
  bottom: "map64_35_conv2"
  top: "map64_35_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_35_bn2"
  type: "BatchNorm"
  bottom: "map64_35_conv2"
  top: "map64_35_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_35_scale2"
  type: "Scale"
  bottom: "map64_35_conv2"
  top: "map64_35_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_35_relu2"
  type: "ReLU"
  bottom: "map64_35_conv2"
  top: "map64_35_conv2"
}
layer {
  name: "map64_35_conv_end"
  type: "Convolution"
  bottom: "map64_35_conv2"
  top: "map64_35_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_35_eltsum"
  type: "Eltwise"
  bottom: "map64_34_eltsum"
  bottom: "map64_35_conv_end"
  top: "map64_35_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_36_bn_pre"
  type: "BatchNorm"
  bottom: "map64_35_eltsum"
  top: "map64_36_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_36_bn_pre"
  type: "BatchNorm"
  bottom: "map64_35_eltsum"
  top: "map64_36_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_36_pre_scale"
  type: "Scale"
  bottom: "map64_36_bn_pre"
  top: "map64_36_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_36_pre_relu"
  type: "ReLU"
  bottom: "map64_36_bn_pre"
  top: "map64_36_bn_pre"
}
layer {
  name: "map64_36_conv1"
  type: "Convolution"
  bottom: "map64_36_bn_pre"
  top: "map64_36_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_36_bn1"
  type: "BatchNorm"
  bottom: "map64_36_conv1"
  top: "map64_36_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_36_bn1"
  type: "BatchNorm"
  bottom: "map64_36_conv1"
  top: "map64_36_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_36_scale1"
  type: "Scale"
  bottom: "map64_36_conv1"
  top: "map64_36_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_36_relu1"
  type: "ReLU"
  bottom: "map64_36_conv1"
  top: "map64_36_conv1"
}
layer {
  name: "map64_36_conv2"
  type: "Convolution"
  bottom: "map64_36_conv1"
  top: "map64_36_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_36_bn2"
  type: "BatchNorm"
  bottom: "map64_36_conv2"
  top: "map64_36_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_36_bn2"
  type: "BatchNorm"
  bottom: "map64_36_conv2"
  top: "map64_36_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_36_scale2"
  type: "Scale"
  bottom: "map64_36_conv2"
  top: "map64_36_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_36_relu2"
  type: "ReLU"
  bottom: "map64_36_conv2"
  top: "map64_36_conv2"
}
layer {
  name: "map64_36_conv_end"
  type: "Convolution"
  bottom: "map64_36_conv2"
  top: "map64_36_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_36_eltsum"
  type: "Eltwise"
  bottom: "map64_35_eltsum"
  bottom: "map64_36_conv_end"
  top: "map64_36_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_37_bn_pre"
  type: "BatchNorm"
  bottom: "map64_36_eltsum"
  top: "map64_37_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_37_bn_pre"
  type: "BatchNorm"
  bottom: "map64_36_eltsum"
  top: "map64_37_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_37_pre_scale"
  type: "Scale"
  bottom: "map64_37_bn_pre"
  top: "map64_37_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_37_pre_relu"
  type: "ReLU"
  bottom: "map64_37_bn_pre"
  top: "map64_37_bn_pre"
}
layer {
  name: "map64_37_conv1"
  type: "Convolution"
  bottom: "map64_37_bn_pre"
  top: "map64_37_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_37_bn1"
  type: "BatchNorm"
  bottom: "map64_37_conv1"
  top: "map64_37_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_37_bn1"
  type: "BatchNorm"
  bottom: "map64_37_conv1"
  top: "map64_37_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_37_scale1"
  type: "Scale"
  bottom: "map64_37_conv1"
  top: "map64_37_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_37_relu1"
  type: "ReLU"
  bottom: "map64_37_conv1"
  top: "map64_37_conv1"
}
layer {
  name: "map64_37_conv2"
  type: "Convolution"
  bottom: "map64_37_conv1"
  top: "map64_37_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_37_bn2"
  type: "BatchNorm"
  bottom: "map64_37_conv2"
  top: "map64_37_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_37_bn2"
  type: "BatchNorm"
  bottom: "map64_37_conv2"
  top: "map64_37_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_37_scale2"
  type: "Scale"
  bottom: "map64_37_conv2"
  top: "map64_37_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_37_relu2"
  type: "ReLU"
  bottom: "map64_37_conv2"
  top: "map64_37_conv2"
}
layer {
  name: "map64_37_conv_end"
  type: "Convolution"
  bottom: "map64_37_conv2"
  top: "map64_37_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_37_eltsum"
  type: "Eltwise"
  bottom: "map64_36_eltsum"
  bottom: "map64_37_conv_end"
  top: "map64_37_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_38_bn_pre"
  type: "BatchNorm"
  bottom: "map64_37_eltsum"
  top: "map64_38_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_38_bn_pre"
  type: "BatchNorm"
  bottom: "map64_37_eltsum"
  top: "map64_38_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_38_pre_scale"
  type: "Scale"
  bottom: "map64_38_bn_pre"
  top: "map64_38_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_38_pre_relu"
  type: "ReLU"
  bottom: "map64_38_bn_pre"
  top: "map64_38_bn_pre"
}
layer {
  name: "map64_38_conv1"
  type: "Convolution"
  bottom: "map64_38_bn_pre"
  top: "map64_38_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_38_bn1"
  type: "BatchNorm"
  bottom: "map64_38_conv1"
  top: "map64_38_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_38_bn1"
  type: "BatchNorm"
  bottom: "map64_38_conv1"
  top: "map64_38_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_38_scale1"
  type: "Scale"
  bottom: "map64_38_conv1"
  top: "map64_38_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_38_relu1"
  type: "ReLU"
  bottom: "map64_38_conv1"
  top: "map64_38_conv1"
}
layer {
  name: "map64_38_conv2"
  type: "Convolution"
  bottom: "map64_38_conv1"
  top: "map64_38_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_38_bn2"
  type: "BatchNorm"
  bottom: "map64_38_conv2"
  top: "map64_38_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_38_bn2"
  type: "BatchNorm"
  bottom: "map64_38_conv2"
  top: "map64_38_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_38_scale2"
  type: "Scale"
  bottom: "map64_38_conv2"
  top: "map64_38_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_38_relu2"
  type: "ReLU"
  bottom: "map64_38_conv2"
  top: "map64_38_conv2"
}
layer {
  name: "map64_38_conv_end"
  type: "Convolution"
  bottom: "map64_38_conv2"
  top: "map64_38_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_38_eltsum"
  type: "Eltwise"
  bottom: "map64_37_eltsum"
  bottom: "map64_38_conv_end"
  top: "map64_38_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_39_bn_pre"
  type: "BatchNorm"
  bottom: "map64_38_eltsum"
  top: "map64_39_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_39_bn_pre"
  type: "BatchNorm"
  bottom: "map64_38_eltsum"
  top: "map64_39_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_39_pre_scale"
  type: "Scale"
  bottom: "map64_39_bn_pre"
  top: "map64_39_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_39_pre_relu"
  type: "ReLU"
  bottom: "map64_39_bn_pre"
  top: "map64_39_bn_pre"
}
layer {
  name: "map64_39_conv1"
  type: "Convolution"
  bottom: "map64_39_bn_pre"
  top: "map64_39_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_39_bn1"
  type: "BatchNorm"
  bottom: "map64_39_conv1"
  top: "map64_39_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_39_bn1"
  type: "BatchNorm"
  bottom: "map64_39_conv1"
  top: "map64_39_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_39_scale1"
  type: "Scale"
  bottom: "map64_39_conv1"
  top: "map64_39_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_39_relu1"
  type: "ReLU"
  bottom: "map64_39_conv1"
  top: "map64_39_conv1"
}
layer {
  name: "map64_39_conv2"
  type: "Convolution"
  bottom: "map64_39_conv1"
  top: "map64_39_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_39_bn2"
  type: "BatchNorm"
  bottom: "map64_39_conv2"
  top: "map64_39_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_39_bn2"
  type: "BatchNorm"
  bottom: "map64_39_conv2"
  top: "map64_39_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_39_scale2"
  type: "Scale"
  bottom: "map64_39_conv2"
  top: "map64_39_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_39_relu2"
  type: "ReLU"
  bottom: "map64_39_conv2"
  top: "map64_39_conv2"
}
layer {
  name: "map64_39_conv_end"
  type: "Convolution"
  bottom: "map64_39_conv2"
  top: "map64_39_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_39_eltsum"
  type: "Eltwise"
  bottom: "map64_38_eltsum"
  bottom: "map64_39_conv_end"
  top: "map64_39_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_40_bn_pre"
  type: "BatchNorm"
  bottom: "map64_39_eltsum"
  top: "map64_40_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_40_bn_pre"
  type: "BatchNorm"
  bottom: "map64_39_eltsum"
  top: "map64_40_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_40_pre_scale"
  type: "Scale"
  bottom: "map64_40_bn_pre"
  top: "map64_40_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_40_pre_relu"
  type: "ReLU"
  bottom: "map64_40_bn_pre"
  top: "map64_40_bn_pre"
}
layer {
  name: "map64_40_conv1"
  type: "Convolution"
  bottom: "map64_40_bn_pre"
  top: "map64_40_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_40_bn1"
  type: "BatchNorm"
  bottom: "map64_40_conv1"
  top: "map64_40_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_40_bn1"
  type: "BatchNorm"
  bottom: "map64_40_conv1"
  top: "map64_40_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_40_scale1"
  type: "Scale"
  bottom: "map64_40_conv1"
  top: "map64_40_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_40_relu1"
  type: "ReLU"
  bottom: "map64_40_conv1"
  top: "map64_40_conv1"
}
layer {
  name: "map64_40_conv2"
  type: "Convolution"
  bottom: "map64_40_conv1"
  top: "map64_40_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_40_bn2"
  type: "BatchNorm"
  bottom: "map64_40_conv2"
  top: "map64_40_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_40_bn2"
  type: "BatchNorm"
  bottom: "map64_40_conv2"
  top: "map64_40_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_40_scale2"
  type: "Scale"
  bottom: "map64_40_conv2"
  top: "map64_40_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_40_relu2"
  type: "ReLU"
  bottom: "map64_40_conv2"
  top: "map64_40_conv2"
}
layer {
  name: "map64_40_conv_end"
  type: "Convolution"
  bottom: "map64_40_conv2"
  top: "map64_40_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_40_eltsum"
  type: "Eltwise"
  bottom: "map64_39_eltsum"
  bottom: "map64_40_conv_end"
  top: "map64_40_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_41_bn_pre"
  type: "BatchNorm"
  bottom: "map64_40_eltsum"
  top: "map64_41_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_41_bn_pre"
  type: "BatchNorm"
  bottom: "map64_40_eltsum"
  top: "map64_41_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_41_pre_scale"
  type: "Scale"
  bottom: "map64_41_bn_pre"
  top: "map64_41_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_41_pre_relu"
  type: "ReLU"
  bottom: "map64_41_bn_pre"
  top: "map64_41_bn_pre"
}
layer {
  name: "map64_41_conv1"
  type: "Convolution"
  bottom: "map64_41_bn_pre"
  top: "map64_41_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_41_bn1"
  type: "BatchNorm"
  bottom: "map64_41_conv1"
  top: "map64_41_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_41_bn1"
  type: "BatchNorm"
  bottom: "map64_41_conv1"
  top: "map64_41_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_41_scale1"
  type: "Scale"
  bottom: "map64_41_conv1"
  top: "map64_41_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_41_relu1"
  type: "ReLU"
  bottom: "map64_41_conv1"
  top: "map64_41_conv1"
}
layer {
  name: "map64_41_conv2"
  type: "Convolution"
  bottom: "map64_41_conv1"
  top: "map64_41_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_41_bn2"
  type: "BatchNorm"
  bottom: "map64_41_conv2"
  top: "map64_41_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_41_bn2"
  type: "BatchNorm"
  bottom: "map64_41_conv2"
  top: "map64_41_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_41_scale2"
  type: "Scale"
  bottom: "map64_41_conv2"
  top: "map64_41_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_41_relu2"
  type: "ReLU"
  bottom: "map64_41_conv2"
  top: "map64_41_conv2"
}
layer {
  name: "map64_41_conv_end"
  type: "Convolution"
  bottom: "map64_41_conv2"
  top: "map64_41_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_41_eltsum"
  type: "Eltwise"
  bottom: "map64_40_eltsum"
  bottom: "map64_41_conv_end"
  top: "map64_41_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_42_bn_pre"
  type: "BatchNorm"
  bottom: "map64_41_eltsum"
  top: "map64_42_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_42_bn_pre"
  type: "BatchNorm"
  bottom: "map64_41_eltsum"
  top: "map64_42_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_42_pre_scale"
  type: "Scale"
  bottom: "map64_42_bn_pre"
  top: "map64_42_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_42_pre_relu"
  type: "ReLU"
  bottom: "map64_42_bn_pre"
  top: "map64_42_bn_pre"
}
layer {
  name: "map64_42_conv1"
  type: "Convolution"
  bottom: "map64_42_bn_pre"
  top: "map64_42_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_42_bn1"
  type: "BatchNorm"
  bottom: "map64_42_conv1"
  top: "map64_42_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_42_bn1"
  type: "BatchNorm"
  bottom: "map64_42_conv1"
  top: "map64_42_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_42_scale1"
  type: "Scale"
  bottom: "map64_42_conv1"
  top: "map64_42_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_42_relu1"
  type: "ReLU"
  bottom: "map64_42_conv1"
  top: "map64_42_conv1"
}
layer {
  name: "map64_42_conv2"
  type: "Convolution"
  bottom: "map64_42_conv1"
  top: "map64_42_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_42_bn2"
  type: "BatchNorm"
  bottom: "map64_42_conv2"
  top: "map64_42_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_42_bn2"
  type: "BatchNorm"
  bottom: "map64_42_conv2"
  top: "map64_42_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_42_scale2"
  type: "Scale"
  bottom: "map64_42_conv2"
  top: "map64_42_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_42_relu2"
  type: "ReLU"
  bottom: "map64_42_conv2"
  top: "map64_42_conv2"
}
layer {
  name: "map64_42_conv_end"
  type: "Convolution"
  bottom: "map64_42_conv2"
  top: "map64_42_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_42_eltsum"
  type: "Eltwise"
  bottom: "map64_41_eltsum"
  bottom: "map64_42_conv_end"
  top: "map64_42_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_43_bn_pre"
  type: "BatchNorm"
  bottom: "map64_42_eltsum"
  top: "map64_43_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_43_bn_pre"
  type: "BatchNorm"
  bottom: "map64_42_eltsum"
  top: "map64_43_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_43_pre_scale"
  type: "Scale"
  bottom: "map64_43_bn_pre"
  top: "map64_43_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_43_pre_relu"
  type: "ReLU"
  bottom: "map64_43_bn_pre"
  top: "map64_43_bn_pre"
}
layer {
  name: "map64_43_conv1"
  type: "Convolution"
  bottom: "map64_43_bn_pre"
  top: "map64_43_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_43_bn1"
  type: "BatchNorm"
  bottom: "map64_43_conv1"
  top: "map64_43_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_43_bn1"
  type: "BatchNorm"
  bottom: "map64_43_conv1"
  top: "map64_43_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_43_scale1"
  type: "Scale"
  bottom: "map64_43_conv1"
  top: "map64_43_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_43_relu1"
  type: "ReLU"
  bottom: "map64_43_conv1"
  top: "map64_43_conv1"
}
layer {
  name: "map64_43_conv2"
  type: "Convolution"
  bottom: "map64_43_conv1"
  top: "map64_43_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_43_bn2"
  type: "BatchNorm"
  bottom: "map64_43_conv2"
  top: "map64_43_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_43_bn2"
  type: "BatchNorm"
  bottom: "map64_43_conv2"
  top: "map64_43_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_43_scale2"
  type: "Scale"
  bottom: "map64_43_conv2"
  top: "map64_43_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_43_relu2"
  type: "ReLU"
  bottom: "map64_43_conv2"
  top: "map64_43_conv2"
}
layer {
  name: "map64_43_conv_end"
  type: "Convolution"
  bottom: "map64_43_conv2"
  top: "map64_43_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_43_eltsum"
  type: "Eltwise"
  bottom: "map64_42_eltsum"
  bottom: "map64_43_conv_end"
  top: "map64_43_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_44_bn_pre"
  type: "BatchNorm"
  bottom: "map64_43_eltsum"
  top: "map64_44_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_44_bn_pre"
  type: "BatchNorm"
  bottom: "map64_43_eltsum"
  top: "map64_44_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_44_pre_scale"
  type: "Scale"
  bottom: "map64_44_bn_pre"
  top: "map64_44_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_44_pre_relu"
  type: "ReLU"
  bottom: "map64_44_bn_pre"
  top: "map64_44_bn_pre"
}
layer {
  name: "map64_44_conv1"
  type: "Convolution"
  bottom: "map64_44_bn_pre"
  top: "map64_44_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_44_bn1"
  type: "BatchNorm"
  bottom: "map64_44_conv1"
  top: "map64_44_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_44_bn1"
  type: "BatchNorm"
  bottom: "map64_44_conv1"
  top: "map64_44_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_44_scale1"
  type: "Scale"
  bottom: "map64_44_conv1"
  top: "map64_44_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_44_relu1"
  type: "ReLU"
  bottom: "map64_44_conv1"
  top: "map64_44_conv1"
}
layer {
  name: "map64_44_conv2"
  type: "Convolution"
  bottom: "map64_44_conv1"
  top: "map64_44_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_44_bn2"
  type: "BatchNorm"
  bottom: "map64_44_conv2"
  top: "map64_44_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_44_bn2"
  type: "BatchNorm"
  bottom: "map64_44_conv2"
  top: "map64_44_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_44_scale2"
  type: "Scale"
  bottom: "map64_44_conv2"
  top: "map64_44_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_44_relu2"
  type: "ReLU"
  bottom: "map64_44_conv2"
  top: "map64_44_conv2"
}
layer {
  name: "map64_44_conv_end"
  type: "Convolution"
  bottom: "map64_44_conv2"
  top: "map64_44_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_44_eltsum"
  type: "Eltwise"
  bottom: "map64_43_eltsum"
  bottom: "map64_44_conv_end"
  top: "map64_44_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_45_bn_pre"
  type: "BatchNorm"
  bottom: "map64_44_eltsum"
  top: "map64_45_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_45_bn_pre"
  type: "BatchNorm"
  bottom: "map64_44_eltsum"
  top: "map64_45_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_45_pre_scale"
  type: "Scale"
  bottom: "map64_45_bn_pre"
  top: "map64_45_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_45_pre_relu"
  type: "ReLU"
  bottom: "map64_45_bn_pre"
  top: "map64_45_bn_pre"
}
layer {
  name: "map64_45_conv1"
  type: "Convolution"
  bottom: "map64_45_bn_pre"
  top: "map64_45_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_45_bn1"
  type: "BatchNorm"
  bottom: "map64_45_conv1"
  top: "map64_45_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_45_bn1"
  type: "BatchNorm"
  bottom: "map64_45_conv1"
  top: "map64_45_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_45_scale1"
  type: "Scale"
  bottom: "map64_45_conv1"
  top: "map64_45_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_45_relu1"
  type: "ReLU"
  bottom: "map64_45_conv1"
  top: "map64_45_conv1"
}
layer {
  name: "map64_45_conv2"
  type: "Convolution"
  bottom: "map64_45_conv1"
  top: "map64_45_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_45_bn2"
  type: "BatchNorm"
  bottom: "map64_45_conv2"
  top: "map64_45_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_45_bn2"
  type: "BatchNorm"
  bottom: "map64_45_conv2"
  top: "map64_45_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_45_scale2"
  type: "Scale"
  bottom: "map64_45_conv2"
  top: "map64_45_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_45_relu2"
  type: "ReLU"
  bottom: "map64_45_conv2"
  top: "map64_45_conv2"
}
layer {
  name: "map64_45_conv_end"
  type: "Convolution"
  bottom: "map64_45_conv2"
  top: "map64_45_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_45_eltsum"
  type: "Eltwise"
  bottom: "map64_44_eltsum"
  bottom: "map64_45_conv_end"
  top: "map64_45_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_46_bn_pre"
  type: "BatchNorm"
  bottom: "map64_45_eltsum"
  top: "map64_46_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_46_bn_pre"
  type: "BatchNorm"
  bottom: "map64_45_eltsum"
  top: "map64_46_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_46_pre_scale"
  type: "Scale"
  bottom: "map64_46_bn_pre"
  top: "map64_46_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_46_pre_relu"
  type: "ReLU"
  bottom: "map64_46_bn_pre"
  top: "map64_46_bn_pre"
}
layer {
  name: "map64_46_conv1"
  type: "Convolution"
  bottom: "map64_46_bn_pre"
  top: "map64_46_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_46_bn1"
  type: "BatchNorm"
  bottom: "map64_46_conv1"
  top: "map64_46_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_46_bn1"
  type: "BatchNorm"
  bottom: "map64_46_conv1"
  top: "map64_46_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_46_scale1"
  type: "Scale"
  bottom: "map64_46_conv1"
  top: "map64_46_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_46_relu1"
  type: "ReLU"
  bottom: "map64_46_conv1"
  top: "map64_46_conv1"
}
layer {
  name: "map64_46_conv2"
  type: "Convolution"
  bottom: "map64_46_conv1"
  top: "map64_46_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_46_bn2"
  type: "BatchNorm"
  bottom: "map64_46_conv2"
  top: "map64_46_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_46_bn2"
  type: "BatchNorm"
  bottom: "map64_46_conv2"
  top: "map64_46_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_46_scale2"
  type: "Scale"
  bottom: "map64_46_conv2"
  top: "map64_46_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_46_relu2"
  type: "ReLU"
  bottom: "map64_46_conv2"
  top: "map64_46_conv2"
}
layer {
  name: "map64_46_conv_end"
  type: "Convolution"
  bottom: "map64_46_conv2"
  top: "map64_46_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_46_eltsum"
  type: "Eltwise"
  bottom: "map64_45_eltsum"
  bottom: "map64_46_conv_end"
  top: "map64_46_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_47_bn_pre"
  type: "BatchNorm"
  bottom: "map64_46_eltsum"
  top: "map64_47_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_47_bn_pre"
  type: "BatchNorm"
  bottom: "map64_46_eltsum"
  top: "map64_47_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_47_pre_scale"
  type: "Scale"
  bottom: "map64_47_bn_pre"
  top: "map64_47_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_47_pre_relu"
  type: "ReLU"
  bottom: "map64_47_bn_pre"
  top: "map64_47_bn_pre"
}
layer {
  name: "map64_47_conv1"
  type: "Convolution"
  bottom: "map64_47_bn_pre"
  top: "map64_47_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_47_bn1"
  type: "BatchNorm"
  bottom: "map64_47_conv1"
  top: "map64_47_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_47_bn1"
  type: "BatchNorm"
  bottom: "map64_47_conv1"
  top: "map64_47_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_47_scale1"
  type: "Scale"
  bottom: "map64_47_conv1"
  top: "map64_47_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_47_relu1"
  type: "ReLU"
  bottom: "map64_47_conv1"
  top: "map64_47_conv1"
}
layer {
  name: "map64_47_conv2"
  type: "Convolution"
  bottom: "map64_47_conv1"
  top: "map64_47_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_47_bn2"
  type: "BatchNorm"
  bottom: "map64_47_conv2"
  top: "map64_47_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_47_bn2"
  type: "BatchNorm"
  bottom: "map64_47_conv2"
  top: "map64_47_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_47_scale2"
  type: "Scale"
  bottom: "map64_47_conv2"
  top: "map64_47_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_47_relu2"
  type: "ReLU"
  bottom: "map64_47_conv2"
  top: "map64_47_conv2"
}
layer {
  name: "map64_47_conv_end"
  type: "Convolution"
  bottom: "map64_47_conv2"
  top: "map64_47_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_47_eltsum"
  type: "Eltwise"
  bottom: "map64_46_eltsum"
  bottom: "map64_47_conv_end"
  top: "map64_47_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_48_bn_pre"
  type: "BatchNorm"
  bottom: "map64_47_eltsum"
  top: "map64_48_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_48_bn_pre"
  type: "BatchNorm"
  bottom: "map64_47_eltsum"
  top: "map64_48_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_48_pre_scale"
  type: "Scale"
  bottom: "map64_48_bn_pre"
  top: "map64_48_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_48_pre_relu"
  type: "ReLU"
  bottom: "map64_48_bn_pre"
  top: "map64_48_bn_pre"
}
layer {
  name: "map64_48_conv1"
  type: "Convolution"
  bottom: "map64_48_bn_pre"
  top: "map64_48_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_48_bn1"
  type: "BatchNorm"
  bottom: "map64_48_conv1"
  top: "map64_48_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_48_bn1"
  type: "BatchNorm"
  bottom: "map64_48_conv1"
  top: "map64_48_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_48_scale1"
  type: "Scale"
  bottom: "map64_48_conv1"
  top: "map64_48_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_48_relu1"
  type: "ReLU"
  bottom: "map64_48_conv1"
  top: "map64_48_conv1"
}
layer {
  name: "map64_48_conv2"
  type: "Convolution"
  bottom: "map64_48_conv1"
  top: "map64_48_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_48_bn2"
  type: "BatchNorm"
  bottom: "map64_48_conv2"
  top: "map64_48_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_48_bn2"
  type: "BatchNorm"
  bottom: "map64_48_conv2"
  top: "map64_48_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_48_scale2"
  type: "Scale"
  bottom: "map64_48_conv2"
  top: "map64_48_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_48_relu2"
  type: "ReLU"
  bottom: "map64_48_conv2"
  top: "map64_48_conv2"
}
layer {
  name: "map64_48_conv_end"
  type: "Convolution"
  bottom: "map64_48_conv2"
  top: "map64_48_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_48_eltsum"
  type: "Eltwise"
  bottom: "map64_47_eltsum"
  bottom: "map64_48_conv_end"
  top: "map64_48_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_49_bn_pre"
  type: "BatchNorm"
  bottom: "map64_48_eltsum"
  top: "map64_49_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_49_bn_pre"
  type: "BatchNorm"
  bottom: "map64_48_eltsum"
  top: "map64_49_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_49_pre_scale"
  type: "Scale"
  bottom: "map64_49_bn_pre"
  top: "map64_49_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_49_pre_relu"
  type: "ReLU"
  bottom: "map64_49_bn_pre"
  top: "map64_49_bn_pre"
}
layer {
  name: "map64_49_conv1"
  type: "Convolution"
  bottom: "map64_49_bn_pre"
  top: "map64_49_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_49_bn1"
  type: "BatchNorm"
  bottom: "map64_49_conv1"
  top: "map64_49_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_49_bn1"
  type: "BatchNorm"
  bottom: "map64_49_conv1"
  top: "map64_49_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_49_scale1"
  type: "Scale"
  bottom: "map64_49_conv1"
  top: "map64_49_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_49_relu1"
  type: "ReLU"
  bottom: "map64_49_conv1"
  top: "map64_49_conv1"
}
layer {
  name: "map64_49_conv2"
  type: "Convolution"
  bottom: "map64_49_conv1"
  top: "map64_49_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_49_bn2"
  type: "BatchNorm"
  bottom: "map64_49_conv2"
  top: "map64_49_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_49_bn2"
  type: "BatchNorm"
  bottom: "map64_49_conv2"
  top: "map64_49_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_49_scale2"
  type: "Scale"
  bottom: "map64_49_conv2"
  top: "map64_49_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_49_relu2"
  type: "ReLU"
  bottom: "map64_49_conv2"
  top: "map64_49_conv2"
}
layer {
  name: "map64_49_conv_end"
  type: "Convolution"
  bottom: "map64_49_conv2"
  top: "map64_49_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_49_eltsum"
  type: "Eltwise"
  bottom: "map64_48_eltsum"
  bottom: "map64_49_conv_end"
  top: "map64_49_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_50_bn_pre"
  type: "BatchNorm"
  bottom: "map64_49_eltsum"
  top: "map64_50_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_50_bn_pre"
  type: "BatchNorm"
  bottom: "map64_49_eltsum"
  top: "map64_50_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_50_pre_scale"
  type: "Scale"
  bottom: "map64_50_bn_pre"
  top: "map64_50_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_50_pre_relu"
  type: "ReLU"
  bottom: "map64_50_bn_pre"
  top: "map64_50_bn_pre"
}
layer {
  name: "map64_50_conv1"
  type: "Convolution"
  bottom: "map64_50_bn_pre"
  top: "map64_50_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_50_bn1"
  type: "BatchNorm"
  bottom: "map64_50_conv1"
  top: "map64_50_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_50_bn1"
  type: "BatchNorm"
  bottom: "map64_50_conv1"
  top: "map64_50_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_50_scale1"
  type: "Scale"
  bottom: "map64_50_conv1"
  top: "map64_50_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_50_relu1"
  type: "ReLU"
  bottom: "map64_50_conv1"
  top: "map64_50_conv1"
}
layer {
  name: "map64_50_conv2"
  type: "Convolution"
  bottom: "map64_50_conv1"
  top: "map64_50_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_50_bn2"
  type: "BatchNorm"
  bottom: "map64_50_conv2"
  top: "map64_50_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_50_bn2"
  type: "BatchNorm"
  bottom: "map64_50_conv2"
  top: "map64_50_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_50_scale2"
  type: "Scale"
  bottom: "map64_50_conv2"
  top: "map64_50_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_50_relu2"
  type: "ReLU"
  bottom: "map64_50_conv2"
  top: "map64_50_conv2"
}
layer {
  name: "map64_50_conv_end"
  type: "Convolution"
  bottom: "map64_50_conv2"
  top: "map64_50_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_50_eltsum"
  type: "Eltwise"
  bottom: "map64_49_eltsum"
  bottom: "map64_50_conv_end"
  top: "map64_50_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_51_bn_pre"
  type: "BatchNorm"
  bottom: "map64_50_eltsum"
  top: "map64_51_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_51_bn_pre"
  type: "BatchNorm"
  bottom: "map64_50_eltsum"
  top: "map64_51_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_51_pre_scale"
  type: "Scale"
  bottom: "map64_51_bn_pre"
  top: "map64_51_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_51_pre_relu"
  type: "ReLU"
  bottom: "map64_51_bn_pre"
  top: "map64_51_bn_pre"
}
layer {
  name: "map64_51_conv1"
  type: "Convolution"
  bottom: "map64_51_bn_pre"
  top: "map64_51_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_51_bn1"
  type: "BatchNorm"
  bottom: "map64_51_conv1"
  top: "map64_51_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_51_bn1"
  type: "BatchNorm"
  bottom: "map64_51_conv1"
  top: "map64_51_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_51_scale1"
  type: "Scale"
  bottom: "map64_51_conv1"
  top: "map64_51_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_51_relu1"
  type: "ReLU"
  bottom: "map64_51_conv1"
  top: "map64_51_conv1"
}
layer {
  name: "map64_51_conv2"
  type: "Convolution"
  bottom: "map64_51_conv1"
  top: "map64_51_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_51_bn2"
  type: "BatchNorm"
  bottom: "map64_51_conv2"
  top: "map64_51_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_51_bn2"
  type: "BatchNorm"
  bottom: "map64_51_conv2"
  top: "map64_51_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_51_scale2"
  type: "Scale"
  bottom: "map64_51_conv2"
  top: "map64_51_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_51_relu2"
  type: "ReLU"
  bottom: "map64_51_conv2"
  top: "map64_51_conv2"
}
layer {
  name: "map64_51_conv_end"
  type: "Convolution"
  bottom: "map64_51_conv2"
  top: "map64_51_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_51_eltsum"
  type: "Eltwise"
  bottom: "map64_50_eltsum"
  bottom: "map64_51_conv_end"
  top: "map64_51_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_52_bn_pre"
  type: "BatchNorm"
  bottom: "map64_51_eltsum"
  top: "map64_52_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_52_bn_pre"
  type: "BatchNorm"
  bottom: "map64_51_eltsum"
  top: "map64_52_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_52_pre_scale"
  type: "Scale"
  bottom: "map64_52_bn_pre"
  top: "map64_52_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_52_pre_relu"
  type: "ReLU"
  bottom: "map64_52_bn_pre"
  top: "map64_52_bn_pre"
}
layer {
  name: "map64_52_conv1"
  type: "Convolution"
  bottom: "map64_52_bn_pre"
  top: "map64_52_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_52_bn1"
  type: "BatchNorm"
  bottom: "map64_52_conv1"
  top: "map64_52_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_52_bn1"
  type: "BatchNorm"
  bottom: "map64_52_conv1"
  top: "map64_52_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_52_scale1"
  type: "Scale"
  bottom: "map64_52_conv1"
  top: "map64_52_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_52_relu1"
  type: "ReLU"
  bottom: "map64_52_conv1"
  top: "map64_52_conv1"
}
layer {
  name: "map64_52_conv2"
  type: "Convolution"
  bottom: "map64_52_conv1"
  top: "map64_52_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_52_bn2"
  type: "BatchNorm"
  bottom: "map64_52_conv2"
  top: "map64_52_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_52_bn2"
  type: "BatchNorm"
  bottom: "map64_52_conv2"
  top: "map64_52_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_52_scale2"
  type: "Scale"
  bottom: "map64_52_conv2"
  top: "map64_52_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_52_relu2"
  type: "ReLU"
  bottom: "map64_52_conv2"
  top: "map64_52_conv2"
}
layer {
  name: "map64_52_conv_end"
  type: "Convolution"
  bottom: "map64_52_conv2"
  top: "map64_52_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_52_eltsum"
  type: "Eltwise"
  bottom: "map64_51_eltsum"
  bottom: "map64_52_conv_end"
  top: "map64_52_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_53_bn_pre"
  type: "BatchNorm"
  bottom: "map64_52_eltsum"
  top: "map64_53_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_53_bn_pre"
  type: "BatchNorm"
  bottom: "map64_52_eltsum"
  top: "map64_53_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_53_pre_scale"
  type: "Scale"
  bottom: "map64_53_bn_pre"
  top: "map64_53_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_53_pre_relu"
  type: "ReLU"
  bottom: "map64_53_bn_pre"
  top: "map64_53_bn_pre"
}
layer {
  name: "map64_53_conv1"
  type: "Convolution"
  bottom: "map64_53_bn_pre"
  top: "map64_53_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_53_bn1"
  type: "BatchNorm"
  bottom: "map64_53_conv1"
  top: "map64_53_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_53_bn1"
  type: "BatchNorm"
  bottom: "map64_53_conv1"
  top: "map64_53_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_53_scale1"
  type: "Scale"
  bottom: "map64_53_conv1"
  top: "map64_53_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_53_relu1"
  type: "ReLU"
  bottom: "map64_53_conv1"
  top: "map64_53_conv1"
}
layer {
  name: "map64_53_conv2"
  type: "Convolution"
  bottom: "map64_53_conv1"
  top: "map64_53_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_53_bn2"
  type: "BatchNorm"
  bottom: "map64_53_conv2"
  top: "map64_53_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_53_bn2"
  type: "BatchNorm"
  bottom: "map64_53_conv2"
  top: "map64_53_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_53_scale2"
  type: "Scale"
  bottom: "map64_53_conv2"
  top: "map64_53_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_53_relu2"
  type: "ReLU"
  bottom: "map64_53_conv2"
  top: "map64_53_conv2"
}
layer {
  name: "map64_53_conv_end"
  type: "Convolution"
  bottom: "map64_53_conv2"
  top: "map64_53_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_53_eltsum"
  type: "Eltwise"
  bottom: "map64_52_eltsum"
  bottom: "map64_53_conv_end"
  top: "map64_53_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_54_bn_pre"
  type: "BatchNorm"
  bottom: "map64_53_eltsum"
  top: "map64_54_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_54_bn_pre"
  type: "BatchNorm"
  bottom: "map64_53_eltsum"
  top: "map64_54_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_54_pre_scale"
  type: "Scale"
  bottom: "map64_54_bn_pre"
  top: "map64_54_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_54_pre_relu"
  type: "ReLU"
  bottom: "map64_54_bn_pre"
  top: "map64_54_bn_pre"
}
layer {
  name: "map64_54_conv1"
  type: "Convolution"
  bottom: "map64_54_bn_pre"
  top: "map64_54_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_54_bn1"
  type: "BatchNorm"
  bottom: "map64_54_conv1"
  top: "map64_54_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_54_bn1"
  type: "BatchNorm"
  bottom: "map64_54_conv1"
  top: "map64_54_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_54_scale1"
  type: "Scale"
  bottom: "map64_54_conv1"
  top: "map64_54_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_54_relu1"
  type: "ReLU"
  bottom: "map64_54_conv1"
  top: "map64_54_conv1"
}
layer {
  name: "map64_54_conv2"
  type: "Convolution"
  bottom: "map64_54_conv1"
  top: "map64_54_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_54_bn2"
  type: "BatchNorm"
  bottom: "map64_54_conv2"
  top: "map64_54_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_54_bn2"
  type: "BatchNorm"
  bottom: "map64_54_conv2"
  top: "map64_54_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_54_scale2"
  type: "Scale"
  bottom: "map64_54_conv2"
  top: "map64_54_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_54_relu2"
  type: "ReLU"
  bottom: "map64_54_conv2"
  top: "map64_54_conv2"
}
layer {
  name: "map64_54_conv_end"
  type: "Convolution"
  bottom: "map64_54_conv2"
  top: "map64_54_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_54_eltsum"
  type: "Eltwise"
  bottom: "map64_53_eltsum"
  bottom: "map64_54_conv_end"
  top: "map64_54_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_55_bn_pre"
  type: "BatchNorm"
  bottom: "map64_54_eltsum"
  top: "map64_55_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_55_bn_pre"
  type: "BatchNorm"
  bottom: "map64_54_eltsum"
  top: "map64_55_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_55_pre_scale"
  type: "Scale"
  bottom: "map64_55_bn_pre"
  top: "map64_55_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_55_pre_relu"
  type: "ReLU"
  bottom: "map64_55_bn_pre"
  top: "map64_55_bn_pre"
}
layer {
  name: "map64_55_conv1"
  type: "Convolution"
  bottom: "map64_55_bn_pre"
  top: "map64_55_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_55_bn1"
  type: "BatchNorm"
  bottom: "map64_55_conv1"
  top: "map64_55_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_55_bn1"
  type: "BatchNorm"
  bottom: "map64_55_conv1"
  top: "map64_55_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_55_scale1"
  type: "Scale"
  bottom: "map64_55_conv1"
  top: "map64_55_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_55_relu1"
  type: "ReLU"
  bottom: "map64_55_conv1"
  top: "map64_55_conv1"
}
layer {
  name: "map64_55_conv2"
  type: "Convolution"
  bottom: "map64_55_conv1"
  top: "map64_55_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_55_bn2"
  type: "BatchNorm"
  bottom: "map64_55_conv2"
  top: "map64_55_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_55_bn2"
  type: "BatchNorm"
  bottom: "map64_55_conv2"
  top: "map64_55_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_55_scale2"
  type: "Scale"
  bottom: "map64_55_conv2"
  top: "map64_55_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_55_relu2"
  type: "ReLU"
  bottom: "map64_55_conv2"
  top: "map64_55_conv2"
}
layer {
  name: "map64_55_conv_end"
  type: "Convolution"
  bottom: "map64_55_conv2"
  top: "map64_55_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_55_eltsum"
  type: "Eltwise"
  bottom: "map64_54_eltsum"
  bottom: "map64_55_conv_end"
  top: "map64_55_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_56_bn_pre"
  type: "BatchNorm"
  bottom: "map64_55_eltsum"
  top: "map64_56_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_56_bn_pre"
  type: "BatchNorm"
  bottom: "map64_55_eltsum"
  top: "map64_56_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_56_pre_scale"
  type: "Scale"
  bottom: "map64_56_bn_pre"
  top: "map64_56_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_56_pre_relu"
  type: "ReLU"
  bottom: "map64_56_bn_pre"
  top: "map64_56_bn_pre"
}
layer {
  name: "map64_56_conv1"
  type: "Convolution"
  bottom: "map64_56_bn_pre"
  top: "map64_56_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_56_bn1"
  type: "BatchNorm"
  bottom: "map64_56_conv1"
  top: "map64_56_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_56_bn1"
  type: "BatchNorm"
  bottom: "map64_56_conv1"
  top: "map64_56_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_56_scale1"
  type: "Scale"
  bottom: "map64_56_conv1"
  top: "map64_56_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_56_relu1"
  type: "ReLU"
  bottom: "map64_56_conv1"
  top: "map64_56_conv1"
}
layer {
  name: "map64_56_conv2"
  type: "Convolution"
  bottom: "map64_56_conv1"
  top: "map64_56_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_56_bn2"
  type: "BatchNorm"
  bottom: "map64_56_conv2"
  top: "map64_56_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_56_bn2"
  type: "BatchNorm"
  bottom: "map64_56_conv2"
  top: "map64_56_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_56_scale2"
  type: "Scale"
  bottom: "map64_56_conv2"
  top: "map64_56_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_56_relu2"
  type: "ReLU"
  bottom: "map64_56_conv2"
  top: "map64_56_conv2"
}
layer {
  name: "map64_56_conv_end"
  type: "Convolution"
  bottom: "map64_56_conv2"
  top: "map64_56_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_56_eltsum"
  type: "Eltwise"
  bottom: "map64_55_eltsum"
  bottom: "map64_56_conv_end"
  top: "map64_56_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_57_bn_pre"
  type: "BatchNorm"
  bottom: "map64_56_eltsum"
  top: "map64_57_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_57_bn_pre"
  type: "BatchNorm"
  bottom: "map64_56_eltsum"
  top: "map64_57_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_57_pre_scale"
  type: "Scale"
  bottom: "map64_57_bn_pre"
  top: "map64_57_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_57_pre_relu"
  type: "ReLU"
  bottom: "map64_57_bn_pre"
  top: "map64_57_bn_pre"
}
layer {
  name: "map64_57_conv1"
  type: "Convolution"
  bottom: "map64_57_bn_pre"
  top: "map64_57_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_57_bn1"
  type: "BatchNorm"
  bottom: "map64_57_conv1"
  top: "map64_57_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_57_bn1"
  type: "BatchNorm"
  bottom: "map64_57_conv1"
  top: "map64_57_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_57_scale1"
  type: "Scale"
  bottom: "map64_57_conv1"
  top: "map64_57_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_57_relu1"
  type: "ReLU"
  bottom: "map64_57_conv1"
  top: "map64_57_conv1"
}
layer {
  name: "map64_57_conv2"
  type: "Convolution"
  bottom: "map64_57_conv1"
  top: "map64_57_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_57_bn2"
  type: "BatchNorm"
  bottom: "map64_57_conv2"
  top: "map64_57_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_57_bn2"
  type: "BatchNorm"
  bottom: "map64_57_conv2"
  top: "map64_57_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_57_scale2"
  type: "Scale"
  bottom: "map64_57_conv2"
  top: "map64_57_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_57_relu2"
  type: "ReLU"
  bottom: "map64_57_conv2"
  top: "map64_57_conv2"
}
layer {
  name: "map64_57_conv_end"
  type: "Convolution"
  bottom: "map64_57_conv2"
  top: "map64_57_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_57_eltsum"
  type: "Eltwise"
  bottom: "map64_56_eltsum"
  bottom: "map64_57_conv_end"
  top: "map64_57_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_58_bn_pre"
  type: "BatchNorm"
  bottom: "map64_57_eltsum"
  top: "map64_58_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_58_bn_pre"
  type: "BatchNorm"
  bottom: "map64_57_eltsum"
  top: "map64_58_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_58_pre_scale"
  type: "Scale"
  bottom: "map64_58_bn_pre"
  top: "map64_58_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_58_pre_relu"
  type: "ReLU"
  bottom: "map64_58_bn_pre"
  top: "map64_58_bn_pre"
}
layer {
  name: "map64_58_conv1"
  type: "Convolution"
  bottom: "map64_58_bn_pre"
  top: "map64_58_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_58_bn1"
  type: "BatchNorm"
  bottom: "map64_58_conv1"
  top: "map64_58_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_58_bn1"
  type: "BatchNorm"
  bottom: "map64_58_conv1"
  top: "map64_58_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_58_scale1"
  type: "Scale"
  bottom: "map64_58_conv1"
  top: "map64_58_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_58_relu1"
  type: "ReLU"
  bottom: "map64_58_conv1"
  top: "map64_58_conv1"
}
layer {
  name: "map64_58_conv2"
  type: "Convolution"
  bottom: "map64_58_conv1"
  top: "map64_58_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_58_bn2"
  type: "BatchNorm"
  bottom: "map64_58_conv2"
  top: "map64_58_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_58_bn2"
  type: "BatchNorm"
  bottom: "map64_58_conv2"
  top: "map64_58_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_58_scale2"
  type: "Scale"
  bottom: "map64_58_conv2"
  top: "map64_58_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_58_relu2"
  type: "ReLU"
  bottom: "map64_58_conv2"
  top: "map64_58_conv2"
}
layer {
  name: "map64_58_conv_end"
  type: "Convolution"
  bottom: "map64_58_conv2"
  top: "map64_58_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_58_eltsum"
  type: "Eltwise"
  bottom: "map64_57_eltsum"
  bottom: "map64_58_conv_end"
  top: "map64_58_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_59_bn_pre"
  type: "BatchNorm"
  bottom: "map64_58_eltsum"
  top: "map64_59_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_59_bn_pre"
  type: "BatchNorm"
  bottom: "map64_58_eltsum"
  top: "map64_59_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_59_pre_scale"
  type: "Scale"
  bottom: "map64_59_bn_pre"
  top: "map64_59_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_59_pre_relu"
  type: "ReLU"
  bottom: "map64_59_bn_pre"
  top: "map64_59_bn_pre"
}
layer {
  name: "map64_59_conv1"
  type: "Convolution"
  bottom: "map64_59_bn_pre"
  top: "map64_59_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_59_bn1"
  type: "BatchNorm"
  bottom: "map64_59_conv1"
  top: "map64_59_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_59_bn1"
  type: "BatchNorm"
  bottom: "map64_59_conv1"
  top: "map64_59_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_59_scale1"
  type: "Scale"
  bottom: "map64_59_conv1"
  top: "map64_59_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_59_relu1"
  type: "ReLU"
  bottom: "map64_59_conv1"
  top: "map64_59_conv1"
}
layer {
  name: "map64_59_conv2"
  type: "Convolution"
  bottom: "map64_59_conv1"
  top: "map64_59_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_59_bn2"
  type: "BatchNorm"
  bottom: "map64_59_conv2"
  top: "map64_59_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_59_bn2"
  type: "BatchNorm"
  bottom: "map64_59_conv2"
  top: "map64_59_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_59_scale2"
  type: "Scale"
  bottom: "map64_59_conv2"
  top: "map64_59_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_59_relu2"
  type: "ReLU"
  bottom: "map64_59_conv2"
  top: "map64_59_conv2"
}
layer {
  name: "map64_59_conv_end"
  type: "Convolution"
  bottom: "map64_59_conv2"
  top: "map64_59_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_59_eltsum"
  type: "Eltwise"
  bottom: "map64_58_eltsum"
  bottom: "map64_59_conv_end"
  top: "map64_59_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_60_bn_pre"
  type: "BatchNorm"
  bottom: "map64_59_eltsum"
  top: "map64_60_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_60_bn_pre"
  type: "BatchNorm"
  bottom: "map64_59_eltsum"
  top: "map64_60_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_60_pre_scale"
  type: "Scale"
  bottom: "map64_60_bn_pre"
  top: "map64_60_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_60_pre_relu"
  type: "ReLU"
  bottom: "map64_60_bn_pre"
  top: "map64_60_bn_pre"
}
layer {
  name: "map64_60_conv1"
  type: "Convolution"
  bottom: "map64_60_bn_pre"
  top: "map64_60_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_60_bn1"
  type: "BatchNorm"
  bottom: "map64_60_conv1"
  top: "map64_60_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_60_bn1"
  type: "BatchNorm"
  bottom: "map64_60_conv1"
  top: "map64_60_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_60_scale1"
  type: "Scale"
  bottom: "map64_60_conv1"
  top: "map64_60_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_60_relu1"
  type: "ReLU"
  bottom: "map64_60_conv1"
  top: "map64_60_conv1"
}
layer {
  name: "map64_60_conv2"
  type: "Convolution"
  bottom: "map64_60_conv1"
  top: "map64_60_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_60_bn2"
  type: "BatchNorm"
  bottom: "map64_60_conv2"
  top: "map64_60_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_60_bn2"
  type: "BatchNorm"
  bottom: "map64_60_conv2"
  top: "map64_60_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_60_scale2"
  type: "Scale"
  bottom: "map64_60_conv2"
  top: "map64_60_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_60_relu2"
  type: "ReLU"
  bottom: "map64_60_conv2"
  top: "map64_60_conv2"
}
layer {
  name: "map64_60_conv_end"
  type: "Convolution"
  bottom: "map64_60_conv2"
  top: "map64_60_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_60_eltsum"
  type: "Eltwise"
  bottom: "map64_59_eltsum"
  bottom: "map64_60_conv_end"
  top: "map64_60_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_61_bn_pre"
  type: "BatchNorm"
  bottom: "map64_60_eltsum"
  top: "map64_61_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_61_bn_pre"
  type: "BatchNorm"
  bottom: "map64_60_eltsum"
  top: "map64_61_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_61_pre_scale"
  type: "Scale"
  bottom: "map64_61_bn_pre"
  top: "map64_61_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_61_pre_relu"
  type: "ReLU"
  bottom: "map64_61_bn_pre"
  top: "map64_61_bn_pre"
}
layer {
  name: "map64_61_conv1"
  type: "Convolution"
  bottom: "map64_61_bn_pre"
  top: "map64_61_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_61_bn1"
  type: "BatchNorm"
  bottom: "map64_61_conv1"
  top: "map64_61_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_61_bn1"
  type: "BatchNorm"
  bottom: "map64_61_conv1"
  top: "map64_61_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_61_scale1"
  type: "Scale"
  bottom: "map64_61_conv1"
  top: "map64_61_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_61_relu1"
  type: "ReLU"
  bottom: "map64_61_conv1"
  top: "map64_61_conv1"
}
layer {
  name: "map64_61_conv2"
  type: "Convolution"
  bottom: "map64_61_conv1"
  top: "map64_61_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_61_bn2"
  type: "BatchNorm"
  bottom: "map64_61_conv2"
  top: "map64_61_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_61_bn2"
  type: "BatchNorm"
  bottom: "map64_61_conv2"
  top: "map64_61_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_61_scale2"
  type: "Scale"
  bottom: "map64_61_conv2"
  top: "map64_61_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_61_relu2"
  type: "ReLU"
  bottom: "map64_61_conv2"
  top: "map64_61_conv2"
}
layer {
  name: "map64_61_conv_end"
  type: "Convolution"
  bottom: "map64_61_conv2"
  top: "map64_61_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_61_eltsum"
  type: "Eltwise"
  bottom: "map64_60_eltsum"
  bottom: "map64_61_conv_end"
  top: "map64_61_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_62_bn_pre"
  type: "BatchNorm"
  bottom: "map64_61_eltsum"
  top: "map64_62_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_62_bn_pre"
  type: "BatchNorm"
  bottom: "map64_61_eltsum"
  top: "map64_62_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_62_pre_scale"
  type: "Scale"
  bottom: "map64_62_bn_pre"
  top: "map64_62_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_62_pre_relu"
  type: "ReLU"
  bottom: "map64_62_bn_pre"
  top: "map64_62_bn_pre"
}
layer {
  name: "map64_62_conv1"
  type: "Convolution"
  bottom: "map64_62_bn_pre"
  top: "map64_62_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_62_bn1"
  type: "BatchNorm"
  bottom: "map64_62_conv1"
  top: "map64_62_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_62_bn1"
  type: "BatchNorm"
  bottom: "map64_62_conv1"
  top: "map64_62_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_62_scale1"
  type: "Scale"
  bottom: "map64_62_conv1"
  top: "map64_62_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_62_relu1"
  type: "ReLU"
  bottom: "map64_62_conv1"
  top: "map64_62_conv1"
}
layer {
  name: "map64_62_conv2"
  type: "Convolution"
  bottom: "map64_62_conv1"
  top: "map64_62_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_62_bn2"
  type: "BatchNorm"
  bottom: "map64_62_conv2"
  top: "map64_62_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_62_bn2"
  type: "BatchNorm"
  bottom: "map64_62_conv2"
  top: "map64_62_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_62_scale2"
  type: "Scale"
  bottom: "map64_62_conv2"
  top: "map64_62_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_62_relu2"
  type: "ReLU"
  bottom: "map64_62_conv2"
  top: "map64_62_conv2"
}
layer {
  name: "map64_62_conv_end"
  type: "Convolution"
  bottom: "map64_62_conv2"
  top: "map64_62_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_62_eltsum"
  type: "Eltwise"
  bottom: "map64_61_eltsum"
  bottom: "map64_62_conv_end"
  top: "map64_62_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_63_bn_pre"
  type: "BatchNorm"
  bottom: "map64_62_eltsum"
  top: "map64_63_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_63_bn_pre"
  type: "BatchNorm"
  bottom: "map64_62_eltsum"
  top: "map64_63_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_63_pre_scale"
  type: "Scale"
  bottom: "map64_63_bn_pre"
  top: "map64_63_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_63_pre_relu"
  type: "ReLU"
  bottom: "map64_63_bn_pre"
  top: "map64_63_bn_pre"
}
layer {
  name: "map64_63_conv1"
  type: "Convolution"
  bottom: "map64_63_bn_pre"
  top: "map64_63_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_63_bn1"
  type: "BatchNorm"
  bottom: "map64_63_conv1"
  top: "map64_63_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_63_bn1"
  type: "BatchNorm"
  bottom: "map64_63_conv1"
  top: "map64_63_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_63_scale1"
  type: "Scale"
  bottom: "map64_63_conv1"
  top: "map64_63_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_63_relu1"
  type: "ReLU"
  bottom: "map64_63_conv1"
  top: "map64_63_conv1"
}
layer {
  name: "map64_63_conv2"
  type: "Convolution"
  bottom: "map64_63_conv1"
  top: "map64_63_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_63_bn2"
  type: "BatchNorm"
  bottom: "map64_63_conv2"
  top: "map64_63_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_63_bn2"
  type: "BatchNorm"
  bottom: "map64_63_conv2"
  top: "map64_63_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_63_scale2"
  type: "Scale"
  bottom: "map64_63_conv2"
  top: "map64_63_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_63_relu2"
  type: "ReLU"
  bottom: "map64_63_conv2"
  top: "map64_63_conv2"
}
layer {
  name: "map64_63_conv_end"
  type: "Convolution"
  bottom: "map64_63_conv2"
  top: "map64_63_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_63_eltsum"
  type: "Eltwise"
  bottom: "map64_62_eltsum"
  bottom: "map64_63_conv_end"
  top: "map64_63_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_64_bn_pre"
  type: "BatchNorm"
  bottom: "map64_63_eltsum"
  top: "map64_64_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_64_bn_pre"
  type: "BatchNorm"
  bottom: "map64_63_eltsum"
  top: "map64_64_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_64_pre_scale"
  type: "Scale"
  bottom: "map64_64_bn_pre"
  top: "map64_64_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_64_pre_relu"
  type: "ReLU"
  bottom: "map64_64_bn_pre"
  top: "map64_64_bn_pre"
}
layer {
  name: "map64_64_conv1"
  type: "Convolution"
  bottom: "map64_64_bn_pre"
  top: "map64_64_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_64_bn1"
  type: "BatchNorm"
  bottom: "map64_64_conv1"
  top: "map64_64_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_64_bn1"
  type: "BatchNorm"
  bottom: "map64_64_conv1"
  top: "map64_64_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_64_scale1"
  type: "Scale"
  bottom: "map64_64_conv1"
  top: "map64_64_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_64_relu1"
  type: "ReLU"
  bottom: "map64_64_conv1"
  top: "map64_64_conv1"
}
layer {
  name: "map64_64_conv2"
  type: "Convolution"
  bottom: "map64_64_conv1"
  top: "map64_64_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_64_bn2"
  type: "BatchNorm"
  bottom: "map64_64_conv2"
  top: "map64_64_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_64_bn2"
  type: "BatchNorm"
  bottom: "map64_64_conv2"
  top: "map64_64_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_64_scale2"
  type: "Scale"
  bottom: "map64_64_conv2"
  top: "map64_64_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_64_relu2"
  type: "ReLU"
  bottom: "map64_64_conv2"
  top: "map64_64_conv2"
}
layer {
  name: "map64_64_conv_end"
  type: "Convolution"
  bottom: "map64_64_conv2"
  top: "map64_64_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_64_eltsum"
  type: "Eltwise"
  bottom: "map64_63_eltsum"
  bottom: "map64_64_conv_end"
  top: "map64_64_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_65_bn_pre"
  type: "BatchNorm"
  bottom: "map64_64_eltsum"
  top: "map64_65_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_65_bn_pre"
  type: "BatchNorm"
  bottom: "map64_64_eltsum"
  top: "map64_65_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_65_pre_scale"
  type: "Scale"
  bottom: "map64_65_bn_pre"
  top: "map64_65_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_65_pre_relu"
  type: "ReLU"
  bottom: "map64_65_bn_pre"
  top: "map64_65_bn_pre"
}
layer {
  name: "map64_65_conv1"
  type: "Convolution"
  bottom: "map64_65_bn_pre"
  top: "map64_65_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_65_bn1"
  type: "BatchNorm"
  bottom: "map64_65_conv1"
  top: "map64_65_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_65_bn1"
  type: "BatchNorm"
  bottom: "map64_65_conv1"
  top: "map64_65_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_65_scale1"
  type: "Scale"
  bottom: "map64_65_conv1"
  top: "map64_65_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_65_relu1"
  type: "ReLU"
  bottom: "map64_65_conv1"
  top: "map64_65_conv1"
}
layer {
  name: "map64_65_conv2"
  type: "Convolution"
  bottom: "map64_65_conv1"
  top: "map64_65_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_65_bn2"
  type: "BatchNorm"
  bottom: "map64_65_conv2"
  top: "map64_65_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_65_bn2"
  type: "BatchNorm"
  bottom: "map64_65_conv2"
  top: "map64_65_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_65_scale2"
  type: "Scale"
  bottom: "map64_65_conv2"
  top: "map64_65_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_65_relu2"
  type: "ReLU"
  bottom: "map64_65_conv2"
  top: "map64_65_conv2"
}
layer {
  name: "map64_65_conv_end"
  type: "Convolution"
  bottom: "map64_65_conv2"
  top: "map64_65_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_65_eltsum"
  type: "Eltwise"
  bottom: "map64_64_eltsum"
  bottom: "map64_65_conv_end"
  top: "map64_65_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_66_bn_pre"
  type: "BatchNorm"
  bottom: "map64_65_eltsum"
  top: "map64_66_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_66_bn_pre"
  type: "BatchNorm"
  bottom: "map64_65_eltsum"
  top: "map64_66_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_66_pre_scale"
  type: "Scale"
  bottom: "map64_66_bn_pre"
  top: "map64_66_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_66_pre_relu"
  type: "ReLU"
  bottom: "map64_66_bn_pre"
  top: "map64_66_bn_pre"
}
layer {
  name: "map64_66_conv1"
  type: "Convolution"
  bottom: "map64_66_bn_pre"
  top: "map64_66_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_66_bn1"
  type: "BatchNorm"
  bottom: "map64_66_conv1"
  top: "map64_66_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_66_bn1"
  type: "BatchNorm"
  bottom: "map64_66_conv1"
  top: "map64_66_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_66_scale1"
  type: "Scale"
  bottom: "map64_66_conv1"
  top: "map64_66_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_66_relu1"
  type: "ReLU"
  bottom: "map64_66_conv1"
  top: "map64_66_conv1"
}
layer {
  name: "map64_66_conv2"
  type: "Convolution"
  bottom: "map64_66_conv1"
  top: "map64_66_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_66_bn2"
  type: "BatchNorm"
  bottom: "map64_66_conv2"
  top: "map64_66_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_66_bn2"
  type: "BatchNorm"
  bottom: "map64_66_conv2"
  top: "map64_66_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_66_scale2"
  type: "Scale"
  bottom: "map64_66_conv2"
  top: "map64_66_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_66_relu2"
  type: "ReLU"
  bottom: "map64_66_conv2"
  top: "map64_66_conv2"
}
layer {
  name: "map64_66_conv_end"
  type: "Convolution"
  bottom: "map64_66_conv2"
  top: "map64_66_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_66_eltsum"
  type: "Eltwise"
  bottom: "map64_65_eltsum"
  bottom: "map64_66_conv_end"
  top: "map64_66_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_67_bn_pre"
  type: "BatchNorm"
  bottom: "map64_66_eltsum"
  top: "map64_67_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_67_bn_pre"
  type: "BatchNorm"
  bottom: "map64_66_eltsum"
  top: "map64_67_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_67_pre_scale"
  type: "Scale"
  bottom: "map64_67_bn_pre"
  top: "map64_67_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_67_pre_relu"
  type: "ReLU"
  bottom: "map64_67_bn_pre"
  top: "map64_67_bn_pre"
}
layer {
  name: "map64_67_conv1"
  type: "Convolution"
  bottom: "map64_67_bn_pre"
  top: "map64_67_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_67_bn1"
  type: "BatchNorm"
  bottom: "map64_67_conv1"
  top: "map64_67_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_67_bn1"
  type: "BatchNorm"
  bottom: "map64_67_conv1"
  top: "map64_67_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_67_scale1"
  type: "Scale"
  bottom: "map64_67_conv1"
  top: "map64_67_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_67_relu1"
  type: "ReLU"
  bottom: "map64_67_conv1"
  top: "map64_67_conv1"
}
layer {
  name: "map64_67_conv2"
  type: "Convolution"
  bottom: "map64_67_conv1"
  top: "map64_67_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_67_bn2"
  type: "BatchNorm"
  bottom: "map64_67_conv2"
  top: "map64_67_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_67_bn2"
  type: "BatchNorm"
  bottom: "map64_67_conv2"
  top: "map64_67_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_67_scale2"
  type: "Scale"
  bottom: "map64_67_conv2"
  top: "map64_67_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_67_relu2"
  type: "ReLU"
  bottom: "map64_67_conv2"
  top: "map64_67_conv2"
}
layer {
  name: "map64_67_conv_end"
  type: "Convolution"
  bottom: "map64_67_conv2"
  top: "map64_67_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_67_eltsum"
  type: "Eltwise"
  bottom: "map64_66_eltsum"
  bottom: "map64_67_conv_end"
  top: "map64_67_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_68_bn_pre"
  type: "BatchNorm"
  bottom: "map64_67_eltsum"
  top: "map64_68_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_68_bn_pre"
  type: "BatchNorm"
  bottom: "map64_67_eltsum"
  top: "map64_68_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_68_pre_scale"
  type: "Scale"
  bottom: "map64_68_bn_pre"
  top: "map64_68_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_68_pre_relu"
  type: "ReLU"
  bottom: "map64_68_bn_pre"
  top: "map64_68_bn_pre"
}
layer {
  name: "map64_68_conv1"
  type: "Convolution"
  bottom: "map64_68_bn_pre"
  top: "map64_68_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_68_bn1"
  type: "BatchNorm"
  bottom: "map64_68_conv1"
  top: "map64_68_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_68_bn1"
  type: "BatchNorm"
  bottom: "map64_68_conv1"
  top: "map64_68_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_68_scale1"
  type: "Scale"
  bottom: "map64_68_conv1"
  top: "map64_68_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_68_relu1"
  type: "ReLU"
  bottom: "map64_68_conv1"
  top: "map64_68_conv1"
}
layer {
  name: "map64_68_conv2"
  type: "Convolution"
  bottom: "map64_68_conv1"
  top: "map64_68_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_68_bn2"
  type: "BatchNorm"
  bottom: "map64_68_conv2"
  top: "map64_68_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_68_bn2"
  type: "BatchNorm"
  bottom: "map64_68_conv2"
  top: "map64_68_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_68_scale2"
  type: "Scale"
  bottom: "map64_68_conv2"
  top: "map64_68_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_68_relu2"
  type: "ReLU"
  bottom: "map64_68_conv2"
  top: "map64_68_conv2"
}
layer {
  name: "map64_68_conv_end"
  type: "Convolution"
  bottom: "map64_68_conv2"
  top: "map64_68_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_68_eltsum"
  type: "Eltwise"
  bottom: "map64_67_eltsum"
  bottom: "map64_68_conv_end"
  top: "map64_68_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_69_bn_pre"
  type: "BatchNorm"
  bottom: "map64_68_eltsum"
  top: "map64_69_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_69_bn_pre"
  type: "BatchNorm"
  bottom: "map64_68_eltsum"
  top: "map64_69_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_69_pre_scale"
  type: "Scale"
  bottom: "map64_69_bn_pre"
  top: "map64_69_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_69_pre_relu"
  type: "ReLU"
  bottom: "map64_69_bn_pre"
  top: "map64_69_bn_pre"
}
layer {
  name: "map64_69_conv1"
  type: "Convolution"
  bottom: "map64_69_bn_pre"
  top: "map64_69_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_69_bn1"
  type: "BatchNorm"
  bottom: "map64_69_conv1"
  top: "map64_69_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_69_bn1"
  type: "BatchNorm"
  bottom: "map64_69_conv1"
  top: "map64_69_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_69_scale1"
  type: "Scale"
  bottom: "map64_69_conv1"
  top: "map64_69_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_69_relu1"
  type: "ReLU"
  bottom: "map64_69_conv1"
  top: "map64_69_conv1"
}
layer {
  name: "map64_69_conv2"
  type: "Convolution"
  bottom: "map64_69_conv1"
  top: "map64_69_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_69_bn2"
  type: "BatchNorm"
  bottom: "map64_69_conv2"
  top: "map64_69_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_69_bn2"
  type: "BatchNorm"
  bottom: "map64_69_conv2"
  top: "map64_69_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_69_scale2"
  type: "Scale"
  bottom: "map64_69_conv2"
  top: "map64_69_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_69_relu2"
  type: "ReLU"
  bottom: "map64_69_conv2"
  top: "map64_69_conv2"
}
layer {
  name: "map64_69_conv_end"
  type: "Convolution"
  bottom: "map64_69_conv2"
  top: "map64_69_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_69_eltsum"
  type: "Eltwise"
  bottom: "map64_68_eltsum"
  bottom: "map64_69_conv_end"
  top: "map64_69_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_70_bn_pre"
  type: "BatchNorm"
  bottom: "map64_69_eltsum"
  top: "map64_70_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_70_bn_pre"
  type: "BatchNorm"
  bottom: "map64_69_eltsum"
  top: "map64_70_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_70_pre_scale"
  type: "Scale"
  bottom: "map64_70_bn_pre"
  top: "map64_70_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_70_pre_relu"
  type: "ReLU"
  bottom: "map64_70_bn_pre"
  top: "map64_70_bn_pre"
}
layer {
  name: "map64_70_conv1"
  type: "Convolution"
  bottom: "map64_70_bn_pre"
  top: "map64_70_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_70_bn1"
  type: "BatchNorm"
  bottom: "map64_70_conv1"
  top: "map64_70_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_70_bn1"
  type: "BatchNorm"
  bottom: "map64_70_conv1"
  top: "map64_70_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_70_scale1"
  type: "Scale"
  bottom: "map64_70_conv1"
  top: "map64_70_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_70_relu1"
  type: "ReLU"
  bottom: "map64_70_conv1"
  top: "map64_70_conv1"
}
layer {
  name: "map64_70_conv2"
  type: "Convolution"
  bottom: "map64_70_conv1"
  top: "map64_70_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_70_bn2"
  type: "BatchNorm"
  bottom: "map64_70_conv2"
  top: "map64_70_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_70_bn2"
  type: "BatchNorm"
  bottom: "map64_70_conv2"
  top: "map64_70_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_70_scale2"
  type: "Scale"
  bottom: "map64_70_conv2"
  top: "map64_70_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_70_relu2"
  type: "ReLU"
  bottom: "map64_70_conv2"
  top: "map64_70_conv2"
}
layer {
  name: "map64_70_conv_end"
  type: "Convolution"
  bottom: "map64_70_conv2"
  top: "map64_70_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_70_eltsum"
  type: "Eltwise"
  bottom: "map64_69_eltsum"
  bottom: "map64_70_conv_end"
  top: "map64_70_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_71_bn_pre"
  type: "BatchNorm"
  bottom: "map64_70_eltsum"
  top: "map64_71_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_71_bn_pre"
  type: "BatchNorm"
  bottom: "map64_70_eltsum"
  top: "map64_71_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_71_pre_scale"
  type: "Scale"
  bottom: "map64_71_bn_pre"
  top: "map64_71_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_71_pre_relu"
  type: "ReLU"
  bottom: "map64_71_bn_pre"
  top: "map64_71_bn_pre"
}
layer {
  name: "map64_71_conv1"
  type: "Convolution"
  bottom: "map64_71_bn_pre"
  top: "map64_71_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_71_bn1"
  type: "BatchNorm"
  bottom: "map64_71_conv1"
  top: "map64_71_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_71_bn1"
  type: "BatchNorm"
  bottom: "map64_71_conv1"
  top: "map64_71_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_71_scale1"
  type: "Scale"
  bottom: "map64_71_conv1"
  top: "map64_71_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_71_relu1"
  type: "ReLU"
  bottom: "map64_71_conv1"
  top: "map64_71_conv1"
}
layer {
  name: "map64_71_conv2"
  type: "Convolution"
  bottom: "map64_71_conv1"
  top: "map64_71_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_71_bn2"
  type: "BatchNorm"
  bottom: "map64_71_conv2"
  top: "map64_71_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_71_bn2"
  type: "BatchNorm"
  bottom: "map64_71_conv2"
  top: "map64_71_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_71_scale2"
  type: "Scale"
  bottom: "map64_71_conv2"
  top: "map64_71_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_71_relu2"
  type: "ReLU"
  bottom: "map64_71_conv2"
  top: "map64_71_conv2"
}
layer {
  name: "map64_71_conv_end"
  type: "Convolution"
  bottom: "map64_71_conv2"
  top: "map64_71_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_71_eltsum"
  type: "Eltwise"
  bottom: "map64_70_eltsum"
  bottom: "map64_71_conv_end"
  top: "map64_71_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_72_bn_pre"
  type: "BatchNorm"
  bottom: "map64_71_eltsum"
  top: "map64_72_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_72_bn_pre"
  type: "BatchNorm"
  bottom: "map64_71_eltsum"
  top: "map64_72_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_72_pre_scale"
  type: "Scale"
  bottom: "map64_72_bn_pre"
  top: "map64_72_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_72_pre_relu"
  type: "ReLU"
  bottom: "map64_72_bn_pre"
  top: "map64_72_bn_pre"
}
layer {
  name: "map64_72_conv1"
  type: "Convolution"
  bottom: "map64_72_bn_pre"
  top: "map64_72_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_72_bn1"
  type: "BatchNorm"
  bottom: "map64_72_conv1"
  top: "map64_72_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_72_bn1"
  type: "BatchNorm"
  bottom: "map64_72_conv1"
  top: "map64_72_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_72_scale1"
  type: "Scale"
  bottom: "map64_72_conv1"
  top: "map64_72_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_72_relu1"
  type: "ReLU"
  bottom: "map64_72_conv1"
  top: "map64_72_conv1"
}
layer {
  name: "map64_72_conv2"
  type: "Convolution"
  bottom: "map64_72_conv1"
  top: "map64_72_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_72_bn2"
  type: "BatchNorm"
  bottom: "map64_72_conv2"
  top: "map64_72_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_72_bn2"
  type: "BatchNorm"
  bottom: "map64_72_conv2"
  top: "map64_72_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_72_scale2"
  type: "Scale"
  bottom: "map64_72_conv2"
  top: "map64_72_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_72_relu2"
  type: "ReLU"
  bottom: "map64_72_conv2"
  top: "map64_72_conv2"
}
layer {
  name: "map64_72_conv_end"
  type: "Convolution"
  bottom: "map64_72_conv2"
  top: "map64_72_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_72_eltsum"
  type: "Eltwise"
  bottom: "map64_71_eltsum"
  bottom: "map64_72_conv_end"
  top: "map64_72_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_73_bn_pre"
  type: "BatchNorm"
  bottom: "map64_72_eltsum"
  top: "map64_73_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_73_bn_pre"
  type: "BatchNorm"
  bottom: "map64_72_eltsum"
  top: "map64_73_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_73_pre_scale"
  type: "Scale"
  bottom: "map64_73_bn_pre"
  top: "map64_73_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_73_pre_relu"
  type: "ReLU"
  bottom: "map64_73_bn_pre"
  top: "map64_73_bn_pre"
}
layer {
  name: "map64_73_conv1"
  type: "Convolution"
  bottom: "map64_73_bn_pre"
  top: "map64_73_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_73_bn1"
  type: "BatchNorm"
  bottom: "map64_73_conv1"
  top: "map64_73_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_73_bn1"
  type: "BatchNorm"
  bottom: "map64_73_conv1"
  top: "map64_73_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_73_scale1"
  type: "Scale"
  bottom: "map64_73_conv1"
  top: "map64_73_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_73_relu1"
  type: "ReLU"
  bottom: "map64_73_conv1"
  top: "map64_73_conv1"
}
layer {
  name: "map64_73_conv2"
  type: "Convolution"
  bottom: "map64_73_conv1"
  top: "map64_73_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_73_bn2"
  type: "BatchNorm"
  bottom: "map64_73_conv2"
  top: "map64_73_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_73_bn2"
  type: "BatchNorm"
  bottom: "map64_73_conv2"
  top: "map64_73_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_73_scale2"
  type: "Scale"
  bottom: "map64_73_conv2"
  top: "map64_73_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_73_relu2"
  type: "ReLU"
  bottom: "map64_73_conv2"
  top: "map64_73_conv2"
}
layer {
  name: "map64_73_conv_end"
  type: "Convolution"
  bottom: "map64_73_conv2"
  top: "map64_73_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_73_eltsum"
  type: "Eltwise"
  bottom: "map64_72_eltsum"
  bottom: "map64_73_conv_end"
  top: "map64_73_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_74_bn_pre"
  type: "BatchNorm"
  bottom: "map64_73_eltsum"
  top: "map64_74_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_74_bn_pre"
  type: "BatchNorm"
  bottom: "map64_73_eltsum"
  top: "map64_74_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_74_pre_scale"
  type: "Scale"
  bottom: "map64_74_bn_pre"
  top: "map64_74_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_74_pre_relu"
  type: "ReLU"
  bottom: "map64_74_bn_pre"
  top: "map64_74_bn_pre"
}
layer {
  name: "map64_74_conv1"
  type: "Convolution"
  bottom: "map64_74_bn_pre"
  top: "map64_74_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_74_bn1"
  type: "BatchNorm"
  bottom: "map64_74_conv1"
  top: "map64_74_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_74_bn1"
  type: "BatchNorm"
  bottom: "map64_74_conv1"
  top: "map64_74_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_74_scale1"
  type: "Scale"
  bottom: "map64_74_conv1"
  top: "map64_74_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_74_relu1"
  type: "ReLU"
  bottom: "map64_74_conv1"
  top: "map64_74_conv1"
}
layer {
  name: "map64_74_conv2"
  type: "Convolution"
  bottom: "map64_74_conv1"
  top: "map64_74_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_74_bn2"
  type: "BatchNorm"
  bottom: "map64_74_conv2"
  top: "map64_74_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_74_bn2"
  type: "BatchNorm"
  bottom: "map64_74_conv2"
  top: "map64_74_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_74_scale2"
  type: "Scale"
  bottom: "map64_74_conv2"
  top: "map64_74_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_74_relu2"
  type: "ReLU"
  bottom: "map64_74_conv2"
  top: "map64_74_conv2"
}
layer {
  name: "map64_74_conv_end"
  type: "Convolution"
  bottom: "map64_74_conv2"
  top: "map64_74_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_74_eltsum"
  type: "Eltwise"
  bottom: "map64_73_eltsum"
  bottom: "map64_74_conv_end"
  top: "map64_74_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_75_bn_pre"
  type: "BatchNorm"
  bottom: "map64_74_eltsum"
  top: "map64_75_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_75_bn_pre"
  type: "BatchNorm"
  bottom: "map64_74_eltsum"
  top: "map64_75_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_75_pre_scale"
  type: "Scale"
  bottom: "map64_75_bn_pre"
  top: "map64_75_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_75_pre_relu"
  type: "ReLU"
  bottom: "map64_75_bn_pre"
  top: "map64_75_bn_pre"
}
layer {
  name: "map64_75_conv1"
  type: "Convolution"
  bottom: "map64_75_bn_pre"
  top: "map64_75_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_75_bn1"
  type: "BatchNorm"
  bottom: "map64_75_conv1"
  top: "map64_75_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_75_bn1"
  type: "BatchNorm"
  bottom: "map64_75_conv1"
  top: "map64_75_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_75_scale1"
  type: "Scale"
  bottom: "map64_75_conv1"
  top: "map64_75_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_75_relu1"
  type: "ReLU"
  bottom: "map64_75_conv1"
  top: "map64_75_conv1"
}
layer {
  name: "map64_75_conv2"
  type: "Convolution"
  bottom: "map64_75_conv1"
  top: "map64_75_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_75_bn2"
  type: "BatchNorm"
  bottom: "map64_75_conv2"
  top: "map64_75_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_75_bn2"
  type: "BatchNorm"
  bottom: "map64_75_conv2"
  top: "map64_75_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_75_scale2"
  type: "Scale"
  bottom: "map64_75_conv2"
  top: "map64_75_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_75_relu2"
  type: "ReLU"
  bottom: "map64_75_conv2"
  top: "map64_75_conv2"
}
layer {
  name: "map64_75_conv_end"
  type: "Convolution"
  bottom: "map64_75_conv2"
  top: "map64_75_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_75_eltsum"
  type: "Eltwise"
  bottom: "map64_74_eltsum"
  bottom: "map64_75_conv_end"
  top: "map64_75_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_76_bn_pre"
  type: "BatchNorm"
  bottom: "map64_75_eltsum"
  top: "map64_76_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_76_bn_pre"
  type: "BatchNorm"
  bottom: "map64_75_eltsum"
  top: "map64_76_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_76_pre_scale"
  type: "Scale"
  bottom: "map64_76_bn_pre"
  top: "map64_76_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_76_pre_relu"
  type: "ReLU"
  bottom: "map64_76_bn_pre"
  top: "map64_76_bn_pre"
}
layer {
  name: "map64_76_conv1"
  type: "Convolution"
  bottom: "map64_76_bn_pre"
  top: "map64_76_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_76_bn1"
  type: "BatchNorm"
  bottom: "map64_76_conv1"
  top: "map64_76_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_76_bn1"
  type: "BatchNorm"
  bottom: "map64_76_conv1"
  top: "map64_76_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_76_scale1"
  type: "Scale"
  bottom: "map64_76_conv1"
  top: "map64_76_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_76_relu1"
  type: "ReLU"
  bottom: "map64_76_conv1"
  top: "map64_76_conv1"
}
layer {
  name: "map64_76_conv2"
  type: "Convolution"
  bottom: "map64_76_conv1"
  top: "map64_76_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_76_bn2"
  type: "BatchNorm"
  bottom: "map64_76_conv2"
  top: "map64_76_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_76_bn2"
  type: "BatchNorm"
  bottom: "map64_76_conv2"
  top: "map64_76_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_76_scale2"
  type: "Scale"
  bottom: "map64_76_conv2"
  top: "map64_76_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_76_relu2"
  type: "ReLU"
  bottom: "map64_76_conv2"
  top: "map64_76_conv2"
}
layer {
  name: "map64_76_conv_end"
  type: "Convolution"
  bottom: "map64_76_conv2"
  top: "map64_76_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_76_eltsum"
  type: "Eltwise"
  bottom: "map64_75_eltsum"
  bottom: "map64_76_conv_end"
  top: "map64_76_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_77_bn_pre"
  type: "BatchNorm"
  bottom: "map64_76_eltsum"
  top: "map64_77_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_77_bn_pre"
  type: "BatchNorm"
  bottom: "map64_76_eltsum"
  top: "map64_77_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_77_pre_scale"
  type: "Scale"
  bottom: "map64_77_bn_pre"
  top: "map64_77_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_77_pre_relu"
  type: "ReLU"
  bottom: "map64_77_bn_pre"
  top: "map64_77_bn_pre"
}
layer {
  name: "map64_77_conv1"
  type: "Convolution"
  bottom: "map64_77_bn_pre"
  top: "map64_77_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_77_bn1"
  type: "BatchNorm"
  bottom: "map64_77_conv1"
  top: "map64_77_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_77_bn1"
  type: "BatchNorm"
  bottom: "map64_77_conv1"
  top: "map64_77_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_77_scale1"
  type: "Scale"
  bottom: "map64_77_conv1"
  top: "map64_77_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_77_relu1"
  type: "ReLU"
  bottom: "map64_77_conv1"
  top: "map64_77_conv1"
}
layer {
  name: "map64_77_conv2"
  type: "Convolution"
  bottom: "map64_77_conv1"
  top: "map64_77_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_77_bn2"
  type: "BatchNorm"
  bottom: "map64_77_conv2"
  top: "map64_77_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_77_bn2"
  type: "BatchNorm"
  bottom: "map64_77_conv2"
  top: "map64_77_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_77_scale2"
  type: "Scale"
  bottom: "map64_77_conv2"
  top: "map64_77_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_77_relu2"
  type: "ReLU"
  bottom: "map64_77_conv2"
  top: "map64_77_conv2"
}
layer {
  name: "map64_77_conv_end"
  type: "Convolution"
  bottom: "map64_77_conv2"
  top: "map64_77_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_77_eltsum"
  type: "Eltwise"
  bottom: "map64_76_eltsum"
  bottom: "map64_77_conv_end"
  top: "map64_77_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_78_bn_pre"
  type: "BatchNorm"
  bottom: "map64_77_eltsum"
  top: "map64_78_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_78_bn_pre"
  type: "BatchNorm"
  bottom: "map64_77_eltsum"
  top: "map64_78_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_78_pre_scale"
  type: "Scale"
  bottom: "map64_78_bn_pre"
  top: "map64_78_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_78_pre_relu"
  type: "ReLU"
  bottom: "map64_78_bn_pre"
  top: "map64_78_bn_pre"
}
layer {
  name: "map64_78_conv1"
  type: "Convolution"
  bottom: "map64_78_bn_pre"
  top: "map64_78_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_78_bn1"
  type: "BatchNorm"
  bottom: "map64_78_conv1"
  top: "map64_78_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_78_bn1"
  type: "BatchNorm"
  bottom: "map64_78_conv1"
  top: "map64_78_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_78_scale1"
  type: "Scale"
  bottom: "map64_78_conv1"
  top: "map64_78_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_78_relu1"
  type: "ReLU"
  bottom: "map64_78_conv1"
  top: "map64_78_conv1"
}
layer {
  name: "map64_78_conv2"
  type: "Convolution"
  bottom: "map64_78_conv1"
  top: "map64_78_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_78_bn2"
  type: "BatchNorm"
  bottom: "map64_78_conv2"
  top: "map64_78_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_78_bn2"
  type: "BatchNorm"
  bottom: "map64_78_conv2"
  top: "map64_78_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_78_scale2"
  type: "Scale"
  bottom: "map64_78_conv2"
  top: "map64_78_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_78_relu2"
  type: "ReLU"
  bottom: "map64_78_conv2"
  top: "map64_78_conv2"
}
layer {
  name: "map64_78_conv_end"
  type: "Convolution"
  bottom: "map64_78_conv2"
  top: "map64_78_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_78_eltsum"
  type: "Eltwise"
  bottom: "map64_77_eltsum"
  bottom: "map64_78_conv_end"
  top: "map64_78_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_79_bn_pre"
  type: "BatchNorm"
  bottom: "map64_78_eltsum"
  top: "map64_79_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_79_bn_pre"
  type: "BatchNorm"
  bottom: "map64_78_eltsum"
  top: "map64_79_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_79_pre_scale"
  type: "Scale"
  bottom: "map64_79_bn_pre"
  top: "map64_79_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_79_pre_relu"
  type: "ReLU"
  bottom: "map64_79_bn_pre"
  top: "map64_79_bn_pre"
}
layer {
  name: "map64_79_conv1"
  type: "Convolution"
  bottom: "map64_79_bn_pre"
  top: "map64_79_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_79_bn1"
  type: "BatchNorm"
  bottom: "map64_79_conv1"
  top: "map64_79_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_79_bn1"
  type: "BatchNorm"
  bottom: "map64_79_conv1"
  top: "map64_79_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_79_scale1"
  type: "Scale"
  bottom: "map64_79_conv1"
  top: "map64_79_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_79_relu1"
  type: "ReLU"
  bottom: "map64_79_conv1"
  top: "map64_79_conv1"
}
layer {
  name: "map64_79_conv2"
  type: "Convolution"
  bottom: "map64_79_conv1"
  top: "map64_79_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_79_bn2"
  type: "BatchNorm"
  bottom: "map64_79_conv2"
  top: "map64_79_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_79_bn2"
  type: "BatchNorm"
  bottom: "map64_79_conv2"
  top: "map64_79_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_79_scale2"
  type: "Scale"
  bottom: "map64_79_conv2"
  top: "map64_79_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_79_relu2"
  type: "ReLU"
  bottom: "map64_79_conv2"
  top: "map64_79_conv2"
}
layer {
  name: "map64_79_conv_end"
  type: "Convolution"
  bottom: "map64_79_conv2"
  top: "map64_79_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_79_eltsum"
  type: "Eltwise"
  bottom: "map64_78_eltsum"
  bottom: "map64_79_conv_end"
  top: "map64_79_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_80_bn_pre"
  type: "BatchNorm"
  bottom: "map64_79_eltsum"
  top: "map64_80_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_80_bn_pre"
  type: "BatchNorm"
  bottom: "map64_79_eltsum"
  top: "map64_80_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_80_pre_scale"
  type: "Scale"
  bottom: "map64_80_bn_pre"
  top: "map64_80_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_80_pre_relu"
  type: "ReLU"
  bottom: "map64_80_bn_pre"
  top: "map64_80_bn_pre"
}
layer {
  name: "map64_80_conv1"
  type: "Convolution"
  bottom: "map64_80_bn_pre"
  top: "map64_80_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_80_bn1"
  type: "BatchNorm"
  bottom: "map64_80_conv1"
  top: "map64_80_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_80_bn1"
  type: "BatchNorm"
  bottom: "map64_80_conv1"
  top: "map64_80_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_80_scale1"
  type: "Scale"
  bottom: "map64_80_conv1"
  top: "map64_80_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_80_relu1"
  type: "ReLU"
  bottom: "map64_80_conv1"
  top: "map64_80_conv1"
}
layer {
  name: "map64_80_conv2"
  type: "Convolution"
  bottom: "map64_80_conv1"
  top: "map64_80_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_80_bn2"
  type: "BatchNorm"
  bottom: "map64_80_conv2"
  top: "map64_80_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_80_bn2"
  type: "BatchNorm"
  bottom: "map64_80_conv2"
  top: "map64_80_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_80_scale2"
  type: "Scale"
  bottom: "map64_80_conv2"
  top: "map64_80_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_80_relu2"
  type: "ReLU"
  bottom: "map64_80_conv2"
  top: "map64_80_conv2"
}
layer {
  name: "map64_80_conv_end"
  type: "Convolution"
  bottom: "map64_80_conv2"
  top: "map64_80_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_80_eltsum"
  type: "Eltwise"
  bottom: "map64_79_eltsum"
  bottom: "map64_80_conv_end"
  top: "map64_80_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_81_bn_pre"
  type: "BatchNorm"
  bottom: "map64_80_eltsum"
  top: "map64_81_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_81_bn_pre"
  type: "BatchNorm"
  bottom: "map64_80_eltsum"
  top: "map64_81_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_81_pre_scale"
  type: "Scale"
  bottom: "map64_81_bn_pre"
  top: "map64_81_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_81_pre_relu"
  type: "ReLU"
  bottom: "map64_81_bn_pre"
  top: "map64_81_bn_pre"
}
layer {
  name: "map64_81_conv1"
  type: "Convolution"
  bottom: "map64_81_bn_pre"
  top: "map64_81_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_81_bn1"
  type: "BatchNorm"
  bottom: "map64_81_conv1"
  top: "map64_81_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_81_bn1"
  type: "BatchNorm"
  bottom: "map64_81_conv1"
  top: "map64_81_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_81_scale1"
  type: "Scale"
  bottom: "map64_81_conv1"
  top: "map64_81_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_81_relu1"
  type: "ReLU"
  bottom: "map64_81_conv1"
  top: "map64_81_conv1"
}
layer {
  name: "map64_81_conv2"
  type: "Convolution"
  bottom: "map64_81_conv1"
  top: "map64_81_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_81_bn2"
  type: "BatchNorm"
  bottom: "map64_81_conv2"
  top: "map64_81_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_81_bn2"
  type: "BatchNorm"
  bottom: "map64_81_conv2"
  top: "map64_81_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_81_scale2"
  type: "Scale"
  bottom: "map64_81_conv2"
  top: "map64_81_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_81_relu2"
  type: "ReLU"
  bottom: "map64_81_conv2"
  top: "map64_81_conv2"
}
layer {
  name: "map64_81_conv_end"
  type: "Convolution"
  bottom: "map64_81_conv2"
  top: "map64_81_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_81_eltsum"
  type: "Eltwise"
  bottom: "map64_80_eltsum"
  bottom: "map64_81_conv_end"
  top: "map64_81_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_82_bn_pre"
  type: "BatchNorm"
  bottom: "map64_81_eltsum"
  top: "map64_82_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_82_bn_pre"
  type: "BatchNorm"
  bottom: "map64_81_eltsum"
  top: "map64_82_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_82_pre_scale"
  type: "Scale"
  bottom: "map64_82_bn_pre"
  top: "map64_82_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_82_pre_relu"
  type: "ReLU"
  bottom: "map64_82_bn_pre"
  top: "map64_82_bn_pre"
}
layer {
  name: "map64_82_conv1"
  type: "Convolution"
  bottom: "map64_82_bn_pre"
  top: "map64_82_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_82_bn1"
  type: "BatchNorm"
  bottom: "map64_82_conv1"
  top: "map64_82_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_82_bn1"
  type: "BatchNorm"
  bottom: "map64_82_conv1"
  top: "map64_82_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_82_scale1"
  type: "Scale"
  bottom: "map64_82_conv1"
  top: "map64_82_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_82_relu1"
  type: "ReLU"
  bottom: "map64_82_conv1"
  top: "map64_82_conv1"
}
layer {
  name: "map64_82_conv2"
  type: "Convolution"
  bottom: "map64_82_conv1"
  top: "map64_82_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_82_bn2"
  type: "BatchNorm"
  bottom: "map64_82_conv2"
  top: "map64_82_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_82_bn2"
  type: "BatchNorm"
  bottom: "map64_82_conv2"
  top: "map64_82_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_82_scale2"
  type: "Scale"
  bottom: "map64_82_conv2"
  top: "map64_82_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_82_relu2"
  type: "ReLU"
  bottom: "map64_82_conv2"
  top: "map64_82_conv2"
}
layer {
  name: "map64_82_conv_end"
  type: "Convolution"
  bottom: "map64_82_conv2"
  top: "map64_82_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_82_eltsum"
  type: "Eltwise"
  bottom: "map64_81_eltsum"
  bottom: "map64_82_conv_end"
  top: "map64_82_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_83_bn_pre"
  type: "BatchNorm"
  bottom: "map64_82_eltsum"
  top: "map64_83_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_83_bn_pre"
  type: "BatchNorm"
  bottom: "map64_82_eltsum"
  top: "map64_83_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_83_pre_scale"
  type: "Scale"
  bottom: "map64_83_bn_pre"
  top: "map64_83_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_83_pre_relu"
  type: "ReLU"
  bottom: "map64_83_bn_pre"
  top: "map64_83_bn_pre"
}
layer {
  name: "map64_83_conv1"
  type: "Convolution"
  bottom: "map64_83_bn_pre"
  top: "map64_83_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_83_bn1"
  type: "BatchNorm"
  bottom: "map64_83_conv1"
  top: "map64_83_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_83_bn1"
  type: "BatchNorm"
  bottom: "map64_83_conv1"
  top: "map64_83_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_83_scale1"
  type: "Scale"
  bottom: "map64_83_conv1"
  top: "map64_83_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_83_relu1"
  type: "ReLU"
  bottom: "map64_83_conv1"
  top: "map64_83_conv1"
}
layer {
  name: "map64_83_conv2"
  type: "Convolution"
  bottom: "map64_83_conv1"
  top: "map64_83_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_83_bn2"
  type: "BatchNorm"
  bottom: "map64_83_conv2"
  top: "map64_83_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_83_bn2"
  type: "BatchNorm"
  bottom: "map64_83_conv2"
  top: "map64_83_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_83_scale2"
  type: "Scale"
  bottom: "map64_83_conv2"
  top: "map64_83_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_83_relu2"
  type: "ReLU"
  bottom: "map64_83_conv2"
  top: "map64_83_conv2"
}
layer {
  name: "map64_83_conv_end"
  type: "Convolution"
  bottom: "map64_83_conv2"
  top: "map64_83_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_83_eltsum"
  type: "Eltwise"
  bottom: "map64_82_eltsum"
  bottom: "map64_83_conv_end"
  top: "map64_83_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_84_bn_pre"
  type: "BatchNorm"
  bottom: "map64_83_eltsum"
  top: "map64_84_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_84_bn_pre"
  type: "BatchNorm"
  bottom: "map64_83_eltsum"
  top: "map64_84_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_84_pre_scale"
  type: "Scale"
  bottom: "map64_84_bn_pre"
  top: "map64_84_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_84_pre_relu"
  type: "ReLU"
  bottom: "map64_84_bn_pre"
  top: "map64_84_bn_pre"
}
layer {
  name: "map64_84_conv1"
  type: "Convolution"
  bottom: "map64_84_bn_pre"
  top: "map64_84_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_84_bn1"
  type: "BatchNorm"
  bottom: "map64_84_conv1"
  top: "map64_84_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_84_bn1"
  type: "BatchNorm"
  bottom: "map64_84_conv1"
  top: "map64_84_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_84_scale1"
  type: "Scale"
  bottom: "map64_84_conv1"
  top: "map64_84_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_84_relu1"
  type: "ReLU"
  bottom: "map64_84_conv1"
  top: "map64_84_conv1"
}
layer {
  name: "map64_84_conv2"
  type: "Convolution"
  bottom: "map64_84_conv1"
  top: "map64_84_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_84_bn2"
  type: "BatchNorm"
  bottom: "map64_84_conv2"
  top: "map64_84_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_84_bn2"
  type: "BatchNorm"
  bottom: "map64_84_conv2"
  top: "map64_84_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_84_scale2"
  type: "Scale"
  bottom: "map64_84_conv2"
  top: "map64_84_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_84_relu2"
  type: "ReLU"
  bottom: "map64_84_conv2"
  top: "map64_84_conv2"
}
layer {
  name: "map64_84_conv_end"
  type: "Convolution"
  bottom: "map64_84_conv2"
  top: "map64_84_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_84_eltsum"
  type: "Eltwise"
  bottom: "map64_83_eltsum"
  bottom: "map64_84_conv_end"
  top: "map64_84_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_85_bn_pre"
  type: "BatchNorm"
  bottom: "map64_84_eltsum"
  top: "map64_85_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_85_bn_pre"
  type: "BatchNorm"
  bottom: "map64_84_eltsum"
  top: "map64_85_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_85_pre_scale"
  type: "Scale"
  bottom: "map64_85_bn_pre"
  top: "map64_85_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_85_pre_relu"
  type: "ReLU"
  bottom: "map64_85_bn_pre"
  top: "map64_85_bn_pre"
}
layer {
  name: "map64_85_conv1"
  type: "Convolution"
  bottom: "map64_85_bn_pre"
  top: "map64_85_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_85_bn1"
  type: "BatchNorm"
  bottom: "map64_85_conv1"
  top: "map64_85_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_85_bn1"
  type: "BatchNorm"
  bottom: "map64_85_conv1"
  top: "map64_85_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_85_scale1"
  type: "Scale"
  bottom: "map64_85_conv1"
  top: "map64_85_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_85_relu1"
  type: "ReLU"
  bottom: "map64_85_conv1"
  top: "map64_85_conv1"
}
layer {
  name: "map64_85_conv2"
  type: "Convolution"
  bottom: "map64_85_conv1"
  top: "map64_85_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_85_bn2"
  type: "BatchNorm"
  bottom: "map64_85_conv2"
  top: "map64_85_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_85_bn2"
  type: "BatchNorm"
  bottom: "map64_85_conv2"
  top: "map64_85_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_85_scale2"
  type: "Scale"
  bottom: "map64_85_conv2"
  top: "map64_85_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_85_relu2"
  type: "ReLU"
  bottom: "map64_85_conv2"
  top: "map64_85_conv2"
}
layer {
  name: "map64_85_conv_end"
  type: "Convolution"
  bottom: "map64_85_conv2"
  top: "map64_85_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_85_eltsum"
  type: "Eltwise"
  bottom: "map64_84_eltsum"
  bottom: "map64_85_conv_end"
  top: "map64_85_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_86_bn_pre"
  type: "BatchNorm"
  bottom: "map64_85_eltsum"
  top: "map64_86_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_86_bn_pre"
  type: "BatchNorm"
  bottom: "map64_85_eltsum"
  top: "map64_86_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_86_pre_scale"
  type: "Scale"
  bottom: "map64_86_bn_pre"
  top: "map64_86_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_86_pre_relu"
  type: "ReLU"
  bottom: "map64_86_bn_pre"
  top: "map64_86_bn_pre"
}
layer {
  name: "map64_86_conv1"
  type: "Convolution"
  bottom: "map64_86_bn_pre"
  top: "map64_86_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_86_bn1"
  type: "BatchNorm"
  bottom: "map64_86_conv1"
  top: "map64_86_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_86_bn1"
  type: "BatchNorm"
  bottom: "map64_86_conv1"
  top: "map64_86_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_86_scale1"
  type: "Scale"
  bottom: "map64_86_conv1"
  top: "map64_86_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_86_relu1"
  type: "ReLU"
  bottom: "map64_86_conv1"
  top: "map64_86_conv1"
}
layer {
  name: "map64_86_conv2"
  type: "Convolution"
  bottom: "map64_86_conv1"
  top: "map64_86_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_86_bn2"
  type: "BatchNorm"
  bottom: "map64_86_conv2"
  top: "map64_86_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_86_bn2"
  type: "BatchNorm"
  bottom: "map64_86_conv2"
  top: "map64_86_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_86_scale2"
  type: "Scale"
  bottom: "map64_86_conv2"
  top: "map64_86_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_86_relu2"
  type: "ReLU"
  bottom: "map64_86_conv2"
  top: "map64_86_conv2"
}
layer {
  name: "map64_86_conv_end"
  type: "Convolution"
  bottom: "map64_86_conv2"
  top: "map64_86_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_86_eltsum"
  type: "Eltwise"
  bottom: "map64_85_eltsum"
  bottom: "map64_86_conv_end"
  top: "map64_86_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_87_bn_pre"
  type: "BatchNorm"
  bottom: "map64_86_eltsum"
  top: "map64_87_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_87_bn_pre"
  type: "BatchNorm"
  bottom: "map64_86_eltsum"
  top: "map64_87_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_87_pre_scale"
  type: "Scale"
  bottom: "map64_87_bn_pre"
  top: "map64_87_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_87_pre_relu"
  type: "ReLU"
  bottom: "map64_87_bn_pre"
  top: "map64_87_bn_pre"
}
layer {
  name: "map64_87_conv1"
  type: "Convolution"
  bottom: "map64_87_bn_pre"
  top: "map64_87_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_87_bn1"
  type: "BatchNorm"
  bottom: "map64_87_conv1"
  top: "map64_87_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_87_bn1"
  type: "BatchNorm"
  bottom: "map64_87_conv1"
  top: "map64_87_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_87_scale1"
  type: "Scale"
  bottom: "map64_87_conv1"
  top: "map64_87_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_87_relu1"
  type: "ReLU"
  bottom: "map64_87_conv1"
  top: "map64_87_conv1"
}
layer {
  name: "map64_87_conv2"
  type: "Convolution"
  bottom: "map64_87_conv1"
  top: "map64_87_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_87_bn2"
  type: "BatchNorm"
  bottom: "map64_87_conv2"
  top: "map64_87_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_87_bn2"
  type: "BatchNorm"
  bottom: "map64_87_conv2"
  top: "map64_87_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_87_scale2"
  type: "Scale"
  bottom: "map64_87_conv2"
  top: "map64_87_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_87_relu2"
  type: "ReLU"
  bottom: "map64_87_conv2"
  top: "map64_87_conv2"
}
layer {
  name: "map64_87_conv_end"
  type: "Convolution"
  bottom: "map64_87_conv2"
  top: "map64_87_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_87_eltsum"
  type: "Eltwise"
  bottom: "map64_86_eltsum"
  bottom: "map64_87_conv_end"
  top: "map64_87_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_88_bn_pre"
  type: "BatchNorm"
  bottom: "map64_87_eltsum"
  top: "map64_88_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_88_bn_pre"
  type: "BatchNorm"
  bottom: "map64_87_eltsum"
  top: "map64_88_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_88_pre_scale"
  type: "Scale"
  bottom: "map64_88_bn_pre"
  top: "map64_88_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_88_pre_relu"
  type: "ReLU"
  bottom: "map64_88_bn_pre"
  top: "map64_88_bn_pre"
}
layer {
  name: "map64_88_conv1"
  type: "Convolution"
  bottom: "map64_88_bn_pre"
  top: "map64_88_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_88_bn1"
  type: "BatchNorm"
  bottom: "map64_88_conv1"
  top: "map64_88_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_88_bn1"
  type: "BatchNorm"
  bottom: "map64_88_conv1"
  top: "map64_88_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_88_scale1"
  type: "Scale"
  bottom: "map64_88_conv1"
  top: "map64_88_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_88_relu1"
  type: "ReLU"
  bottom: "map64_88_conv1"
  top: "map64_88_conv1"
}
layer {
  name: "map64_88_conv2"
  type: "Convolution"
  bottom: "map64_88_conv1"
  top: "map64_88_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_88_bn2"
  type: "BatchNorm"
  bottom: "map64_88_conv2"
  top: "map64_88_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_88_bn2"
  type: "BatchNorm"
  bottom: "map64_88_conv2"
  top: "map64_88_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_88_scale2"
  type: "Scale"
  bottom: "map64_88_conv2"
  top: "map64_88_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_88_relu2"
  type: "ReLU"
  bottom: "map64_88_conv2"
  top: "map64_88_conv2"
}
layer {
  name: "map64_88_conv_end"
  type: "Convolution"
  bottom: "map64_88_conv2"
  top: "map64_88_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_88_eltsum"
  type: "Eltwise"
  bottom: "map64_87_eltsum"
  bottom: "map64_88_conv_end"
  top: "map64_88_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_89_bn_pre"
  type: "BatchNorm"
  bottom: "map64_88_eltsum"
  top: "map64_89_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_89_bn_pre"
  type: "BatchNorm"
  bottom: "map64_88_eltsum"
  top: "map64_89_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_89_pre_scale"
  type: "Scale"
  bottom: "map64_89_bn_pre"
  top: "map64_89_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_89_pre_relu"
  type: "ReLU"
  bottom: "map64_89_bn_pre"
  top: "map64_89_bn_pre"
}
layer {
  name: "map64_89_conv1"
  type: "Convolution"
  bottom: "map64_89_bn_pre"
  top: "map64_89_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_89_bn1"
  type: "BatchNorm"
  bottom: "map64_89_conv1"
  top: "map64_89_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_89_bn1"
  type: "BatchNorm"
  bottom: "map64_89_conv1"
  top: "map64_89_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_89_scale1"
  type: "Scale"
  bottom: "map64_89_conv1"
  top: "map64_89_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_89_relu1"
  type: "ReLU"
  bottom: "map64_89_conv1"
  top: "map64_89_conv1"
}
layer {
  name: "map64_89_conv2"
  type: "Convolution"
  bottom: "map64_89_conv1"
  top: "map64_89_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_89_bn2"
  type: "BatchNorm"
  bottom: "map64_89_conv2"
  top: "map64_89_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_89_bn2"
  type: "BatchNorm"
  bottom: "map64_89_conv2"
  top: "map64_89_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_89_scale2"
  type: "Scale"
  bottom: "map64_89_conv2"
  top: "map64_89_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_89_relu2"
  type: "ReLU"
  bottom: "map64_89_conv2"
  top: "map64_89_conv2"
}
layer {
  name: "map64_89_conv_end"
  type: "Convolution"
  bottom: "map64_89_conv2"
  top: "map64_89_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_89_eltsum"
  type: "Eltwise"
  bottom: "map64_88_eltsum"
  bottom: "map64_89_conv_end"
  top: "map64_89_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_90_bn_pre"
  type: "BatchNorm"
  bottom: "map64_89_eltsum"
  top: "map64_90_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_90_bn_pre"
  type: "BatchNorm"
  bottom: "map64_89_eltsum"
  top: "map64_90_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_90_pre_scale"
  type: "Scale"
  bottom: "map64_90_bn_pre"
  top: "map64_90_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_90_pre_relu"
  type: "ReLU"
  bottom: "map64_90_bn_pre"
  top: "map64_90_bn_pre"
}
layer {
  name: "map64_90_conv1"
  type: "Convolution"
  bottom: "map64_90_bn_pre"
  top: "map64_90_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_90_bn1"
  type: "BatchNorm"
  bottom: "map64_90_conv1"
  top: "map64_90_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_90_bn1"
  type: "BatchNorm"
  bottom: "map64_90_conv1"
  top: "map64_90_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_90_scale1"
  type: "Scale"
  bottom: "map64_90_conv1"
  top: "map64_90_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_90_relu1"
  type: "ReLU"
  bottom: "map64_90_conv1"
  top: "map64_90_conv1"
}
layer {
  name: "map64_90_conv2"
  type: "Convolution"
  bottom: "map64_90_conv1"
  top: "map64_90_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_90_bn2"
  type: "BatchNorm"
  bottom: "map64_90_conv2"
  top: "map64_90_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_90_bn2"
  type: "BatchNorm"
  bottom: "map64_90_conv2"
  top: "map64_90_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_90_scale2"
  type: "Scale"
  bottom: "map64_90_conv2"
  top: "map64_90_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_90_relu2"
  type: "ReLU"
  bottom: "map64_90_conv2"
  top: "map64_90_conv2"
}
layer {
  name: "map64_90_conv_end"
  type: "Convolution"
  bottom: "map64_90_conv2"
  top: "map64_90_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_90_eltsum"
  type: "Eltwise"
  bottom: "map64_89_eltsum"
  bottom: "map64_90_conv_end"
  top: "map64_90_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_91_bn_pre"
  type: "BatchNorm"
  bottom: "map64_90_eltsum"
  top: "map64_91_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_91_bn_pre"
  type: "BatchNorm"
  bottom: "map64_90_eltsum"
  top: "map64_91_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_91_pre_scale"
  type: "Scale"
  bottom: "map64_91_bn_pre"
  top: "map64_91_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_91_pre_relu"
  type: "ReLU"
  bottom: "map64_91_bn_pre"
  top: "map64_91_bn_pre"
}
layer {
  name: "map64_91_conv1"
  type: "Convolution"
  bottom: "map64_91_bn_pre"
  top: "map64_91_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_91_bn1"
  type: "BatchNorm"
  bottom: "map64_91_conv1"
  top: "map64_91_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_91_bn1"
  type: "BatchNorm"
  bottom: "map64_91_conv1"
  top: "map64_91_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_91_scale1"
  type: "Scale"
  bottom: "map64_91_conv1"
  top: "map64_91_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_91_relu1"
  type: "ReLU"
  bottom: "map64_91_conv1"
  top: "map64_91_conv1"
}
layer {
  name: "map64_91_conv2"
  type: "Convolution"
  bottom: "map64_91_conv1"
  top: "map64_91_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_91_bn2"
  type: "BatchNorm"
  bottom: "map64_91_conv2"
  top: "map64_91_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_91_bn2"
  type: "BatchNorm"
  bottom: "map64_91_conv2"
  top: "map64_91_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_91_scale2"
  type: "Scale"
  bottom: "map64_91_conv2"
  top: "map64_91_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_91_relu2"
  type: "ReLU"
  bottom: "map64_91_conv2"
  top: "map64_91_conv2"
}
layer {
  name: "map64_91_conv_end"
  type: "Convolution"
  bottom: "map64_91_conv2"
  top: "map64_91_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_91_eltsum"
  type: "Eltwise"
  bottom: "map64_90_eltsum"
  bottom: "map64_91_conv_end"
  top: "map64_91_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_92_bn_pre"
  type: "BatchNorm"
  bottom: "map64_91_eltsum"
  top: "map64_92_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_92_bn_pre"
  type: "BatchNorm"
  bottom: "map64_91_eltsum"
  top: "map64_92_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_92_pre_scale"
  type: "Scale"
  bottom: "map64_92_bn_pre"
  top: "map64_92_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_92_pre_relu"
  type: "ReLU"
  bottom: "map64_92_bn_pre"
  top: "map64_92_bn_pre"
}
layer {
  name: "map64_92_conv1"
  type: "Convolution"
  bottom: "map64_92_bn_pre"
  top: "map64_92_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_92_bn1"
  type: "BatchNorm"
  bottom: "map64_92_conv1"
  top: "map64_92_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_92_bn1"
  type: "BatchNorm"
  bottom: "map64_92_conv1"
  top: "map64_92_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_92_scale1"
  type: "Scale"
  bottom: "map64_92_conv1"
  top: "map64_92_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_92_relu1"
  type: "ReLU"
  bottom: "map64_92_conv1"
  top: "map64_92_conv1"
}
layer {
  name: "map64_92_conv2"
  type: "Convolution"
  bottom: "map64_92_conv1"
  top: "map64_92_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_92_bn2"
  type: "BatchNorm"
  bottom: "map64_92_conv2"
  top: "map64_92_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_92_bn2"
  type: "BatchNorm"
  bottom: "map64_92_conv2"
  top: "map64_92_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_92_scale2"
  type: "Scale"
  bottom: "map64_92_conv2"
  top: "map64_92_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_92_relu2"
  type: "ReLU"
  bottom: "map64_92_conv2"
  top: "map64_92_conv2"
}
layer {
  name: "map64_92_conv_end"
  type: "Convolution"
  bottom: "map64_92_conv2"
  top: "map64_92_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_92_eltsum"
  type: "Eltwise"
  bottom: "map64_91_eltsum"
  bottom: "map64_92_conv_end"
  top: "map64_92_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_93_bn_pre"
  type: "BatchNorm"
  bottom: "map64_92_eltsum"
  top: "map64_93_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_93_bn_pre"
  type: "BatchNorm"
  bottom: "map64_92_eltsum"
  top: "map64_93_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_93_pre_scale"
  type: "Scale"
  bottom: "map64_93_bn_pre"
  top: "map64_93_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_93_pre_relu"
  type: "ReLU"
  bottom: "map64_93_bn_pre"
  top: "map64_93_bn_pre"
}
layer {
  name: "map64_93_conv1"
  type: "Convolution"
  bottom: "map64_93_bn_pre"
  top: "map64_93_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_93_bn1"
  type: "BatchNorm"
  bottom: "map64_93_conv1"
  top: "map64_93_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_93_bn1"
  type: "BatchNorm"
  bottom: "map64_93_conv1"
  top: "map64_93_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_93_scale1"
  type: "Scale"
  bottom: "map64_93_conv1"
  top: "map64_93_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_93_relu1"
  type: "ReLU"
  bottom: "map64_93_conv1"
  top: "map64_93_conv1"
}
layer {
  name: "map64_93_conv2"
  type: "Convolution"
  bottom: "map64_93_conv1"
  top: "map64_93_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_93_bn2"
  type: "BatchNorm"
  bottom: "map64_93_conv2"
  top: "map64_93_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_93_bn2"
  type: "BatchNorm"
  bottom: "map64_93_conv2"
  top: "map64_93_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_93_scale2"
  type: "Scale"
  bottom: "map64_93_conv2"
  top: "map64_93_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_93_relu2"
  type: "ReLU"
  bottom: "map64_93_conv2"
  top: "map64_93_conv2"
}
layer {
  name: "map64_93_conv_end"
  type: "Convolution"
  bottom: "map64_93_conv2"
  top: "map64_93_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_93_eltsum"
  type: "Eltwise"
  bottom: "map64_92_eltsum"
  bottom: "map64_93_conv_end"
  top: "map64_93_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_94_bn_pre"
  type: "BatchNorm"
  bottom: "map64_93_eltsum"
  top: "map64_94_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_94_bn_pre"
  type: "BatchNorm"
  bottom: "map64_93_eltsum"
  top: "map64_94_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_94_pre_scale"
  type: "Scale"
  bottom: "map64_94_bn_pre"
  top: "map64_94_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_94_pre_relu"
  type: "ReLU"
  bottom: "map64_94_bn_pre"
  top: "map64_94_bn_pre"
}
layer {
  name: "map64_94_conv1"
  type: "Convolution"
  bottom: "map64_94_bn_pre"
  top: "map64_94_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_94_bn1"
  type: "BatchNorm"
  bottom: "map64_94_conv1"
  top: "map64_94_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_94_bn1"
  type: "BatchNorm"
  bottom: "map64_94_conv1"
  top: "map64_94_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_94_scale1"
  type: "Scale"
  bottom: "map64_94_conv1"
  top: "map64_94_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_94_relu1"
  type: "ReLU"
  bottom: "map64_94_conv1"
  top: "map64_94_conv1"
}
layer {
  name: "map64_94_conv2"
  type: "Convolution"
  bottom: "map64_94_conv1"
  top: "map64_94_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_94_bn2"
  type: "BatchNorm"
  bottom: "map64_94_conv2"
  top: "map64_94_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_94_bn2"
  type: "BatchNorm"
  bottom: "map64_94_conv2"
  top: "map64_94_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_94_scale2"
  type: "Scale"
  bottom: "map64_94_conv2"
  top: "map64_94_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_94_relu2"
  type: "ReLU"
  bottom: "map64_94_conv2"
  top: "map64_94_conv2"
}
layer {
  name: "map64_94_conv_end"
  type: "Convolution"
  bottom: "map64_94_conv2"
  top: "map64_94_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_94_eltsum"
  type: "Eltwise"
  bottom: "map64_93_eltsum"
  bottom: "map64_94_conv_end"
  top: "map64_94_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_95_bn_pre"
  type: "BatchNorm"
  bottom: "map64_94_eltsum"
  top: "map64_95_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_95_bn_pre"
  type: "BatchNorm"
  bottom: "map64_94_eltsum"
  top: "map64_95_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_95_pre_scale"
  type: "Scale"
  bottom: "map64_95_bn_pre"
  top: "map64_95_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_95_pre_relu"
  type: "ReLU"
  bottom: "map64_95_bn_pre"
  top: "map64_95_bn_pre"
}
layer {
  name: "map64_95_conv1"
  type: "Convolution"
  bottom: "map64_95_bn_pre"
  top: "map64_95_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_95_bn1"
  type: "BatchNorm"
  bottom: "map64_95_conv1"
  top: "map64_95_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_95_bn1"
  type: "BatchNorm"
  bottom: "map64_95_conv1"
  top: "map64_95_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_95_scale1"
  type: "Scale"
  bottom: "map64_95_conv1"
  top: "map64_95_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_95_relu1"
  type: "ReLU"
  bottom: "map64_95_conv1"
  top: "map64_95_conv1"
}
layer {
  name: "map64_95_conv2"
  type: "Convolution"
  bottom: "map64_95_conv1"
  top: "map64_95_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_95_bn2"
  type: "BatchNorm"
  bottom: "map64_95_conv2"
  top: "map64_95_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_95_bn2"
  type: "BatchNorm"
  bottom: "map64_95_conv2"
  top: "map64_95_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_95_scale2"
  type: "Scale"
  bottom: "map64_95_conv2"
  top: "map64_95_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_95_relu2"
  type: "ReLU"
  bottom: "map64_95_conv2"
  top: "map64_95_conv2"
}
layer {
  name: "map64_95_conv_end"
  type: "Convolution"
  bottom: "map64_95_conv2"
  top: "map64_95_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_95_eltsum"
  type: "Eltwise"
  bottom: "map64_94_eltsum"
  bottom: "map64_95_conv_end"
  top: "map64_95_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_96_bn_pre"
  type: "BatchNorm"
  bottom: "map64_95_eltsum"
  top: "map64_96_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_96_bn_pre"
  type: "BatchNorm"
  bottom: "map64_95_eltsum"
  top: "map64_96_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_96_pre_scale"
  type: "Scale"
  bottom: "map64_96_bn_pre"
  top: "map64_96_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_96_pre_relu"
  type: "ReLU"
  bottom: "map64_96_bn_pre"
  top: "map64_96_bn_pre"
}
layer {
  name: "map64_96_conv1"
  type: "Convolution"
  bottom: "map64_96_bn_pre"
  top: "map64_96_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_96_bn1"
  type: "BatchNorm"
  bottom: "map64_96_conv1"
  top: "map64_96_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_96_bn1"
  type: "BatchNorm"
  bottom: "map64_96_conv1"
  top: "map64_96_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_96_scale1"
  type: "Scale"
  bottom: "map64_96_conv1"
  top: "map64_96_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_96_relu1"
  type: "ReLU"
  bottom: "map64_96_conv1"
  top: "map64_96_conv1"
}
layer {
  name: "map64_96_conv2"
  type: "Convolution"
  bottom: "map64_96_conv1"
  top: "map64_96_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_96_bn2"
  type: "BatchNorm"
  bottom: "map64_96_conv2"
  top: "map64_96_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_96_bn2"
  type: "BatchNorm"
  bottom: "map64_96_conv2"
  top: "map64_96_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_96_scale2"
  type: "Scale"
  bottom: "map64_96_conv2"
  top: "map64_96_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_96_relu2"
  type: "ReLU"
  bottom: "map64_96_conv2"
  top: "map64_96_conv2"
}
layer {
  name: "map64_96_conv_end"
  type: "Convolution"
  bottom: "map64_96_conv2"
  top: "map64_96_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_96_eltsum"
  type: "Eltwise"
  bottom: "map64_95_eltsum"
  bottom: "map64_96_conv_end"
  top: "map64_96_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_97_bn_pre"
  type: "BatchNorm"
  bottom: "map64_96_eltsum"
  top: "map64_97_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_97_bn_pre"
  type: "BatchNorm"
  bottom: "map64_96_eltsum"
  top: "map64_97_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_97_pre_scale"
  type: "Scale"
  bottom: "map64_97_bn_pre"
  top: "map64_97_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_97_pre_relu"
  type: "ReLU"
  bottom: "map64_97_bn_pre"
  top: "map64_97_bn_pre"
}
layer {
  name: "map64_97_conv1"
  type: "Convolution"
  bottom: "map64_97_bn_pre"
  top: "map64_97_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_97_bn1"
  type: "BatchNorm"
  bottom: "map64_97_conv1"
  top: "map64_97_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_97_bn1"
  type: "BatchNorm"
  bottom: "map64_97_conv1"
  top: "map64_97_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_97_scale1"
  type: "Scale"
  bottom: "map64_97_conv1"
  top: "map64_97_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_97_relu1"
  type: "ReLU"
  bottom: "map64_97_conv1"
  top: "map64_97_conv1"
}
layer {
  name: "map64_97_conv2"
  type: "Convolution"
  bottom: "map64_97_conv1"
  top: "map64_97_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_97_bn2"
  type: "BatchNorm"
  bottom: "map64_97_conv2"
  top: "map64_97_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_97_bn2"
  type: "BatchNorm"
  bottom: "map64_97_conv2"
  top: "map64_97_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_97_scale2"
  type: "Scale"
  bottom: "map64_97_conv2"
  top: "map64_97_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_97_relu2"
  type: "ReLU"
  bottom: "map64_97_conv2"
  top: "map64_97_conv2"
}
layer {
  name: "map64_97_conv_end"
  type: "Convolution"
  bottom: "map64_97_conv2"
  top: "map64_97_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_97_eltsum"
  type: "Eltwise"
  bottom: "map64_96_eltsum"
  bottom: "map64_97_conv_end"
  top: "map64_97_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_98_bn_pre"
  type: "BatchNorm"
  bottom: "map64_97_eltsum"
  top: "map64_98_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_98_bn_pre"
  type: "BatchNorm"
  bottom: "map64_97_eltsum"
  top: "map64_98_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_98_pre_scale"
  type: "Scale"
  bottom: "map64_98_bn_pre"
  top: "map64_98_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_98_pre_relu"
  type: "ReLU"
  bottom: "map64_98_bn_pre"
  top: "map64_98_bn_pre"
}
layer {
  name: "map64_98_conv1"
  type: "Convolution"
  bottom: "map64_98_bn_pre"
  top: "map64_98_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_98_bn1"
  type: "BatchNorm"
  bottom: "map64_98_conv1"
  top: "map64_98_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_98_bn1"
  type: "BatchNorm"
  bottom: "map64_98_conv1"
  top: "map64_98_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_98_scale1"
  type: "Scale"
  bottom: "map64_98_conv1"
  top: "map64_98_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_98_relu1"
  type: "ReLU"
  bottom: "map64_98_conv1"
  top: "map64_98_conv1"
}
layer {
  name: "map64_98_conv2"
  type: "Convolution"
  bottom: "map64_98_conv1"
  top: "map64_98_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_98_bn2"
  type: "BatchNorm"
  bottom: "map64_98_conv2"
  top: "map64_98_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_98_bn2"
  type: "BatchNorm"
  bottom: "map64_98_conv2"
  top: "map64_98_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_98_scale2"
  type: "Scale"
  bottom: "map64_98_conv2"
  top: "map64_98_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_98_relu2"
  type: "ReLU"
  bottom: "map64_98_conv2"
  top: "map64_98_conv2"
}
layer {
  name: "map64_98_conv_end"
  type: "Convolution"
  bottom: "map64_98_conv2"
  top: "map64_98_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_98_eltsum"
  type: "Eltwise"
  bottom: "map64_97_eltsum"
  bottom: "map64_98_conv_end"
  top: "map64_98_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_99_bn_pre"
  type: "BatchNorm"
  bottom: "map64_98_eltsum"
  top: "map64_99_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_99_bn_pre"
  type: "BatchNorm"
  bottom: "map64_98_eltsum"
  top: "map64_99_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_99_pre_scale"
  type: "Scale"
  bottom: "map64_99_bn_pre"
  top: "map64_99_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_99_pre_relu"
  type: "ReLU"
  bottom: "map64_99_bn_pre"
  top: "map64_99_bn_pre"
}
layer {
  name: "map64_99_conv1"
  type: "Convolution"
  bottom: "map64_99_bn_pre"
  top: "map64_99_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_99_bn1"
  type: "BatchNorm"
  bottom: "map64_99_conv1"
  top: "map64_99_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_99_bn1"
  type: "BatchNorm"
  bottom: "map64_99_conv1"
  top: "map64_99_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_99_scale1"
  type: "Scale"
  bottom: "map64_99_conv1"
  top: "map64_99_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_99_relu1"
  type: "ReLU"
  bottom: "map64_99_conv1"
  top: "map64_99_conv1"
}
layer {
  name: "map64_99_conv2"
  type: "Convolution"
  bottom: "map64_99_conv1"
  top: "map64_99_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_99_bn2"
  type: "BatchNorm"
  bottom: "map64_99_conv2"
  top: "map64_99_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_99_bn2"
  type: "BatchNorm"
  bottom: "map64_99_conv2"
  top: "map64_99_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_99_scale2"
  type: "Scale"
  bottom: "map64_99_conv2"
  top: "map64_99_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_99_relu2"
  type: "ReLU"
  bottom: "map64_99_conv2"
  top: "map64_99_conv2"
}
layer {
  name: "map64_99_conv_end"
  type: "Convolution"
  bottom: "map64_99_conv2"
  top: "map64_99_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_99_eltsum"
  type: "Eltwise"
  bottom: "map64_98_eltsum"
  bottom: "map64_99_conv_end"
  top: "map64_99_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_100_bn_pre"
  type: "BatchNorm"
  bottom: "map64_99_eltsum"
  top: "map64_100_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_100_bn_pre"
  type: "BatchNorm"
  bottom: "map64_99_eltsum"
  top: "map64_100_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_100_pre_scale"
  type: "Scale"
  bottom: "map64_100_bn_pre"
  top: "map64_100_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_100_pre_relu"
  type: "ReLU"
  bottom: "map64_100_bn_pre"
  top: "map64_100_bn_pre"
}
layer {
  name: "map64_100_conv1"
  type: "Convolution"
  bottom: "map64_100_bn_pre"
  top: "map64_100_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_100_bn1"
  type: "BatchNorm"
  bottom: "map64_100_conv1"
  top: "map64_100_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_100_bn1"
  type: "BatchNorm"
  bottom: "map64_100_conv1"
  top: "map64_100_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_100_scale1"
  type: "Scale"
  bottom: "map64_100_conv1"
  top: "map64_100_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_100_relu1"
  type: "ReLU"
  bottom: "map64_100_conv1"
  top: "map64_100_conv1"
}
layer {
  name: "map64_100_conv2"
  type: "Convolution"
  bottom: "map64_100_conv1"
  top: "map64_100_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_100_bn2"
  type: "BatchNorm"
  bottom: "map64_100_conv2"
  top: "map64_100_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_100_bn2"
  type: "BatchNorm"
  bottom: "map64_100_conv2"
  top: "map64_100_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_100_scale2"
  type: "Scale"
  bottom: "map64_100_conv2"
  top: "map64_100_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_100_relu2"
  type: "ReLU"
  bottom: "map64_100_conv2"
  top: "map64_100_conv2"
}
layer {
  name: "map64_100_conv_end"
  type: "Convolution"
  bottom: "map64_100_conv2"
  top: "map64_100_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_100_eltsum"
  type: "Eltwise"
  bottom: "map64_99_eltsum"
  bottom: "map64_100_conv_end"
  top: "map64_100_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_101_bn_pre"
  type: "BatchNorm"
  bottom: "map64_100_eltsum"
  top: "map64_101_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_101_bn_pre"
  type: "BatchNorm"
  bottom: "map64_100_eltsum"
  top: "map64_101_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_101_pre_scale"
  type: "Scale"
  bottom: "map64_101_bn_pre"
  top: "map64_101_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_101_pre_relu"
  type: "ReLU"
  bottom: "map64_101_bn_pre"
  top: "map64_101_bn_pre"
}
layer {
  name: "map64_101_conv1"
  type: "Convolution"
  bottom: "map64_101_bn_pre"
  top: "map64_101_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_101_bn1"
  type: "BatchNorm"
  bottom: "map64_101_conv1"
  top: "map64_101_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_101_bn1"
  type: "BatchNorm"
  bottom: "map64_101_conv1"
  top: "map64_101_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_101_scale1"
  type: "Scale"
  bottom: "map64_101_conv1"
  top: "map64_101_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_101_relu1"
  type: "ReLU"
  bottom: "map64_101_conv1"
  top: "map64_101_conv1"
}
layer {
  name: "map64_101_conv2"
  type: "Convolution"
  bottom: "map64_101_conv1"
  top: "map64_101_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_101_bn2"
  type: "BatchNorm"
  bottom: "map64_101_conv2"
  top: "map64_101_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_101_bn2"
  type: "BatchNorm"
  bottom: "map64_101_conv2"
  top: "map64_101_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_101_scale2"
  type: "Scale"
  bottom: "map64_101_conv2"
  top: "map64_101_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_101_relu2"
  type: "ReLU"
  bottom: "map64_101_conv2"
  top: "map64_101_conv2"
}
layer {
  name: "map64_101_conv_end"
  type: "Convolution"
  bottom: "map64_101_conv2"
  top: "map64_101_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_101_eltsum"
  type: "Eltwise"
  bottom: "map64_100_eltsum"
  bottom: "map64_101_conv_end"
  top: "map64_101_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_102_bn_pre"
  type: "BatchNorm"
  bottom: "map64_101_eltsum"
  top: "map64_102_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_102_bn_pre"
  type: "BatchNorm"
  bottom: "map64_101_eltsum"
  top: "map64_102_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_102_pre_scale"
  type: "Scale"
  bottom: "map64_102_bn_pre"
  top: "map64_102_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_102_pre_relu"
  type: "ReLU"
  bottom: "map64_102_bn_pre"
  top: "map64_102_bn_pre"
}
layer {
  name: "map64_102_conv1"
  type: "Convolution"
  bottom: "map64_102_bn_pre"
  top: "map64_102_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_102_bn1"
  type: "BatchNorm"
  bottom: "map64_102_conv1"
  top: "map64_102_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_102_bn1"
  type: "BatchNorm"
  bottom: "map64_102_conv1"
  top: "map64_102_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_102_scale1"
  type: "Scale"
  bottom: "map64_102_conv1"
  top: "map64_102_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_102_relu1"
  type: "ReLU"
  bottom: "map64_102_conv1"
  top: "map64_102_conv1"
}
layer {
  name: "map64_102_conv2"
  type: "Convolution"
  bottom: "map64_102_conv1"
  top: "map64_102_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_102_bn2"
  type: "BatchNorm"
  bottom: "map64_102_conv2"
  top: "map64_102_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_102_bn2"
  type: "BatchNorm"
  bottom: "map64_102_conv2"
  top: "map64_102_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_102_scale2"
  type: "Scale"
  bottom: "map64_102_conv2"
  top: "map64_102_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_102_relu2"
  type: "ReLU"
  bottom: "map64_102_conv2"
  top: "map64_102_conv2"
}
layer {
  name: "map64_102_conv_end"
  type: "Convolution"
  bottom: "map64_102_conv2"
  top: "map64_102_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_102_eltsum"
  type: "Eltwise"
  bottom: "map64_101_eltsum"
  bottom: "map64_102_conv_end"
  top: "map64_102_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_103_bn_pre"
  type: "BatchNorm"
  bottom: "map64_102_eltsum"
  top: "map64_103_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_103_bn_pre"
  type: "BatchNorm"
  bottom: "map64_102_eltsum"
  top: "map64_103_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_103_pre_scale"
  type: "Scale"
  bottom: "map64_103_bn_pre"
  top: "map64_103_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_103_pre_relu"
  type: "ReLU"
  bottom: "map64_103_bn_pre"
  top: "map64_103_bn_pre"
}
layer {
  name: "map64_103_conv1"
  type: "Convolution"
  bottom: "map64_103_bn_pre"
  top: "map64_103_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_103_bn1"
  type: "BatchNorm"
  bottom: "map64_103_conv1"
  top: "map64_103_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_103_bn1"
  type: "BatchNorm"
  bottom: "map64_103_conv1"
  top: "map64_103_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_103_scale1"
  type: "Scale"
  bottom: "map64_103_conv1"
  top: "map64_103_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_103_relu1"
  type: "ReLU"
  bottom: "map64_103_conv1"
  top: "map64_103_conv1"
}
layer {
  name: "map64_103_conv2"
  type: "Convolution"
  bottom: "map64_103_conv1"
  top: "map64_103_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_103_bn2"
  type: "BatchNorm"
  bottom: "map64_103_conv2"
  top: "map64_103_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_103_bn2"
  type: "BatchNorm"
  bottom: "map64_103_conv2"
  top: "map64_103_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_103_scale2"
  type: "Scale"
  bottom: "map64_103_conv2"
  top: "map64_103_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_103_relu2"
  type: "ReLU"
  bottom: "map64_103_conv2"
  top: "map64_103_conv2"
}
layer {
  name: "map64_103_conv_end"
  type: "Convolution"
  bottom: "map64_103_conv2"
  top: "map64_103_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_103_eltsum"
  type: "Eltwise"
  bottom: "map64_102_eltsum"
  bottom: "map64_103_conv_end"
  top: "map64_103_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_104_bn_pre"
  type: "BatchNorm"
  bottom: "map64_103_eltsum"
  top: "map64_104_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_104_bn_pre"
  type: "BatchNorm"
  bottom: "map64_103_eltsum"
  top: "map64_104_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_104_pre_scale"
  type: "Scale"
  bottom: "map64_104_bn_pre"
  top: "map64_104_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_104_pre_relu"
  type: "ReLU"
  bottom: "map64_104_bn_pre"
  top: "map64_104_bn_pre"
}
layer {
  name: "map64_104_conv1"
  type: "Convolution"
  bottom: "map64_104_bn_pre"
  top: "map64_104_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_104_bn1"
  type: "BatchNorm"
  bottom: "map64_104_conv1"
  top: "map64_104_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_104_bn1"
  type: "BatchNorm"
  bottom: "map64_104_conv1"
  top: "map64_104_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_104_scale1"
  type: "Scale"
  bottom: "map64_104_conv1"
  top: "map64_104_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_104_relu1"
  type: "ReLU"
  bottom: "map64_104_conv1"
  top: "map64_104_conv1"
}
layer {
  name: "map64_104_conv2"
  type: "Convolution"
  bottom: "map64_104_conv1"
  top: "map64_104_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_104_bn2"
  type: "BatchNorm"
  bottom: "map64_104_conv2"
  top: "map64_104_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_104_bn2"
  type: "BatchNorm"
  bottom: "map64_104_conv2"
  top: "map64_104_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_104_scale2"
  type: "Scale"
  bottom: "map64_104_conv2"
  top: "map64_104_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_104_relu2"
  type: "ReLU"
  bottom: "map64_104_conv2"
  top: "map64_104_conv2"
}
layer {
  name: "map64_104_conv_end"
  type: "Convolution"
  bottom: "map64_104_conv2"
  top: "map64_104_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_104_eltsum"
  type: "Eltwise"
  bottom: "map64_103_eltsum"
  bottom: "map64_104_conv_end"
  top: "map64_104_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_105_bn_pre"
  type: "BatchNorm"
  bottom: "map64_104_eltsum"
  top: "map64_105_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_105_bn_pre"
  type: "BatchNorm"
  bottom: "map64_104_eltsum"
  top: "map64_105_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_105_pre_scale"
  type: "Scale"
  bottom: "map64_105_bn_pre"
  top: "map64_105_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_105_pre_relu"
  type: "ReLU"
  bottom: "map64_105_bn_pre"
  top: "map64_105_bn_pre"
}
layer {
  name: "map64_105_conv1"
  type: "Convolution"
  bottom: "map64_105_bn_pre"
  top: "map64_105_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_105_bn1"
  type: "BatchNorm"
  bottom: "map64_105_conv1"
  top: "map64_105_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_105_bn1"
  type: "BatchNorm"
  bottom: "map64_105_conv1"
  top: "map64_105_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_105_scale1"
  type: "Scale"
  bottom: "map64_105_conv1"
  top: "map64_105_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_105_relu1"
  type: "ReLU"
  bottom: "map64_105_conv1"
  top: "map64_105_conv1"
}
layer {
  name: "map64_105_conv2"
  type: "Convolution"
  bottom: "map64_105_conv1"
  top: "map64_105_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_105_bn2"
  type: "BatchNorm"
  bottom: "map64_105_conv2"
  top: "map64_105_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_105_bn2"
  type: "BatchNorm"
  bottom: "map64_105_conv2"
  top: "map64_105_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_105_scale2"
  type: "Scale"
  bottom: "map64_105_conv2"
  top: "map64_105_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_105_relu2"
  type: "ReLU"
  bottom: "map64_105_conv2"
  top: "map64_105_conv2"
}
layer {
  name: "map64_105_conv_end"
  type: "Convolution"
  bottom: "map64_105_conv2"
  top: "map64_105_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_105_eltsum"
  type: "Eltwise"
  bottom: "map64_104_eltsum"
  bottom: "map64_105_conv_end"
  top: "map64_105_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_106_bn_pre"
  type: "BatchNorm"
  bottom: "map64_105_eltsum"
  top: "map64_106_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_106_bn_pre"
  type: "BatchNorm"
  bottom: "map64_105_eltsum"
  top: "map64_106_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_106_pre_scale"
  type: "Scale"
  bottom: "map64_106_bn_pre"
  top: "map64_106_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_106_pre_relu"
  type: "ReLU"
  bottom: "map64_106_bn_pre"
  top: "map64_106_bn_pre"
}
layer {
  name: "map64_106_conv1"
  type: "Convolution"
  bottom: "map64_106_bn_pre"
  top: "map64_106_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_106_bn1"
  type: "BatchNorm"
  bottom: "map64_106_conv1"
  top: "map64_106_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_106_bn1"
  type: "BatchNorm"
  bottom: "map64_106_conv1"
  top: "map64_106_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_106_scale1"
  type: "Scale"
  bottom: "map64_106_conv1"
  top: "map64_106_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_106_relu1"
  type: "ReLU"
  bottom: "map64_106_conv1"
  top: "map64_106_conv1"
}
layer {
  name: "map64_106_conv2"
  type: "Convolution"
  bottom: "map64_106_conv1"
  top: "map64_106_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_106_bn2"
  type: "BatchNorm"
  bottom: "map64_106_conv2"
  top: "map64_106_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_106_bn2"
  type: "BatchNorm"
  bottom: "map64_106_conv2"
  top: "map64_106_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_106_scale2"
  type: "Scale"
  bottom: "map64_106_conv2"
  top: "map64_106_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_106_relu2"
  type: "ReLU"
  bottom: "map64_106_conv2"
  top: "map64_106_conv2"
}
layer {
  name: "map64_106_conv_end"
  type: "Convolution"
  bottom: "map64_106_conv2"
  top: "map64_106_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_106_eltsum"
  type: "Eltwise"
  bottom: "map64_105_eltsum"
  bottom: "map64_106_conv_end"
  top: "map64_106_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_107_bn_pre"
  type: "BatchNorm"
  bottom: "map64_106_eltsum"
  top: "map64_107_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_107_bn_pre"
  type: "BatchNorm"
  bottom: "map64_106_eltsum"
  top: "map64_107_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_107_pre_scale"
  type: "Scale"
  bottom: "map64_107_bn_pre"
  top: "map64_107_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_107_pre_relu"
  type: "ReLU"
  bottom: "map64_107_bn_pre"
  top: "map64_107_bn_pre"
}
layer {
  name: "map64_107_conv1"
  type: "Convolution"
  bottom: "map64_107_bn_pre"
  top: "map64_107_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_107_bn1"
  type: "BatchNorm"
  bottom: "map64_107_conv1"
  top: "map64_107_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_107_bn1"
  type: "BatchNorm"
  bottom: "map64_107_conv1"
  top: "map64_107_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_107_scale1"
  type: "Scale"
  bottom: "map64_107_conv1"
  top: "map64_107_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_107_relu1"
  type: "ReLU"
  bottom: "map64_107_conv1"
  top: "map64_107_conv1"
}
layer {
  name: "map64_107_conv2"
  type: "Convolution"
  bottom: "map64_107_conv1"
  top: "map64_107_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_107_bn2"
  type: "BatchNorm"
  bottom: "map64_107_conv2"
  top: "map64_107_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_107_bn2"
  type: "BatchNorm"
  bottom: "map64_107_conv2"
  top: "map64_107_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_107_scale2"
  type: "Scale"
  bottom: "map64_107_conv2"
  top: "map64_107_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_107_relu2"
  type: "ReLU"
  bottom: "map64_107_conv2"
  top: "map64_107_conv2"
}
layer {
  name: "map64_107_conv_end"
  type: "Convolution"
  bottom: "map64_107_conv2"
  top: "map64_107_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_107_eltsum"
  type: "Eltwise"
  bottom: "map64_106_eltsum"
  bottom: "map64_107_conv_end"
  top: "map64_107_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_108_bn_pre"
  type: "BatchNorm"
  bottom: "map64_107_eltsum"
  top: "map64_108_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_108_bn_pre"
  type: "BatchNorm"
  bottom: "map64_107_eltsum"
  top: "map64_108_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_108_pre_scale"
  type: "Scale"
  bottom: "map64_108_bn_pre"
  top: "map64_108_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_108_pre_relu"
  type: "ReLU"
  bottom: "map64_108_bn_pre"
  top: "map64_108_bn_pre"
}
layer {
  name: "map64_108_conv1"
  type: "Convolution"
  bottom: "map64_108_bn_pre"
  top: "map64_108_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_108_bn1"
  type: "BatchNorm"
  bottom: "map64_108_conv1"
  top: "map64_108_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_108_bn1"
  type: "BatchNorm"
  bottom: "map64_108_conv1"
  top: "map64_108_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_108_scale1"
  type: "Scale"
  bottom: "map64_108_conv1"
  top: "map64_108_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_108_relu1"
  type: "ReLU"
  bottom: "map64_108_conv1"
  top: "map64_108_conv1"
}
layer {
  name: "map64_108_conv2"
  type: "Convolution"
  bottom: "map64_108_conv1"
  top: "map64_108_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_108_bn2"
  type: "BatchNorm"
  bottom: "map64_108_conv2"
  top: "map64_108_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_108_bn2"
  type: "BatchNorm"
  bottom: "map64_108_conv2"
  top: "map64_108_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_108_scale2"
  type: "Scale"
  bottom: "map64_108_conv2"
  top: "map64_108_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_108_relu2"
  type: "ReLU"
  bottom: "map64_108_conv2"
  top: "map64_108_conv2"
}
layer {
  name: "map64_108_conv_end"
  type: "Convolution"
  bottom: "map64_108_conv2"
  top: "map64_108_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_108_eltsum"
  type: "Eltwise"
  bottom: "map64_107_eltsum"
  bottom: "map64_108_conv_end"
  top: "map64_108_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_109_bn_pre"
  type: "BatchNorm"
  bottom: "map64_108_eltsum"
  top: "map64_109_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_109_bn_pre"
  type: "BatchNorm"
  bottom: "map64_108_eltsum"
  top: "map64_109_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_109_pre_scale"
  type: "Scale"
  bottom: "map64_109_bn_pre"
  top: "map64_109_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_109_pre_relu"
  type: "ReLU"
  bottom: "map64_109_bn_pre"
  top: "map64_109_bn_pre"
}
layer {
  name: "map64_109_conv1"
  type: "Convolution"
  bottom: "map64_109_bn_pre"
  top: "map64_109_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_109_bn1"
  type: "BatchNorm"
  bottom: "map64_109_conv1"
  top: "map64_109_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_109_bn1"
  type: "BatchNorm"
  bottom: "map64_109_conv1"
  top: "map64_109_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_109_scale1"
  type: "Scale"
  bottom: "map64_109_conv1"
  top: "map64_109_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_109_relu1"
  type: "ReLU"
  bottom: "map64_109_conv1"
  top: "map64_109_conv1"
}
layer {
  name: "map64_109_conv2"
  type: "Convolution"
  bottom: "map64_109_conv1"
  top: "map64_109_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_109_bn2"
  type: "BatchNorm"
  bottom: "map64_109_conv2"
  top: "map64_109_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_109_bn2"
  type: "BatchNorm"
  bottom: "map64_109_conv2"
  top: "map64_109_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_109_scale2"
  type: "Scale"
  bottom: "map64_109_conv2"
  top: "map64_109_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_109_relu2"
  type: "ReLU"
  bottom: "map64_109_conv2"
  top: "map64_109_conv2"
}
layer {
  name: "map64_109_conv_end"
  type: "Convolution"
  bottom: "map64_109_conv2"
  top: "map64_109_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_109_eltsum"
  type: "Eltwise"
  bottom: "map64_108_eltsum"
  bottom: "map64_109_conv_end"
  top: "map64_109_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_110_bn_pre"
  type: "BatchNorm"
  bottom: "map64_109_eltsum"
  top: "map64_110_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_110_bn_pre"
  type: "BatchNorm"
  bottom: "map64_109_eltsum"
  top: "map64_110_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_110_pre_scale"
  type: "Scale"
  bottom: "map64_110_bn_pre"
  top: "map64_110_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_110_pre_relu"
  type: "ReLU"
  bottom: "map64_110_bn_pre"
  top: "map64_110_bn_pre"
}
layer {
  name: "map64_110_conv1"
  type: "Convolution"
  bottom: "map64_110_bn_pre"
  top: "map64_110_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_110_bn1"
  type: "BatchNorm"
  bottom: "map64_110_conv1"
  top: "map64_110_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_110_bn1"
  type: "BatchNorm"
  bottom: "map64_110_conv1"
  top: "map64_110_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_110_scale1"
  type: "Scale"
  bottom: "map64_110_conv1"
  top: "map64_110_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_110_relu1"
  type: "ReLU"
  bottom: "map64_110_conv1"
  top: "map64_110_conv1"
}
layer {
  name: "map64_110_conv2"
  type: "Convolution"
  bottom: "map64_110_conv1"
  top: "map64_110_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_110_bn2"
  type: "BatchNorm"
  bottom: "map64_110_conv2"
  top: "map64_110_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_110_bn2"
  type: "BatchNorm"
  bottom: "map64_110_conv2"
  top: "map64_110_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_110_scale2"
  type: "Scale"
  bottom: "map64_110_conv2"
  top: "map64_110_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_110_relu2"
  type: "ReLU"
  bottom: "map64_110_conv2"
  top: "map64_110_conv2"
}
layer {
  name: "map64_110_conv_end"
  type: "Convolution"
  bottom: "map64_110_conv2"
  top: "map64_110_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_110_eltsum"
  type: "Eltwise"
  bottom: "map64_109_eltsum"
  bottom: "map64_110_conv_end"
  top: "map64_110_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "map64_111_bn_pre"
  type: "BatchNorm"
  bottom: "map64_110_eltsum"
  top: "map64_111_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_111_bn_pre"
  type: "BatchNorm"
  bottom: "map64_110_eltsum"
  top: "map64_111_bn_pre"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_111_pre_scale"
  type: "Scale"
  bottom: "map64_111_bn_pre"
  top: "map64_111_bn_pre"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_111_pre_relu"
  type: "ReLU"
  bottom: "map64_111_bn_pre"
  top: "map64_111_bn_pre"
}
layer {
  name: "map64_111_conv1"
  type: "Convolution"
  bottom: "map64_111_bn_pre"
  top: "map64_111_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_111_bn1"
  type: "BatchNorm"
  bottom: "map64_111_conv1"
  top: "map64_111_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_111_bn1"
  type: "BatchNorm"
  bottom: "map64_111_conv1"
  top: "map64_111_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_111_scale1"
  type: "Scale"
  bottom: "map64_111_conv1"
  top: "map64_111_conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_111_relu1"
  type: "ReLU"
  bottom: "map64_111_conv1"
  top: "map64_111_conv1"
}
layer {
  name: "map64_111_conv2"
  type: "Convolution"
  bottom: "map64_111_conv1"
  top: "map64_111_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_111_bn2"
  type: "BatchNorm"
  bottom: "map64_111_conv2"
  top: "map64_111_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "map64_111_bn2"
  type: "BatchNorm"
  bottom: "map64_111_conv2"
  top: "map64_111_conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "map64_111_scale2"
  type: "Scale"
  bottom: "map64_111_conv2"
  top: "map64_111_conv2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "map64_111_relu2"
  type: "ReLU"
  bottom: "map64_111_conv2"
  top: "map64_111_conv2"
}
layer {
  name: "map64_111_conv_end"
  type: "Convolution"
  bottom: "map64_111_conv2"
  top: "map64_111_conv_end"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "map64_111_eltsum"
  type: "Eltwise"
  bottom: "map64_110_eltsum"
  bottom: "map64_111_conv_end"
  top: "map64_111_eltsum"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "BN_end"
  type: "BatchNorm"
  bottom: "map64_111_eltsum"
  top: "BN_end"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "BN_end"
  type: "BatchNorm"
  bottom: "map64_111_eltsum"
  top: "BN_end"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "scale_end"
  type: "Scale"
  bottom: "BN_end"
  top: "BN_end"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu_end"
  type: "ReLU"
  bottom: "BN_end"
  top: "BN_end"
}
layer {
  name: "pool_global"
  type: "Pooling"
  bottom: "BN_end"
  top: "pool_global"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "score"
  type: "InnerProduct"
  bottom: "pool_global"
  top: "score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "score"
  bottom: "label"
  top: "loss"
}
layer {
  name: "acc"
  type: "Accuracy"
  bottom: "score"
  bottom: "label"
  top: "acc"
}
